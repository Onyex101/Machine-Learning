{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPaffEc8JzTra2a6p6x3X5H"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hIlMFRYXz1qW","executionInfo":{"status":"ok","timestamp":1674168258381,"user_tz":360,"elapsed":19440,"user":{"displayName":"Austin Okoye","userId":"09681958171144788598"}},"outputId":"ee117c97-16e5-43eb-e5fa-a3689d082452"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","python-opengl is already the newest version (3.1.0+dfsg-2build1).\n","0 upgraded, 0 newly installed, 0 to remove and 23 not upgraded.\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","xvfb is already the newest version (2:1.20.13-1ubuntu1~20.04.5).\n","0 upgraded, 0 newly installed, 0 to remove and 23 not upgraded.\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.8/dist-packages (3.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: piglet in /usr/local/lib/python3.8/dist-packages (1.0.0)\n","Requirement already satisfied: piglet-templates in /usr/local/lib/python3.8/dist-packages (from piglet) (1.3.0)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.8/dist-packages (from piglet-templates->piglet) (3.0.9)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.8/dist-packages (from piglet-templates->piglet) (22.2.0)\n","Requirement already satisfied: markupsafe in /usr/local/lib/python3.8/dist-packages (from piglet-templates->piglet) (2.0.1)\n","Requirement already satisfied: astunparse in /usr/local/lib/python3.8/dist-packages (from piglet-templates->piglet) (1.6.3)\n","Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.8/dist-packages (from astunparse->piglet-templates->piglet) (1.15.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse->piglet-templates->piglet) (0.38.4)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gym[classic_control] in /usr/local/lib/python3.8/dist-packages (0.25.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym[classic_control]) (2.2.0)\n","Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym[classic_control]) (6.0.0)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym[classic_control]) (1.21.6)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym[classic_control]) (0.0.8)\n","Requirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.8/dist-packages (from gym[classic_control]) (2.1.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym[classic_control]) (3.11.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting keras-rl2\n","  Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 KB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (from keras-rl2) (2.9.2)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (0.4.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (4.4.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (1.14.1)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n","Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (2.9.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (0.29.0)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (1.21.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (57.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (21.3)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n","Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (2.9.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (1.51.1)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (1.3.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (2.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (15.0.6.1)\n","Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (1.12)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (3.19.6)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n","Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (2.9.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.38.4)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras-rl2) (3.4.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras-rl2) (2.16.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras-rl2) (2.25.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras-rl2) (1.0.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras-rl2) (0.6.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras-rl2) (1.8.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras-rl2) (0.4.6)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow->keras-rl2) (3.0.9)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->keras-rl2) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->keras-rl2) (4.9)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->keras-rl2) (5.2.1)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow->keras-rl2) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow->keras-rl2) (6.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow->keras-rl2) (2.10)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow->keras-rl2) (4.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow->keras-rl2) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow->keras-rl2) (1.24.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow->keras-rl2) (3.11.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->keras-rl2) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow->keras-rl2) (3.2.2)\n","Installing collected packages: keras-rl2\n","Successfully installed keras-rl2-1.0.5\n"]},{"output_type":"execute_result","data":{"text/plain":["<pyvirtualdisplay.display.Display at 0x7f17dff3dac0>"]},"metadata":{},"execution_count":3}],"source":["!apt-get install python-opengl -y\n","!apt install xvfb -y\n","!pip install pyvirtualdisplay\n","!pip install piglet\n","!pip install gym[classic_control]\n","!pip install keras-rl2\n","\n","from pyvirtualdisplay import Display\n","Display().start()"]},{"cell_type":"code","source":["import numpy as np\n","import random\n","import gym\n","from IPython import display\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.models import Sequential, clone_model\n","from tensorflow.keras.layers import Dense, Activation, Flatten\n","from tensorflow.keras.optimizers import Adam\n","\n","from rl.agents.dqn import DQNAgent\n","%matplotlib inline"],"metadata":{"id":"qvwtBCP7z2uQ","executionInfo":{"status":"ok","timestamp":1674168260577,"user_tz":360,"elapsed":2,"user":{"displayName":"Austin Okoye","userId":"09681958171144788598"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["env_name = 'CartPole-v1'\n","env = gym.make(env_name)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MbEc4dEyz2xP","executionInfo":{"status":"ok","timestamp":1674168268382,"user_tz":360,"elapsed":148,"user":{"displayName":"Austin Okoye","userId":"09681958171144788598"}},"outputId":"1896d6ca-2777-46a4-f928-c28df72b6fa3"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n"]}]},{"cell_type":"code","source":["env.reset()\n","img = plt.imshow(env.render('rgb_array')) # only call this once\n","for _ in range(20):\n","  img.set_data(env.render('rgb_array')) # just update the data\n","  display.display(plt.gcf())\n","  display.clear_output(wait=True)\n","  action = env.action_space.sample()\n","  observation, reward, done, info = env.step(action)\n","  # if done:\n","  #   env.reset()\n","env.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":269},"id":"cOkyazdrz20O","executionInfo":{"status":"ok","timestamp":1674168315988,"user_tz":360,"elapsed":3819,"user":{"displayName":"Austin Okoye","userId":"09681958171144788598"}},"outputId":"bfea817b-f809-48a2-8268-ec40e1e8b29a"},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["nb_actions = env.action_space.n\n","nb_actions"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"03trlUYSz22-","executionInfo":{"status":"ok","timestamp":1674168466595,"user_tz":360,"elapsed":114,"user":{"displayName":"Austin Okoye","userId":"09681958171144788598"}},"outputId":"770b6678-7d40-456b-8639-a644721bb22d"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["nb_obs = env.observation_space.shape\n","nb_obs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"63uTtOa2z250","executionInfo":{"status":"ok","timestamp":1674168474515,"user_tz":360,"elapsed":119,"user":{"displayName":"Austin Okoye","userId":"09681958171144788598"}},"outputId":"9e15b16c-96b6-45b3-e6b0-f2d020e0a34f"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(4,)"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["model = Sequential()\n","\n","model.add(Flatten(input_shape=(1,)+nb_obs))\n","\n","model.add(Dense(16))\n","model.add(Activation('relu'))\n","\n","model.add(Dense(32))\n","model.add(Activation('relu'))\n","\n","# Neurons == action_space\n","model.add(Dense(nb_actions))\n","model.add(Activation('linear'))"],"metadata":{"id":"3cyuRasKz2_R","executionInfo":{"status":"ok","timestamp":1674168647060,"user_tz":360,"elapsed":165,"user":{"displayName":"Austin Okoye","userId":"09681958171144788598"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["print(model.summary())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dUlk3C7-z3CB","executionInfo":{"status":"ok","timestamp":1674168666815,"user_tz":360,"elapsed":131,"user":{"displayName":"Austin Okoye","userId":"09681958171144788598"}},"outputId":"30b249aa-ce87-4647-8f05-34bf5808e546"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," flatten (Flatten)           (None, 4)                 0         \n","                                                                 \n"," dense (Dense)               (None, 16)                80        \n","                                                                 \n"," activation (Activation)     (None, 16)                0         \n","                                                                 \n"," dense_1 (Dense)             (None, 32)                544       \n","                                                                 \n"," activation_1 (Activation)   (None, 32)                0         \n","                                                                 \n"," dense_2 (Dense)             (None, 2)                 66        \n","                                                                 \n"," activation_2 (Activation)   (None, 2)                 0         \n","                                                                 \n","=================================================================\n","Total params: 690\n","Trainable params: 690\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"]}]},{"cell_type":"code","source":["from rl.memory import SequentialMemory\n","from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy"],"metadata":{"id":"yPqxbglSOo5K","executionInfo":{"status":"ok","timestamp":1674169197208,"user_tz":360,"elapsed":132,"user":{"displayName":"Austin Okoye","userId":"09681958171144788598"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["memory = SequentialMemory(limit=20000,window_length=1)"],"metadata":{"id":"-VwvdsPIOo8H","executionInfo":{"status":"ok","timestamp":1674169192439,"user_tz":360,"elapsed":192,"user":{"displayName":"Austin Okoye","userId":"09681958171144788598"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps',\n","              value_max=1.0,\n","              value_min=0.1, \n","              value_test=0.05,\n","              nb_steps=20000\n","              )"],"metadata":{"id":"y2WWe5x6Oo-6","executionInfo":{"status":"ok","timestamp":1674169200520,"user_tz":360,"elapsed":212,"user":{"displayName":"Austin Okoye","userId":"09681958171144788598"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["dqn = DQNAgent(model=model,nb_actions=nb_actions,\n","               memory=memory,nb_steps_warmup=10,\n","               target_model_update=100,policy=policy)"],"metadata":{"id":"iwGe2eH2z3FR","executionInfo":{"status":"ok","timestamp":1674169216497,"user_tz":360,"elapsed":192,"user":{"displayName":"Austin Okoye","userId":"09681958171144788598"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["dqn.compile(Adam(learning_rate=1e-3),metrics=['mae'])"],"metadata":{"id":"75_D5ylfQso6","executionInfo":{"status":"ok","timestamp":1674169276021,"user_tz":360,"elapsed":1103,"user":{"displayName":"Austin Okoye","userId":"09681958171144788598"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["dqn.fit(env,nb_steps=20000,visualize=False,verbose=2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RCzBaqEMQsrr","executionInfo":{"status":"ok","timestamp":1674169481452,"user_tz":360,"elapsed":161333,"user":{"displayName":"Austin Okoye","userId":"09681958171144788598"}},"outputId":"93d76ddd-7986-4c90-ba18-cc5d692227aa"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Training for 20000 steps ...\n","    10/20000: episode: 1, duration: 0.090s, episode steps:  10, steps per second: 111, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates=self.state_updates,\n","/usr/local/lib/python3.8/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n","  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n","/usr/local/lib/python3.8/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 11 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.8/dist-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates=self.state_updates,\n","/usr/local/lib/python3.8/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 12 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.8/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 13 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.8/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 14 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.8/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 15 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.8/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 16 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.8/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 17 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.8/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 18 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.8/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 19 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.8/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 20 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.8/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 21 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.8/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 22 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.8/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 23 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.8/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n","  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n","/usr/local/lib/python3.8/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 24 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.8/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 25 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.8/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 26 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.8/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 27 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.8/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 28 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.8/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 29 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.8/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 30 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.8/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 31 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n"]},{"output_type":"stream","name":"stdout","text":["    23/20000: episode: 2, duration: 0.686s, episode steps:  13, steps per second:  19, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.491474, mae: 0.532847, mean_q: 0.015474, mean_eps: 0.999258\n","    37/20000: episode: 3, duration: 0.104s, episode steps:  14, steps per second: 135, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.857 [0.000, 1.000],  loss: 0.311866, mae: 0.449661, mean_q: 0.232773, mean_eps: 0.998672\n","    53/20000: episode: 4, duration: 0.123s, episode steps:  16, steps per second: 130, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 0.180273, mae: 0.421191, mean_q: 0.468962, mean_eps: 0.997997\n","    78/20000: episode: 5, duration: 0.172s, episode steps:  25, steps per second: 145, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  loss: 0.130692, mae: 0.482848, mean_q: 0.604871, mean_eps: 0.997075\n","   100/20000: episode: 6, duration: 0.161s, episode steps:  22, steps per second: 136, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.096765, mae: 0.528165, mean_q: 0.724915, mean_eps: 0.996018\n","   117/20000: episode: 7, duration: 0.125s, episode steps:  17, steps per second: 136, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 0.465200, mae: 0.888673, mean_q: 0.953297, mean_eps: 0.995140\n","   134/20000: episode: 8, duration: 0.129s, episode steps:  17, steps per second: 132, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 0.251941, mae: 0.900216, mean_q: 1.428019, mean_eps: 0.994375\n","   148/20000: episode: 9, duration: 0.102s, episode steps:  14, steps per second: 137, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.167845, mae: 0.974760, mean_q: 1.648432, mean_eps: 0.993678\n","   164/20000: episode: 10, duration: 0.108s, episode steps:  16, steps per second: 148, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 0.106295, mae: 0.969240, mean_q: 1.670018, mean_eps: 0.993003\n","   175/20000: episode: 11, duration: 0.077s, episode steps:  11, steps per second: 143, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.115289, mae: 0.992781, mean_q: 1.692416, mean_eps: 0.992395\n","   191/20000: episode: 12, duration: 0.125s, episode steps:  16, steps per second: 128, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.072482, mae: 0.943882, mean_q: 1.679787, mean_eps: 0.991788\n","   229/20000: episode: 13, duration: 0.266s, episode steps:  38, steps per second: 143, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.605 [0.000, 1.000],  loss: 0.299745, mae: 1.319573, mean_q: 2.192843, mean_eps: 0.990572\n","   238/20000: episode: 14, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.296901, mae: 1.530661, mean_q: 2.865998, mean_eps: 0.989515\n","   253/20000: episode: 15, duration: 0.109s, episode steps:  15, steps per second: 138, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.229596, mae: 1.493586, mean_q: 2.642180, mean_eps: 0.988975\n","   265/20000: episode: 16, duration: 0.085s, episode steps:  12, steps per second: 142, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 0.180430, mae: 1.477555, mean_q: 2.665467, mean_eps: 0.988368\n","   278/20000: episode: 17, duration: 0.107s, episode steps:  13, steps per second: 121, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.156385, mae: 1.462468, mean_q: 2.732021, mean_eps: 0.987805\n","   318/20000: episode: 18, duration: 0.279s, episode steps:  40, steps per second: 143, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.575 [0.000, 1.000],  loss: 0.310375, mae: 1.652377, mean_q: 2.883968, mean_eps: 0.986613\n","   348/20000: episode: 19, duration: 0.201s, episode steps:  30, steps per second: 149, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.320900, mae: 1.938338, mean_q: 3.607370, mean_eps: 0.985038\n","   394/20000: episode: 20, duration: 0.316s, episode steps:  46, steps per second: 145, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.198858, mae: 1.911568, mean_q: 3.641100, mean_eps: 0.983328\n","   430/20000: episode: 21, duration: 0.263s, episode steps:  36, steps per second: 137, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.334490, mae: 2.275535, mean_q: 4.148479, mean_eps: 0.981482\n","   468/20000: episode: 22, duration: 0.265s, episode steps:  38, steps per second: 143, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.447 [0.000, 1.000],  loss: 0.265680, mae: 2.346795, mean_q: 4.439201, mean_eps: 0.979818\n","   479/20000: episode: 23, duration: 0.084s, episode steps:  11, steps per second: 131, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.247330, mae: 2.342122, mean_q: 4.458403, mean_eps: 0.978715\n","   492/20000: episode: 24, duration: 0.084s, episode steps:  13, steps per second: 154, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.255535, mae: 2.345066, mean_q: 4.434627, mean_eps: 0.978175\n","   503/20000: episode: 25, duration: 0.082s, episode steps:  11, steps per second: 134, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.391859, mae: 2.448188, mean_q: 4.474535, mean_eps: 0.977635\n","   515/20000: episode: 26, duration: 0.083s, episode steps:  12, steps per second: 145, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.379153, mae: 2.742820, mean_q: 4.911604, mean_eps: 0.977117\n","   542/20000: episode: 27, duration: 0.183s, episode steps:  27, steps per second: 148, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.382138, mae: 2.834294, mean_q: 5.379909, mean_eps: 0.976240\n","   564/20000: episode: 28, duration: 0.164s, episode steps:  22, steps per second: 134, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.340786, mae: 2.772503, mean_q: 5.319047, mean_eps: 0.975137\n","   581/20000: episode: 29, duration: 0.126s, episode steps:  17, steps per second: 135, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 0.290149, mae: 2.778468, mean_q: 5.342643, mean_eps: 0.974260\n","   602/20000: episode: 30, duration: 0.143s, episode steps:  21, steps per second: 146, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.619 [0.000, 1.000],  loss: 0.329978, mae: 2.779135, mean_q: 5.227250, mean_eps: 0.973405\n","   618/20000: episode: 31, duration: 0.126s, episode steps:  16, steps per second: 127, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.566423, mae: 3.159552, mean_q: 5.802208, mean_eps: 0.972573\n","   632/20000: episode: 32, duration: 0.102s, episode steps:  14, steps per second: 137, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 0.567296, mae: 3.191581, mean_q: 6.039243, mean_eps: 0.971898\n","   648/20000: episode: 33, duration: 0.118s, episode steps:  16, steps per second: 136, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 0.257822, mae: 3.111613, mean_q: 6.014575, mean_eps: 0.971222\n","   667/20000: episode: 34, duration: 0.138s, episode steps:  19, steps per second: 138, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 0.259540, mae: 3.170611, mean_q: 6.180106, mean_eps: 0.970435\n","   696/20000: episode: 35, duration: 0.208s, episode steps:  29, steps per second: 139, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  loss: 0.417299, mae: 3.151186, mean_q: 5.986476, mean_eps: 0.969355\n","   727/20000: episode: 36, duration: 0.229s, episode steps:  31, steps per second: 135, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 0.559573, mae: 3.523060, mean_q: 6.575598, mean_eps: 0.968005\n","   756/20000: episode: 37, duration: 0.208s, episode steps:  29, steps per second: 140, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.586 [0.000, 1.000],  loss: 0.484737, mae: 3.604322, mean_q: 6.883227, mean_eps: 0.966655\n","   779/20000: episode: 38, duration: 0.202s, episode steps:  23, steps per second: 114, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 0.565535, mae: 3.607904, mean_q: 6.809584, mean_eps: 0.965485\n","   790/20000: episode: 39, duration: 0.081s, episode steps:  11, steps per second: 136, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.630766, mae: 3.620874, mean_q: 6.845015, mean_eps: 0.964720\n","   802/20000: episode: 40, duration: 0.088s, episode steps:  12, steps per second: 136, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.442947, mae: 3.577190, mean_q: 6.763167, mean_eps: 0.964203\n","   849/20000: episode: 41, duration: 0.334s, episode steps:  47, steps per second: 141, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.596 [0.000, 1.000],  loss: 0.634820, mae: 4.009808, mean_q: 7.676380, mean_eps: 0.962875\n","   858/20000: episode: 42, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.375893, mae: 3.957843, mean_q: 7.753828, mean_eps: 0.961615\n","   873/20000: episode: 43, duration: 0.100s, episode steps:  15, steps per second: 150, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.601207, mae: 4.012418, mean_q: 7.731688, mean_eps: 0.961075\n","   896/20000: episode: 44, duration: 0.168s, episode steps:  23, steps per second: 137, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 0.672759, mae: 3.979785, mean_q: 7.681453, mean_eps: 0.960220\n","   937/20000: episode: 45, duration: 0.268s, episode steps:  41, steps per second: 153, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 0.690119, mae: 4.357040, mean_q: 8.377120, mean_eps: 0.958780\n","   953/20000: episode: 46, duration: 0.119s, episode steps:  16, steps per second: 134, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.683011, mae: 4.431876, mean_q: 8.681394, mean_eps: 0.957498\n","   974/20000: episode: 47, duration: 0.154s, episode steps:  21, steps per second: 136, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.619 [0.000, 1.000],  loss: 0.824165, mae: 4.391862, mean_q: 8.427019, mean_eps: 0.956665\n","   985/20000: episode: 48, duration: 0.085s, episode steps:  11, steps per second: 129, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.707930, mae: 4.446174, mean_q: 8.554301, mean_eps: 0.955945\n","  1000/20000: episode: 49, duration: 0.113s, episode steps:  15, steps per second: 133, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 0.778444, mae: 4.414580, mean_q: 8.521582, mean_eps: 0.955360\n","  1016/20000: episode: 50, duration: 0.117s, episode steps:  16, steps per second: 136, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 0.768969, mae: 4.832822, mean_q: 9.174948, mean_eps: 0.954662\n","  1035/20000: episode: 51, duration: 0.140s, episode steps:  19, steps per second: 136, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.316 [0.000, 1.000],  loss: 0.848097, mae: 4.898340, mean_q: 9.472322, mean_eps: 0.953875\n","  1045/20000: episode: 52, duration: 0.076s, episode steps:  10, steps per second: 131, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.981077, mae: 4.829797, mean_q: 9.306184, mean_eps: 0.953223\n","  1109/20000: episode: 53, duration: 0.439s, episode steps:  64, steps per second: 146, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.721775, mae: 4.920273, mean_q: 9.529521, mean_eps: 0.951557\n","  1128/20000: episode: 54, duration: 0.134s, episode steps:  19, steps per second: 141, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 0.840136, mae: 5.405436, mean_q: 10.646428, mean_eps: 0.949690\n","  1169/20000: episode: 55, duration: 0.280s, episode steps:  41, steps per second: 147, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 0.855122, mae: 5.357110, mean_q: 10.496205, mean_eps: 0.948340\n","  1205/20000: episode: 56, duration: 0.249s, episode steps:  36, steps per second: 145, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.840477, mae: 5.412712, mean_q: 10.587297, mean_eps: 0.946607\n","  1218/20000: episode: 57, duration: 0.095s, episode steps:  13, steps per second: 137, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 1.422514, mae: 5.854223, mean_q: 11.361834, mean_eps: 0.945505\n","  1231/20000: episode: 58, duration: 0.087s, episode steps:  13, steps per second: 150, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 1.110770, mae: 5.688921, mean_q: 11.061662, mean_eps: 0.944920\n","  1240/20000: episode: 59, duration: 0.072s, episode steps:   9, steps per second: 125, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.364381, mae: 5.728250, mean_q: 11.415771, mean_eps: 0.944425\n","  1266/20000: episode: 60, duration: 0.180s, episode steps:  26, steps per second: 145, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 1.131456, mae: 5.746573, mean_q: 11.164473, mean_eps: 0.943638\n","  1295/20000: episode: 61, duration: 0.205s, episode steps:  29, steps per second: 141, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  loss: 0.986975, mae: 5.769411, mean_q: 11.318855, mean_eps: 0.942400\n","  1313/20000: episode: 62, duration: 0.130s, episode steps:  18, steps per second: 139, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 1.330411, mae: 6.019375, mean_q: 11.633445, mean_eps: 0.941343\n","  1327/20000: episode: 63, duration: 0.102s, episode steps:  14, steps per second: 138, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 1.289835, mae: 6.198863, mean_q: 12.133609, mean_eps: 0.940622\n","  1384/20000: episode: 64, duration: 0.374s, episode steps:  57, steps per second: 152, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 0.992555, mae: 6.181401, mean_q: 12.138170, mean_eps: 0.939025\n","  1418/20000: episode: 65, duration: 0.242s, episode steps:  34, steps per second: 140, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 1.259909, mae: 6.348591, mean_q: 12.369645, mean_eps: 0.936978\n","  1433/20000: episode: 66, duration: 0.112s, episode steps:  15, steps per second: 133, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 1.122524, mae: 6.628917, mean_q: 13.113543, mean_eps: 0.935875\n","  1456/20000: episode: 67, duration: 0.159s, episode steps:  23, steps per second: 145, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.391 [0.000, 1.000],  loss: 1.275594, mae: 6.576728, mean_q: 12.974422, mean_eps: 0.935020\n","  1474/20000: episode: 68, duration: 0.130s, episode steps:  18, steps per second: 139, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 1.162732, mae: 6.570145, mean_q: 12.943627, mean_eps: 0.934098\n","  1491/20000: episode: 69, duration: 0.120s, episode steps:  17, steps per second: 142, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 0.973886, mae: 6.638914, mean_q: 13.166925, mean_eps: 0.933310\n","  1555/20000: episode: 70, duration: 0.431s, episode steps:  64, steps per second: 149, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 1.326486, mae: 6.957231, mean_q: 13.762113, mean_eps: 0.931488\n","  1575/20000: episode: 71, duration: 0.140s, episode steps:  20, steps per second: 143, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.940725, mae: 7.000608, mean_q: 13.980887, mean_eps: 0.929597\n","  1595/20000: episode: 72, duration: 0.136s, episode steps:  20, steps per second: 147, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.650 [0.000, 1.000],  loss: 1.199322, mae: 6.991153, mean_q: 13.891381, mean_eps: 0.928698\n","  1628/20000: episode: 73, duration: 0.228s, episode steps:  33, steps per second: 145, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.576 [0.000, 1.000],  loss: 1.255159, mae: 7.398315, mean_q: 14.685757, mean_eps: 0.927505\n","  1654/20000: episode: 74, duration: 0.171s, episode steps:  26, steps per second: 152, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.577 [0.000, 1.000],  loss: 1.555896, mae: 7.494808, mean_q: 14.910755, mean_eps: 0.926178\n","  1673/20000: episode: 75, duration: 0.133s, episode steps:  19, steps per second: 143, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 1.304400, mae: 7.500573, mean_q: 14.918170, mean_eps: 0.925165\n","  1684/20000: episode: 76, duration: 0.084s, episode steps:  11, steps per second: 131, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 1.409377, mae: 7.401936, mean_q: 14.645518, mean_eps: 0.924490\n","  1695/20000: episode: 77, duration: 0.079s, episode steps:  11, steps per second: 139, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 1.376376, mae: 7.669569, mean_q: 15.142382, mean_eps: 0.923995\n","  1708/20000: episode: 78, duration: 0.106s, episode steps:  13, steps per second: 122, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 1.553647, mae: 7.713393, mean_q: 15.126299, mean_eps: 0.923455\n","  1720/20000: episode: 79, duration: 0.082s, episode steps:  12, steps per second: 146, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 1.480179, mae: 7.858855, mean_q: 15.809884, mean_eps: 0.922892\n","  1740/20000: episode: 80, duration: 0.142s, episode steps:  20, steps per second: 141, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 1.666606, mae: 7.955219, mean_q: 15.768214, mean_eps: 0.922173\n","  1768/20000: episode: 81, duration: 0.200s, episode steps:  28, steps per second: 140, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.409259, mae: 8.062413, mean_q: 16.031712, mean_eps: 0.921092\n","  1795/20000: episode: 82, duration: 0.193s, episode steps:  27, steps per second: 140, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.370 [0.000, 1.000],  loss: 1.459847, mae: 7.970969, mean_q: 15.832794, mean_eps: 0.919855\n","  1807/20000: episode: 83, duration: 0.088s, episode steps:  12, steps per second: 137, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 2.000675, mae: 8.049862, mean_q: 15.825150, mean_eps: 0.918978\n","  1819/20000: episode: 84, duration: 0.096s, episode steps:  12, steps per second: 125, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 1.901927, mae: 8.520139, mean_q: 16.916403, mean_eps: 0.918437\n","  1840/20000: episode: 85, duration: 0.162s, episode steps:  21, steps per second: 130, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 1.832424, mae: 8.360785, mean_q: 16.628329, mean_eps: 0.917695\n","  1852/20000: episode: 86, duration: 0.089s, episode steps:  12, steps per second: 135, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 1.847980, mae: 8.417652, mean_q: 16.767034, mean_eps: 0.916953\n","  1878/20000: episode: 87, duration: 0.183s, episode steps:  26, steps per second: 142, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 1.671585, mae: 8.494715, mean_q: 16.849305, mean_eps: 0.916097\n","  1930/20000: episode: 88, duration: 0.374s, episode steps:  52, steps per second: 139, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.423 [0.000, 1.000],  loss: 1.666795, mae: 8.641736, mean_q: 17.116506, mean_eps: 0.914343\n","  1974/20000: episode: 89, duration: 0.296s, episode steps:  44, steps per second: 149, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.078280, mae: 8.746523, mean_q: 17.612404, mean_eps: 0.912182\n","  2005/20000: episode: 90, duration: 0.225s, episode steps:  31, steps per second: 138, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.419 [0.000, 1.000],  loss: 1.286157, mae: 8.702269, mean_q: 17.492739, mean_eps: 0.910495\n","  2019/20000: episode: 91, duration: 0.104s, episode steps:  14, steps per second: 134, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 2.491555, mae: 9.027807, mean_q: 17.891578, mean_eps: 0.909482\n","  2029/20000: episode: 92, duration: 0.071s, episode steps:  10, steps per second: 141, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 1.850029, mae: 9.196989, mean_q: 18.263384, mean_eps: 0.908942\n","  2048/20000: episode: 93, duration: 0.133s, episode steps:  19, steps per second: 142, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.316 [0.000, 1.000],  loss: 1.730985, mae: 9.054896, mean_q: 18.086478, mean_eps: 0.908290\n","  2059/20000: episode: 94, duration: 0.077s, episode steps:  11, steps per second: 143, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 2.136033, mae: 9.222439, mean_q: 18.294205, mean_eps: 0.907615\n","  2073/20000: episode: 95, duration: 0.093s, episode steps:  14, steps per second: 150, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 1.094078, mae: 9.122675, mean_q: 18.345336, mean_eps: 0.907053\n","  2101/20000: episode: 96, duration: 0.212s, episode steps:  28, steps per second: 132, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.393 [0.000, 1.000],  loss: 1.513868, mae: 9.112767, mean_q: 18.284619, mean_eps: 0.906108\n","  2110/20000: episode: 97, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 2.342398, mae: 9.302508, mean_q: 18.338253, mean_eps: 0.905275\n","  2144/20000: episode: 98, duration: 0.246s, episode steps:  34, steps per second: 138, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 1.597094, mae: 9.408376, mean_q: 18.922587, mean_eps: 0.904308\n","  2173/20000: episode: 99, duration: 0.210s, episode steps:  29, steps per second: 138, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.586 [0.000, 1.000],  loss: 1.211936, mae: 9.530076, mean_q: 19.189563, mean_eps: 0.902890\n","  2212/20000: episode: 100, duration: 0.297s, episode steps:  39, steps per second: 131, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 1.922388, mae: 9.447726, mean_q: 18.833631, mean_eps: 0.901360\n","  2231/20000: episode: 101, duration: 0.132s, episode steps:  19, steps per second: 144, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: 1.212910, mae: 9.786583, mean_q: 19.706383, mean_eps: 0.900055\n","  2266/20000: episode: 102, duration: 0.266s, episode steps:  35, steps per second: 132, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 1.295577, mae: 9.808791, mean_q: 19.812721, mean_eps: 0.898840\n","  2290/20000: episode: 103, duration: 0.165s, episode steps:  24, steps per second: 146, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.419953, mae: 9.743267, mean_q: 19.737752, mean_eps: 0.897513\n","  2336/20000: episode: 104, duration: 0.314s, episode steps:  46, steps per second: 146, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 1.466467, mae: 10.076168, mean_q: 20.411293, mean_eps: 0.895938\n","  2358/20000: episode: 105, duration: 0.155s, episode steps:  22, steps per second: 142, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.682 [0.000, 1.000],  loss: 1.106129, mae: 10.436302, mean_q: 21.268953, mean_eps: 0.894407\n","  2371/20000: episode: 106, duration: 0.093s, episode steps:  13, steps per second: 140, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 1.740440, mae: 10.205959, mean_q: 20.668705, mean_eps: 0.893620\n","  2404/20000: episode: 107, duration: 0.665s, episode steps:  33, steps per second:  50, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 1.637235, mae: 10.198585, mean_q: 20.705339, mean_eps: 0.892585\n","  2429/20000: episode: 108, duration: 0.336s, episode steps:  25, steps per second:  75, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 2.151290, mae: 10.636457, mean_q: 21.433946, mean_eps: 0.891280\n","  2447/20000: episode: 109, duration: 0.128s, episode steps:  18, steps per second: 141, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 1.746573, mae: 10.509035, mean_q: 21.291581, mean_eps: 0.890312\n","  2523/20000: episode: 110, duration: 0.513s, episode steps:  76, steps per second: 148, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.539 [0.000, 1.000],  loss: 1.702126, mae: 10.794311, mean_q: 21.921778, mean_eps: 0.888198\n","  2571/20000: episode: 111, duration: 0.320s, episode steps:  48, steps per second: 150, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 2.624569, mae: 11.264052, mean_q: 22.706097, mean_eps: 0.885407\n","  2586/20000: episode: 112, duration: 0.105s, episode steps:  15, steps per second: 143, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 1.031962, mae: 11.302425, mean_q: 23.004946, mean_eps: 0.883990\n","  2600/20000: episode: 113, duration: 0.113s, episode steps:  14, steps per second: 124, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 2.298399, mae: 11.062337, mean_q: 22.422963, mean_eps: 0.883337\n","  2644/20000: episode: 114, duration: 0.304s, episode steps:  44, steps per second: 145, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 1.611676, mae: 11.685906, mean_q: 23.742930, mean_eps: 0.882032\n","  2659/20000: episode: 115, duration: 0.107s, episode steps:  15, steps per second: 140, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 2.040771, mae: 11.523643, mean_q: 23.465418, mean_eps: 0.880705\n","  2684/20000: episode: 116, duration: 0.181s, episode steps:  25, steps per second: 138, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 2.471053, mae: 11.572212, mean_q: 23.338423, mean_eps: 0.879805\n","  2750/20000: episode: 117, duration: 0.450s, episode steps:  66, steps per second: 147, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 2.237632, mae: 12.046781, mean_q: 24.337660, mean_eps: 0.877757\n","  2801/20000: episode: 118, duration: 0.341s, episode steps:  51, steps per second: 150, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.549 [0.000, 1.000],  loss: 2.416305, mae: 12.156798, mean_q: 24.616777, mean_eps: 0.875125\n","  2837/20000: episode: 119, duration: 0.249s, episode steps:  36, steps per second: 144, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 2.563370, mae: 12.606017, mean_q: 25.499660, mean_eps: 0.873167\n","  2862/20000: episode: 120, duration: 0.167s, episode steps:  25, steps per second: 150, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 1.614110, mae: 12.813430, mean_q: 26.035220, mean_eps: 0.871795\n","  2874/20000: episode: 121, duration: 0.086s, episode steps:  12, steps per second: 139, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 2.836498, mae: 12.724619, mean_q: 25.747858, mean_eps: 0.870963\n","  2901/20000: episode: 122, duration: 0.179s, episode steps:  27, steps per second: 151, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 2.115755, mae: 12.662520, mean_q: 25.732996, mean_eps: 0.870085\n","  2936/20000: episode: 123, duration: 0.245s, episode steps:  35, steps per second: 143, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 1.980241, mae: 13.078770, mean_q: 26.473252, mean_eps: 0.868690\n","  2975/20000: episode: 124, duration: 0.266s, episode steps:  39, steps per second: 146, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 2.121182, mae: 13.200951, mean_q: 26.857502, mean_eps: 0.867025\n","  2996/20000: episode: 125, duration: 0.146s, episode steps:  21, steps per second: 144, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 2.156556, mae: 12.875456, mean_q: 26.232633, mean_eps: 0.865675\n","  3013/20000: episode: 126, duration: 0.118s, episode steps:  17, steps per second: 144, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 1.951774, mae: 13.247682, mean_q: 26.797743, mean_eps: 0.864820\n","  3056/20000: episode: 127, duration: 0.286s, episode steps:  43, steps per second: 150, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.558 [0.000, 1.000],  loss: 2.133812, mae: 13.389082, mean_q: 27.233932, mean_eps: 0.863470\n","  3080/20000: episode: 128, duration: 0.177s, episode steps:  24, steps per second: 136, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 3.072839, mae: 13.621656, mean_q: 27.542653, mean_eps: 0.861963\n","  3123/20000: episode: 129, duration: 0.303s, episode steps:  43, steps per second: 142, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 2.577063, mae: 13.797097, mean_q: 27.951216, mean_eps: 0.860455\n","  3135/20000: episode: 130, duration: 0.084s, episode steps:  12, steps per second: 144, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 1.339174, mae: 13.559020, mean_q: 27.627537, mean_eps: 0.859217\n","  3147/20000: episode: 131, duration: 0.089s, episode steps:  12, steps per second: 135, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 2.457021, mae: 13.832085, mean_q: 28.344560, mean_eps: 0.858678\n","  3161/20000: episode: 132, duration: 0.102s, episode steps:  14, steps per second: 137, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 2.330952, mae: 13.611314, mean_q: 27.773778, mean_eps: 0.858092\n","  3253/20000: episode: 133, duration: 0.614s, episode steps:  92, steps per second: 150, episode reward: 92.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 2.626528, mae: 14.247565, mean_q: 29.019861, mean_eps: 0.855708\n","  3301/20000: episode: 134, duration: 0.321s, episode steps:  48, steps per second: 149, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 2.203053, mae: 14.426352, mean_q: 29.476422, mean_eps: 0.852558\n","  3312/20000: episode: 135, duration: 0.079s, episode steps:  11, steps per second: 139, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 5.037133, mae: 14.933614, mean_q: 29.821311, mean_eps: 0.851230\n","  3326/20000: episode: 136, duration: 0.095s, episode steps:  14, steps per second: 147, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 4.173665, mae: 15.012671, mean_q: 30.485076, mean_eps: 0.850667\n","  3344/20000: episode: 137, duration: 0.124s, episode steps:  18, steps per second: 146, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 2.007124, mae: 14.908207, mean_q: 30.526445, mean_eps: 0.849947\n","  3380/20000: episode: 138, duration: 0.256s, episode steps:  36, steps per second: 141, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.999658, mae: 15.036355, mean_q: 30.600770, mean_eps: 0.848732\n","  3406/20000: episode: 139, duration: 0.182s, episode steps:  26, steps per second: 143, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 3.030867, mae: 15.127831, mean_q: 30.630228, mean_eps: 0.847338\n","  3425/20000: episode: 140, duration: 0.145s, episode steps:  19, steps per second: 131, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 2.275991, mae: 15.480490, mean_q: 31.567480, mean_eps: 0.846325\n","  3461/20000: episode: 141, duration: 0.247s, episode steps:  36, steps per second: 146, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.982388, mae: 15.288458, mean_q: 31.218361, mean_eps: 0.845087\n","  3481/20000: episode: 142, duration: 0.140s, episode steps:  20, steps per second: 143, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 3.298487, mae: 15.485860, mean_q: 31.239342, mean_eps: 0.843827\n","  3503/20000: episode: 143, duration: 0.165s, episode steps:  22, steps per second: 133, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  loss: 3.098350, mae: 15.646269, mean_q: 31.721669, mean_eps: 0.842883\n","  3531/20000: episode: 144, duration: 0.190s, episode steps:  28, steps per second: 147, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 2.964083, mae: 15.902151, mean_q: 32.360844, mean_eps: 0.841757\n","  3555/20000: episode: 145, duration: 0.185s, episode steps:  24, steps per second: 130, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.329681, mae: 15.931169, mean_q: 32.523183, mean_eps: 0.840587\n","  3577/20000: episode: 146, duration: 0.179s, episode steps:  22, steps per second: 123, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 2.790986, mae: 15.967028, mean_q: 32.632758, mean_eps: 0.839552\n","  3593/20000: episode: 147, duration: 0.119s, episode steps:  16, steps per second: 135, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 2.729450, mae: 15.793237, mean_q: 32.402844, mean_eps: 0.838697\n","  3611/20000: episode: 148, duration: 0.124s, episode steps:  18, steps per second: 145, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 3.666343, mae: 15.772872, mean_q: 32.384154, mean_eps: 0.837933\n","  3650/20000: episode: 149, duration: 0.272s, episode steps:  39, steps per second: 143, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.436 [0.000, 1.000],  loss: 2.708868, mae: 16.397654, mean_q: 33.565676, mean_eps: 0.836650\n","  3694/20000: episode: 150, duration: 0.308s, episode steps:  44, steps per second: 143, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 3.441055, mae: 16.300723, mean_q: 33.275664, mean_eps: 0.834783\n","  3708/20000: episode: 151, duration: 0.093s, episode steps:  14, steps per second: 151, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.786 [0.000, 1.000],  loss: 4.073313, mae: 16.701049, mean_q: 33.833659, mean_eps: 0.833478\n","  3764/20000: episode: 152, duration: 0.369s, episode steps:  56, steps per second: 152, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 2.879858, mae: 16.699326, mean_q: 34.168265, mean_eps: 0.831902\n","  3811/20000: episode: 153, duration: 0.324s, episode steps:  47, steps per second: 145, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 4.212059, mae: 16.837290, mean_q: 34.215011, mean_eps: 0.829585\n","  3894/20000: episode: 154, duration: 0.573s, episode steps:  83, steps per second: 145, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 2.945184, mae: 17.172824, mean_q: 35.163045, mean_eps: 0.826660\n","  3915/20000: episode: 155, duration: 0.142s, episode steps:  21, steps per second: 147, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 3.312919, mae: 17.526950, mean_q: 35.825679, mean_eps: 0.824320\n","  3926/20000: episode: 156, duration: 0.080s, episode steps:  11, steps per second: 137, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 1.761347, mae: 17.487449, mean_q: 36.041718, mean_eps: 0.823600\n","  4025/20000: episode: 157, duration: 0.679s, episode steps:  99, steps per second: 146, episode reward: 99.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 2.426687, mae: 17.780814, mean_q: 36.489340, mean_eps: 0.821125\n","  4038/20000: episode: 158, duration: 0.097s, episode steps:  13, steps per second: 133, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 2.375534, mae: 18.119498, mean_q: 37.435817, mean_eps: 0.818605\n","  4065/20000: episode: 159, duration: 0.188s, episode steps:  27, steps per second: 143, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 3.701121, mae: 18.128006, mean_q: 37.260634, mean_eps: 0.817705\n","  4150/20000: episode: 160, duration: 0.600s, episode steps:  85, steps per second: 142, episode reward: 85.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 3.059590, mae: 18.481260, mean_q: 37.991717, mean_eps: 0.815185\n","  4195/20000: episode: 161, duration: 0.410s, episode steps:  45, steps per second: 110, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.644 [0.000, 1.000],  loss: 2.724743, mae: 18.651927, mean_q: 38.352491, mean_eps: 0.812260\n","  4250/20000: episode: 162, duration: 0.565s, episode steps:  55, steps per second:  97, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 3.936222, mae: 18.674027, mean_q: 38.347218, mean_eps: 0.810010\n","  4285/20000: episode: 163, duration: 0.360s, episode steps:  35, steps per second:  97, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 2.856817, mae: 18.835660, mean_q: 38.850836, mean_eps: 0.807985\n","  4309/20000: episode: 164, duration: 0.255s, episode steps:  24, steps per second:  94, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 3.639519, mae: 19.130092, mean_q: 39.222592, mean_eps: 0.806657\n","  4328/20000: episode: 165, duration: 0.202s, episode steps:  19, steps per second:  94, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 3.120491, mae: 19.760417, mean_q: 40.573686, mean_eps: 0.805690\n","  4356/20000: episode: 166, duration: 0.299s, episode steps:  28, steps per second:  94, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 2.340157, mae: 19.261745, mean_q: 39.944289, mean_eps: 0.804632\n","  4376/20000: episode: 167, duration: 0.214s, episode steps:  20, steps per second:  94, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.358214, mae: 19.496048, mean_q: 39.995217, mean_eps: 0.803553\n","  4441/20000: episode: 168, duration: 0.690s, episode steps:  65, steps per second:  94, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 4.115010, mae: 19.631243, mean_q: 40.308120, mean_eps: 0.801640\n","  4469/20000: episode: 169, duration: 0.320s, episode steps:  28, steps per second:  87, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 3.218237, mae: 20.004378, mean_q: 41.073221, mean_eps: 0.799547\n","  4494/20000: episode: 170, duration: 0.271s, episode steps:  25, steps per second:  92, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 4.159233, mae: 20.076110, mean_q: 40.924980, mean_eps: 0.798355\n","  4522/20000: episode: 171, duration: 0.313s, episode steps:  28, steps per second:  89, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.393 [0.000, 1.000],  loss: 3.343431, mae: 20.262610, mean_q: 41.528575, mean_eps: 0.797163\n","  4652/20000: episode: 172, duration: 1.277s, episode steps: 130, steps per second: 102, episode reward: 130.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 3.994213, mae: 20.884995, mean_q: 42.734889, mean_eps: 0.793608\n","  4702/20000: episode: 173, duration: 0.325s, episode steps:  50, steps per second: 154, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 3.719602, mae: 21.237526, mean_q: 43.566091, mean_eps: 0.789557\n","  4782/20000: episode: 174, duration: 0.545s, episode steps:  80, steps per second: 147, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 2.969254, mae: 21.587708, mean_q: 44.515026, mean_eps: 0.786632\n","  4835/20000: episode: 175, duration: 0.372s, episode steps:  53, steps per second: 143, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.547 [0.000, 1.000],  loss: 5.245897, mae: 21.872164, mean_q: 44.867477, mean_eps: 0.783640\n","  4856/20000: episode: 176, duration: 0.142s, episode steps:  21, steps per second: 148, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 1.907555, mae: 22.356157, mean_q: 46.635871, mean_eps: 0.781975\n","  4870/20000: episode: 177, duration: 0.099s, episode steps:  14, steps per second: 141, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 4.931510, mae: 21.606796, mean_q: 44.944213, mean_eps: 0.781187\n","  4926/20000: episode: 178, duration: 0.396s, episode steps:  56, steps per second: 142, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.393 [0.000, 1.000],  loss: 4.170057, mae: 22.442981, mean_q: 46.123258, mean_eps: 0.779612\n","  4952/20000: episode: 179, duration: 0.176s, episode steps:  26, steps per second: 148, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.702581, mae: 22.362123, mean_q: 46.120833, mean_eps: 0.777767\n","  4971/20000: episode: 180, duration: 0.134s, episode steps:  19, steps per second: 141, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.316 [0.000, 1.000],  loss: 7.173570, mae: 22.557119, mean_q: 46.245132, mean_eps: 0.776755\n","  4985/20000: episode: 181, duration: 0.099s, episode steps:  14, steps per second: 142, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 3.838878, mae: 22.404245, mean_q: 46.331716, mean_eps: 0.776012\n","  5059/20000: episode: 182, duration: 0.517s, episode steps:  74, steps per second: 143, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.581 [0.000, 1.000],  loss: 5.263970, mae: 23.193402, mean_q: 47.698282, mean_eps: 0.774033\n","  5113/20000: episode: 183, duration: 0.360s, episode steps:  54, steps per second: 150, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 4.841364, mae: 23.406103, mean_q: 48.471469, mean_eps: 0.771152\n","  5125/20000: episode: 184, duration: 0.082s, episode steps:  12, steps per second: 147, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 6.683944, mae: 24.123744, mean_q: 49.629000, mean_eps: 0.769668\n","  5153/20000: episode: 185, duration: 0.197s, episode steps:  28, steps per second: 142, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 4.103125, mae: 23.957072, mean_q: 49.785201, mean_eps: 0.768767\n","  5200/20000: episode: 186, duration: 0.339s, episode steps:  47, steps per second: 139, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.426 [0.000, 1.000],  loss: 5.852740, mae: 23.711518, mean_q: 49.046492, mean_eps: 0.767080\n","  5225/20000: episode: 187, duration: 0.176s, episode steps:  25, steps per second: 142, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 6.336403, mae: 24.487466, mean_q: 50.143340, mean_eps: 0.765460\n","  5271/20000: episode: 188, duration: 0.310s, episode steps:  46, steps per second: 149, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 5.693123, mae: 24.558868, mean_q: 50.543771, mean_eps: 0.763862\n","  5294/20000: episode: 189, duration: 0.156s, episode steps:  23, steps per second: 148, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  loss: 4.398219, mae: 24.157107, mean_q: 49.599124, mean_eps: 0.762310\n","  5310/20000: episode: 190, duration: 0.116s, episode steps:  16, steps per second: 138, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 7.765174, mae: 24.077782, mean_q: 49.341080, mean_eps: 0.761432\n","  5384/20000: episode: 191, duration: 0.515s, episode steps:  74, steps per second: 144, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 5.403424, mae: 24.835421, mean_q: 51.268630, mean_eps: 0.759407\n","  5457/20000: episode: 192, duration: 0.506s, episode steps:  73, steps per second: 144, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 5.114884, mae: 25.144342, mean_q: 51.926663, mean_eps: 0.756100\n","  5550/20000: episode: 193, duration: 0.641s, episode steps:  93, steps per second: 145, episode reward: 93.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 5.645660, mae: 25.782896, mean_q: 53.140512, mean_eps: 0.752365\n","  5728/20000: episode: 194, duration: 1.165s, episode steps: 178, steps per second: 153, episode reward: 178.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 5.956421, mae: 26.638610, mean_q: 54.825312, mean_eps: 0.746267\n","  5772/20000: episode: 195, duration: 0.311s, episode steps:  44, steps per second: 141, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.700793, mae: 27.407146, mean_q: 56.275187, mean_eps: 0.741272\n","  5823/20000: episode: 196, duration: 0.382s, episode steps:  51, steps per second: 134, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 5.742659, mae: 27.318087, mean_q: 56.336146, mean_eps: 0.739135\n","  5862/20000: episode: 197, duration: 0.258s, episode steps:  39, steps per second: 151, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 6.039300, mae: 27.972069, mean_q: 57.502596, mean_eps: 0.737110\n","  5984/20000: episode: 198, duration: 0.861s, episode steps: 122, steps per second: 142, episode reward: 122.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 7.765660, mae: 28.359562, mean_q: 58.311481, mean_eps: 0.733488\n","  6009/20000: episode: 199, duration: 0.173s, episode steps:  25, steps per second: 144, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 7.041089, mae: 28.956914, mean_q: 59.436252, mean_eps: 0.730180\n","  6021/20000: episode: 200, duration: 0.088s, episode steps:  12, steps per second: 136, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.765494, mae: 29.012191, mean_q: 59.409984, mean_eps: 0.729348\n","  6042/20000: episode: 201, duration: 0.154s, episode steps:  21, steps per second: 136, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 7.131110, mae: 29.384244, mean_q: 60.536120, mean_eps: 0.728605\n","  6063/20000: episode: 202, duration: 0.175s, episode steps:  21, steps per second: 120, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 10.894996, mae: 29.411919, mean_q: 60.208167, mean_eps: 0.727660\n","  6103/20000: episode: 203, duration: 0.288s, episode steps:  40, steps per second: 139, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 9.992555, mae: 28.930883, mean_q: 59.523122, mean_eps: 0.726287\n","  6116/20000: episode: 204, duration: 0.093s, episode steps:  13, steps per second: 140, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 5.842889, mae: 30.555126, mean_q: 62.416446, mean_eps: 0.725095\n","  6128/20000: episode: 205, duration: 0.082s, episode steps:  12, steps per second: 146, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 14.444628, mae: 30.578292, mean_q: 62.049487, mean_eps: 0.724533\n","  6274/20000: episode: 206, duration: 0.994s, episode steps: 146, steps per second: 147, episode reward: 146.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 7.806505, mae: 29.999029, mean_q: 61.757698, mean_eps: 0.720977\n","  6351/20000: episode: 207, duration: 0.507s, episode steps:  77, steps per second: 152, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 6.287580, mae: 30.663064, mean_q: 63.286429, mean_eps: 0.715960\n","  6389/20000: episode: 208, duration: 0.281s, episode steps:  38, steps per second: 135, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 6.037268, mae: 31.070603, mean_q: 64.033499, mean_eps: 0.713373\n","  6441/20000: episode: 209, duration: 0.354s, episode steps:  52, steps per second: 147, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.558 [0.000, 1.000],  loss: 8.441668, mae: 31.311845, mean_q: 64.698825, mean_eps: 0.711348\n","  6491/20000: episode: 210, duration: 0.346s, episode steps:  50, steps per second: 145, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 6.697837, mae: 31.841690, mean_q: 65.434313, mean_eps: 0.709052\n","  6533/20000: episode: 211, duration: 0.306s, episode steps:  42, steps per second: 137, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 10.476448, mae: 32.618737, mean_q: 66.939560, mean_eps: 0.706983\n","  6574/20000: episode: 212, duration: 0.285s, episode steps:  41, steps per second: 144, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.439 [0.000, 1.000],  loss: 12.981961, mae: 32.306353, mean_q: 66.287568, mean_eps: 0.705115\n","  6591/20000: episode: 213, duration: 0.125s, episode steps:  17, steps per second: 136, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 10.576266, mae: 32.290861, mean_q: 66.172289, mean_eps: 0.703810\n","  6664/20000: episode: 214, duration: 0.499s, episode steps:  73, steps per second: 146, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 8.859461, mae: 32.437884, mean_q: 66.780850, mean_eps: 0.701785\n","  6806/20000: episode: 215, duration: 0.933s, episode steps: 142, steps per second: 152, episode reward: 142.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 8.065036, mae: 32.853835, mean_q: 68.051097, mean_eps: 0.696948\n","  6987/20000: episode: 216, duration: 1.204s, episode steps: 181, steps per second: 150, episode reward: 181.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 9.854356, mae: 34.333340, mean_q: 70.847208, mean_eps: 0.689680\n","  7004/20000: episode: 217, duration: 0.118s, episode steps:  17, steps per second: 144, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 9.652131, mae: 35.488410, mean_q: 73.004661, mean_eps: 0.685225\n","  7068/20000: episode: 218, duration: 0.422s, episode steps:  64, steps per second: 152, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.578 [0.000, 1.000],  loss: 11.230784, mae: 34.957854, mean_q: 71.994560, mean_eps: 0.683403\n","  7151/20000: episode: 219, duration: 0.557s, episode steps:  83, steps per second: 149, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 11.607850, mae: 35.200040, mean_q: 72.572327, mean_eps: 0.680095\n","  7259/20000: episode: 220, duration: 0.739s, episode steps: 108, steps per second: 146, episode reward: 108.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 10.200940, mae: 36.195959, mean_q: 74.550942, mean_eps: 0.675797\n","  7317/20000: episode: 221, duration: 0.411s, episode steps:  58, steps per second: 141, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.977267, mae: 37.022706, mean_q: 75.901809, mean_eps: 0.672063\n","  7338/20000: episode: 222, duration: 0.137s, episode steps:  21, steps per second: 153, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 5.869522, mae: 36.904926, mean_q: 76.097895, mean_eps: 0.670285\n","  7368/20000: episode: 223, duration: 0.209s, episode steps:  30, steps per second: 143, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 7.859544, mae: 36.886315, mean_q: 75.805105, mean_eps: 0.669138\n","  7396/20000: episode: 224, duration: 0.195s, episode steps:  28, steps per second: 144, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 13.076643, mae: 37.075555, mean_q: 76.261132, mean_eps: 0.667832\n","  7420/20000: episode: 225, duration: 0.186s, episode steps:  24, steps per second: 129, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 12.465855, mae: 37.050657, mean_q: 76.487668, mean_eps: 0.666663\n","  7479/20000: episode: 226, duration: 0.389s, episode steps:  59, steps per second: 152, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 12.283075, mae: 36.986865, mean_q: 76.462505, mean_eps: 0.664795\n","  7574/20000: episode: 227, duration: 0.648s, episode steps:  95, steps per second: 147, episode reward: 95.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 8.919801, mae: 37.969861, mean_q: 78.335584, mean_eps: 0.661330\n","  7614/20000: episode: 228, duration: 0.286s, episode steps:  40, steps per second: 140, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 11.205923, mae: 37.980513, mean_q: 78.403210, mean_eps: 0.658292\n","  7721/20000: episode: 229, duration: 0.733s, episode steps: 107, steps per second: 146, episode reward: 107.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 13.078552, mae: 39.244351, mean_q: 80.747110, mean_eps: 0.654985\n","  7750/20000: episode: 230, duration: 0.225s, episode steps:  29, steps per second: 129, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 7.972711, mae: 39.004570, mean_q: 80.808657, mean_eps: 0.651925\n","  7824/20000: episode: 231, duration: 0.508s, episode steps:  74, steps per second: 146, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 10.473712, mae: 39.616047, mean_q: 81.824125, mean_eps: 0.649608\n","  7922/20000: episode: 232, duration: 0.648s, episode steps:  98, steps per second: 151, episode reward: 98.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.551 [0.000, 1.000],  loss: 13.886779, mae: 40.372519, mean_q: 83.191867, mean_eps: 0.645737\n","  8035/20000: episode: 233, duration: 0.756s, episode steps: 113, steps per second: 149, episode reward: 113.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 13.209154, mae: 40.726580, mean_q: 83.723624, mean_eps: 0.640990\n","  8065/20000: episode: 234, duration: 0.203s, episode steps:  30, steps per second: 148, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.433 [0.000, 1.000],  loss: 15.255525, mae: 41.369869, mean_q: 84.949188, mean_eps: 0.637772\n","  8080/20000: episode: 235, duration: 0.111s, episode steps:  15, steps per second: 136, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 11.658949, mae: 40.445519, mean_q: 84.105442, mean_eps: 0.636760\n","  8136/20000: episode: 236, duration: 0.392s, episode steps:  56, steps per second: 143, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 15.026756, mae: 41.028430, mean_q: 84.237587, mean_eps: 0.635162\n","  8239/20000: episode: 237, duration: 0.698s, episode steps: 103, steps per second: 148, episode reward: 103.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 12.631957, mae: 41.467846, mean_q: 85.341870, mean_eps: 0.631585\n","  8254/20000: episode: 238, duration: 0.112s, episode steps:  15, steps per second: 134, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 15.757781, mae: 42.031214, mean_q: 86.440295, mean_eps: 0.628930\n","  8304/20000: episode: 239, duration: 0.350s, episode steps:  50, steps per second: 143, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  loss: 10.911626, mae: 41.502689, mean_q: 85.799440, mean_eps: 0.627467\n","  8320/20000: episode: 240, duration: 0.107s, episode steps:  16, steps per second: 150, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 21.340101, mae: 41.357659, mean_q: 84.779425, mean_eps: 0.625982\n","  8393/20000: episode: 241, duration: 0.489s, episode steps:  73, steps per second: 149, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 14.382595, mae: 42.474598, mean_q: 87.085892, mean_eps: 0.623980\n","  8482/20000: episode: 242, duration: 0.596s, episode steps:  89, steps per second: 149, episode reward: 89.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 14.291214, mae: 42.846792, mean_q: 87.938755, mean_eps: 0.620335\n","  8527/20000: episode: 243, duration: 0.312s, episode steps:  45, steps per second: 144, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 11.319204, mae: 42.642569, mean_q: 88.050127, mean_eps: 0.617320\n","  8551/20000: episode: 244, duration: 0.163s, episode steps:  24, steps per second: 147, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 24.475947, mae: 43.032303, mean_q: 88.661250, mean_eps: 0.615767\n","  8584/20000: episode: 245, duration: 0.230s, episode steps:  33, steps per second: 144, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.606 [0.000, 1.000],  loss: 10.357607, mae: 43.379578, mean_q: 88.866644, mean_eps: 0.614485\n","  8615/20000: episode: 246, duration: 0.224s, episode steps:  31, steps per second: 138, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 15.944944, mae: 43.966327, mean_q: 90.019267, mean_eps: 0.613045\n","  8657/20000: episode: 247, duration: 0.288s, episode steps:  42, steps per second: 146, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 14.539868, mae: 44.082707, mean_q: 90.839375, mean_eps: 0.611402\n","  8809/20000: episode: 248, duration: 1.299s, episode steps: 152, steps per second: 117, episode reward: 152.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 13.769894, mae: 44.404064, mean_q: 91.444575, mean_eps: 0.607038\n","  8938/20000: episode: 249, duration: 2.773s, episode steps: 129, steps per second:  47, episode reward: 129.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 15.115457, mae: 45.567578, mean_q: 93.555241, mean_eps: 0.600715\n","  9118/20000: episode: 250, duration: 2.926s, episode steps: 180, steps per second:  62, episode reward: 180.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 12.045508, mae: 46.818709, mean_q: 96.505177, mean_eps: 0.593762\n","  9219/20000: episode: 251, duration: 1.782s, episode steps: 101, steps per second:  57, episode reward: 101.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 12.234826, mae: 48.257364, mean_q: 99.060767, mean_eps: 0.587440\n","  9283/20000: episode: 252, duration: 0.765s, episode steps:  64, steps per second:  84, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 17.471403, mae: 48.517570, mean_q: 99.672641, mean_eps: 0.583727\n","  9403/20000: episode: 253, duration: 1.479s, episode steps: 120, steps per second:  81, episode reward: 120.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 16.780942, mae: 48.796266, mean_q: 100.280460, mean_eps: 0.579588\n","  9567/20000: episode: 254, duration: 1.850s, episode steps: 164, steps per second:  89, episode reward: 164.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 17.656031, mae: 49.830547, mean_q: 102.165962, mean_eps: 0.573197\n","  9598/20000: episode: 255, duration: 0.292s, episode steps:  31, steps per second: 106, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 12.275979, mae: 49.843695, mean_q: 102.881367, mean_eps: 0.568810\n","  9756/20000: episode: 256, duration: 1.811s, episode steps: 158, steps per second:  87, episode reward: 158.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 14.488260, mae: 50.842315, mean_q: 104.648989, mean_eps: 0.564558\n","  9819/20000: episode: 257, duration: 0.777s, episode steps:  63, steps per second:  81, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 17.328933, mae: 50.984039, mean_q: 104.739192, mean_eps: 0.559585\n","  9867/20000: episode: 258, duration: 0.589s, episode steps:  48, steps per second:  81, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 10.167016, mae: 51.284549, mean_q: 105.612626, mean_eps: 0.557087\n"," 10152/20000: episode: 259, duration: 3.059s, episode steps: 285, steps per second:  93, episode reward: 285.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 16.130006, mae: 52.194225, mean_q: 107.150677, mean_eps: 0.549595\n"," 10171/20000: episode: 260, duration: 0.188s, episode steps:  19, steps per second: 101, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 17.465002, mae: 53.157688, mean_q: 108.920318, mean_eps: 0.542755\n"," 10203/20000: episode: 261, duration: 0.274s, episode steps:  32, steps per second: 117, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 20.580370, mae: 52.683598, mean_q: 107.918715, mean_eps: 0.541608\n"," 10228/20000: episode: 262, duration: 0.185s, episode steps:  25, steps per second: 135, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 10.779125, mae: 54.164952, mean_q: 110.798157, mean_eps: 0.540325\n"," 10242/20000: episode: 263, duration: 0.114s, episode steps:  14, steps per second: 122, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 18.878331, mae: 53.036844, mean_q: 108.951846, mean_eps: 0.539447\n"," 10255/20000: episode: 264, duration: 0.088s, episode steps:  13, steps per second: 147, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 12.249474, mae: 53.642700, mean_q: 110.153702, mean_eps: 0.538840\n"," 10321/20000: episode: 265, duration: 0.517s, episode steps:  66, steps per second: 128, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 30.264461, mae: 53.233720, mean_q: 108.943800, mean_eps: 0.537063\n"," 10680/20000: episode: 266, duration: 3.613s, episode steps: 359, steps per second:  99, episode reward: 359.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 21.483214, mae: 54.295776, mean_q: 111.173538, mean_eps: 0.527500\n"," 10812/20000: episode: 267, duration: 1.002s, episode steps: 132, steps per second: 132, episode reward: 132.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 17.619811, mae: 55.089718, mean_q: 113.437063, mean_eps: 0.516453\n"," 10852/20000: episode: 268, duration: 0.580s, episode steps:  40, steps per second:  69, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 24.184472, mae: 55.863443, mean_q: 114.618498, mean_eps: 0.512582\n"," 10941/20000: episode: 269, duration: 1.063s, episode steps:  89, steps per second:  84, episode reward: 89.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.539 [0.000, 1.000],  loss: 17.033659, mae: 56.649937, mean_q: 115.606195, mean_eps: 0.509680\n"," 10980/20000: episode: 270, duration: 0.437s, episode steps:  39, steps per second:  89, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 21.710906, mae: 56.539586, mean_q: 115.476163, mean_eps: 0.506800\n"," 11157/20000: episode: 271, duration: 2.137s, episode steps: 177, steps per second:  83, episode reward: 177.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 27.158881, mae: 57.000893, mean_q: 116.529709, mean_eps: 0.501940\n"," 11318/20000: episode: 272, duration: 2.234s, episode steps: 161, steps per second:  72, episode reward: 161.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 19.130348, mae: 57.749130, mean_q: 118.530762, mean_eps: 0.494335\n"," 11527/20000: episode: 273, duration: 1.799s, episode steps: 209, steps per second: 116, episode reward: 209.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 25.742882, mae: 58.605883, mean_q: 119.943337, mean_eps: 0.486010\n"," 11705/20000: episode: 274, duration: 1.200s, episode steps: 178, steps per second: 148, episode reward: 178.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 19.036002, mae: 59.450120, mean_q: 121.712212, mean_eps: 0.477303\n"," 11808/20000: episode: 275, duration: 0.696s, episode steps: 103, steps per second: 148, episode reward: 103.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 18.008679, mae: 60.182140, mean_q: 123.246249, mean_eps: 0.470980\n"," 11867/20000: episode: 276, duration: 0.398s, episode steps:  59, steps per second: 148, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 24.375585, mae: 60.472983, mean_q: 123.445871, mean_eps: 0.467335\n"," 12016/20000: episode: 277, duration: 0.980s, episode steps: 149, steps per second: 152, episode reward: 149.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 22.179725, mae: 61.356728, mean_q: 125.300937, mean_eps: 0.462655\n"," 12157/20000: episode: 278, duration: 1.105s, episode steps: 141, steps per second: 128, episode reward: 141.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 22.194900, mae: 61.167730, mean_q: 124.864900, mean_eps: 0.456130\n"," 12203/20000: episode: 279, duration: 0.522s, episode steps:  46, steps per second:  88, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  loss: 19.001032, mae: 61.186781, mean_q: 124.926247, mean_eps: 0.451923\n"," 12400/20000: episode: 280, duration: 2.717s, episode steps: 197, steps per second:  73, episode reward: 197.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 22.146308, mae: 61.454932, mean_q: 125.385867, mean_eps: 0.446455\n"," 12610/20000: episode: 281, duration: 4.532s, episode steps: 210, steps per second:  46, episode reward: 210.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 25.940206, mae: 61.947339, mean_q: 126.337599, mean_eps: 0.437297\n"," 12640/20000: episode: 282, duration: 0.449s, episode steps:  30, steps per second:  67, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 22.435868, mae: 61.832613, mean_q: 126.367251, mean_eps: 0.431898\n"," 12752/20000: episode: 283, duration: 1.399s, episode steps: 112, steps per second:  80, episode reward: 112.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 16.026961, mae: 62.500143, mean_q: 127.515343, mean_eps: 0.428702\n"," 12822/20000: episode: 284, duration: 0.757s, episode steps:  70, steps per second:  92, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 17.855789, mae: 62.546158, mean_q: 127.910945, mean_eps: 0.424608\n"," 12901/20000: episode: 285, duration: 0.961s, episode steps:  79, steps per second:  82, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 19.816327, mae: 62.657675, mean_q: 128.262736, mean_eps: 0.421255\n"," 13068/20000: episode: 286, duration: 1.949s, episode steps: 167, steps per second:  86, episode reward: 167.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 21.070792, mae: 63.760498, mean_q: 130.656922, mean_eps: 0.415720\n"," 13182/20000: episode: 287, duration: 0.770s, episode steps: 114, steps per second: 148, episode reward: 114.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 21.503965, mae: 63.920116, mean_q: 130.711560, mean_eps: 0.409397\n"," 13355/20000: episode: 288, duration: 1.152s, episode steps: 173, steps per second: 150, episode reward: 173.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 18.063197, mae: 64.938959, mean_q: 133.043685, mean_eps: 0.402940\n"," 13553/20000: episode: 289, duration: 1.295s, episode steps: 198, steps per second: 153, episode reward: 198.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 22.634304, mae: 65.181650, mean_q: 133.360157, mean_eps: 0.394593\n"," 13764/20000: episode: 290, duration: 1.441s, episode steps: 211, steps per second: 146, episode reward: 211.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 20.123866, mae: 65.877812, mean_q: 134.295713, mean_eps: 0.385390\n"," 13944/20000: episode: 291, duration: 1.183s, episode steps: 180, steps per second: 152, episode reward: 180.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 23.427173, mae: 66.360061, mean_q: 135.254577, mean_eps: 0.376593\n"," 14105/20000: episode: 292, duration: 1.043s, episode steps: 161, steps per second: 154, episode reward: 161.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 26.816099, mae: 66.403663, mean_q: 135.465768, mean_eps: 0.368920\n"," 14267/20000: episode: 293, duration: 1.069s, episode steps: 162, steps per second: 152, episode reward: 162.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 20.444387, mae: 67.169804, mean_q: 136.992564, mean_eps: 0.361652\n"," 14440/20000: episode: 294, duration: 1.142s, episode steps: 173, steps per second: 151, episode reward: 173.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 29.269801, mae: 67.302196, mean_q: 137.180791, mean_eps: 0.354115\n"," 14688/20000: episode: 295, duration: 1.628s, episode steps: 248, steps per second: 152, episode reward: 248.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 18.612958, mae: 67.229404, mean_q: 137.266808, mean_eps: 0.344642\n"," 14909/20000: episode: 296, duration: 1.468s, episode steps: 221, steps per second: 151, episode reward: 221.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 24.711999, mae: 67.218048, mean_q: 137.068856, mean_eps: 0.334090\n"," 15048/20000: episode: 297, duration: 0.924s, episode steps: 139, steps per second: 150, episode reward: 139.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 25.070157, mae: 67.634029, mean_q: 137.807238, mean_eps: 0.325990\n"," 15294/20000: episode: 298, duration: 1.635s, episode steps: 246, steps per second: 150, episode reward: 246.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 22.246233, mae: 68.100954, mean_q: 138.758942, mean_eps: 0.317328\n"," 15456/20000: episode: 299, duration: 1.070s, episode steps: 162, steps per second: 151, episode reward: 162.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 19.224341, mae: 68.575615, mean_q: 139.723334, mean_eps: 0.308147\n"," 15714/20000: episode: 300, duration: 1.714s, episode steps: 258, steps per second: 151, episode reward: 258.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 22.154228, mae: 69.133016, mean_q: 140.791019, mean_eps: 0.298697\n"," 15938/20000: episode: 301, duration: 1.486s, episode steps: 224, steps per second: 151, episode reward: 224.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 17.109117, mae: 69.424776, mean_q: 141.775610, mean_eps: 0.287852\n"," 16121/20000: episode: 302, duration: 1.240s, episode steps: 183, steps per second: 148, episode reward: 183.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 21.906892, mae: 69.234225, mean_q: 141.032222, mean_eps: 0.278695\n"," 16278/20000: episode: 303, duration: 1.059s, episode steps: 157, steps per second: 148, episode reward: 157.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 17.226686, mae: 69.724689, mean_q: 142.317058, mean_eps: 0.271045\n"," 16444/20000: episode: 304, duration: 1.107s, episode steps: 166, steps per second: 150, episode reward: 166.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 23.115853, mae: 69.872049, mean_q: 142.386821, mean_eps: 0.263777\n"," 16651/20000: episode: 305, duration: 1.361s, episode steps: 207, steps per second: 152, episode reward: 207.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 22.578349, mae: 70.831908, mean_q: 144.252776, mean_eps: 0.255385\n"," 16801/20000: episode: 306, duration: 1.010s, episode steps: 150, steps per second: 149, episode reward: 150.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 30.248217, mae: 71.237603, mean_q: 145.149578, mean_eps: 0.247352\n"," 16959/20000: episode: 307, duration: 1.065s, episode steps: 158, steps per second: 148, episode reward: 158.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 16.352365, mae: 71.197861, mean_q: 144.961683, mean_eps: 0.240422\n"," 17141/20000: episode: 308, duration: 1.237s, episode steps: 182, steps per second: 147, episode reward: 182.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 24.917078, mae: 70.716902, mean_q: 144.112272, mean_eps: 0.232772\n"," 17327/20000: episode: 309, duration: 1.269s, episode steps: 186, steps per second: 147, episode reward: 186.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 33.796376, mae: 70.445593, mean_q: 142.970950, mean_eps: 0.224492\n"," 17404/20000: episode: 310, duration: 0.541s, episode steps:  77, steps per second: 142, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 15.821834, mae: 69.421852, mean_q: 141.606451, mean_eps: 0.218575\n"," 17576/20000: episode: 311, duration: 1.154s, episode steps: 172, steps per second: 149, episode reward: 172.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 24.783053, mae: 69.636321, mean_q: 142.303158, mean_eps: 0.212972\n"," 17816/20000: episode: 312, duration: 1.641s, episode steps: 240, steps per second: 146, episode reward: 240.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 22.918976, mae: 70.127506, mean_q: 142.579436, mean_eps: 0.203702\n"," 17981/20000: episode: 313, duration: 1.115s, episode steps: 165, steps per second: 148, episode reward: 165.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.539 [0.000, 1.000],  loss: 27.642185, mae: 70.568319, mean_q: 143.613983, mean_eps: 0.194590\n"," 18212/20000: episode: 314, duration: 1.546s, episode steps: 231, steps per second: 149, episode reward: 231.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 22.999417, mae: 70.708507, mean_q: 143.998079, mean_eps: 0.185680\n"," 18424/20000: episode: 315, duration: 1.429s, episode steps: 212, steps per second: 148, episode reward: 212.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 21.732371, mae: 70.958013, mean_q: 144.295146, mean_eps: 0.175712\n"," 18631/20000: episode: 316, duration: 1.353s, episode steps: 207, steps per second: 153, episode reward: 207.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 18.687852, mae: 71.324853, mean_q: 144.562979, mean_eps: 0.166285\n"," 18802/20000: episode: 317, duration: 1.137s, episode steps: 171, steps per second: 150, episode reward: 171.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 25.913652, mae: 69.688075, mean_q: 141.656446, mean_eps: 0.157780\n"," 19001/20000: episode: 318, duration: 1.348s, episode steps: 199, steps per second: 148, episode reward: 199.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 21.764836, mae: 69.923425, mean_q: 141.995446, mean_eps: 0.149455\n"," 19198/20000: episode: 319, duration: 1.312s, episode steps: 197, steps per second: 150, episode reward: 197.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 20.526124, mae: 70.552288, mean_q: 143.419587, mean_eps: 0.140545\n"," 19355/20000: episode: 320, duration: 1.070s, episode steps: 157, steps per second: 147, episode reward: 157.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 18.562799, mae: 70.330379, mean_q: 143.350764, mean_eps: 0.132580\n"," 19538/20000: episode: 321, duration: 1.188s, episode steps: 183, steps per second: 154, episode reward: 183.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 30.250705, mae: 69.737989, mean_q: 141.562853, mean_eps: 0.124930\n"," 19861/20000: episode: 322, duration: 2.136s, episode steps: 323, steps per second: 151, episode reward: 323.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 24.560245, mae: 69.095522, mean_q: 140.259048, mean_eps: 0.113545\n","done, took 161.176 seconds\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f17aa3148e0>"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["dqn.save_weights(f\"my_weights_cartpole.h5f\",overwrite=True)"],"metadata":{"id":"jmVAQj2NQsuo","executionInfo":{"status":"ok","timestamp":1674169571101,"user_tz":360,"elapsed":301,"user":{"displayName":"Austin Okoye","userId":"09681958171144788598"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# dqn.load_weights(f\"my_weights_cartpole.h5f\",overwrite=True)"],"metadata":{"id":"v18QtFM_Qsxb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dqn.test(env,nb_episodes=5,visualize=True)\n","env.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":435},"id":"1AtN0gZNQs0c","executionInfo":{"status":"error","timestamp":1674169649291,"user_tz":360,"elapsed":127,"user":{"displayName":"Austin Okoye","userId":"09681958171144788598"}},"outputId":"6d453d0b-b76c-433d-9468-e708fce9da58"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Testing for 5 episodes ...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n","See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n","  deprecation(\n"]},{"output_type":"error","ename":"error","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-8360abd639e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/rl/core.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, env, nb_episodes, action_repetition, callbacks, visualize, nb_max_episode_steps, nb_max_start_steps, start_step_policy, verbose)\u001b[0m\n\u001b[1;32m    350\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_action_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m                     \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/rl/callbacks.py\u001b[0m in \u001b[0;36mon_action_end\u001b[0;34m(self, action, logs)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'on_action_end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                 \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_action_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/rl/callbacks.py\u001b[0m in \u001b[0;36mon_action_end\u001b[0;34m(self, action, logs)\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_action_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0;34m\"\"\" Render environment at the end of each action \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m             )\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrender_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m     ) -> Optional[Union[RenderFrame, List[RenderFrame]]]:\n\u001b[1;32m    420\u001b[0m         \u001b[0;34m\"\"\"Renders the environment.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m             )\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrender_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;34m\"set `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             )\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m             )\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrender_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m     ) -> Optional[Union[RenderFrame, List[RenderFrame]]]:\n\u001b[1;32m    420\u001b[0m         \u001b[0;34m\"\"\"Renders the environment.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m             )\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrender_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/wrappers/env_checker.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_render_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m             )\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrender_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_renders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"human\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscreen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msurf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"human\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m             \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"render_fps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31merror\u001b[0m: video system not initialized"]}]},{"cell_type":"code","source":[],"metadata":{"id":"7b7LgAmVQs3a"},"execution_count":null,"outputs":[]}]}