{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPFXGPL30h+ouUQF+I1mP8z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jixB2yRSTHjY","executionInfo":{"status":"ok","timestamp":1674170089463,"user_tz":360,"elapsed":38868,"user":{"displayName":"Austin Okoye","userId":"09681958171144788598"}},"outputId":"b7a63c61-5887-49ed-d4e0-a54309457541"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following additional packages will be installed:\n","  freeglut3 libpython2-stdlib python2 python2-minimal\n","Suggested packages:\n","  python-tk python-numpy libgle3 python2-doc\n","The following NEW packages will be installed:\n","  freeglut3 libpython2-stdlib python-opengl python2 python2-minimal\n","0 upgraded, 5 newly installed, 0 to remove and 23 not upgraded.\n","Need to get 621 kB of archives.\n","After this operation, 6,059 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 python2-minimal amd64 2.7.17-2ubuntu4 [27.5 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 libpython2-stdlib amd64 2.7.17-2ubuntu4 [7,072 B]\n","Get:3 http://archive.ubuntu.com/ubuntu focal/universe amd64 python2 amd64 2.7.17-2ubuntu4 [26.5 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu focal/universe amd64 freeglut3 amd64 2.8.1-3 [73.6 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu focal/universe amd64 python-opengl all 3.1.0+dfsg-2build1 [486 kB]\n","Fetched 621 kB in 1s (820 kB/s)\n","Selecting previously unselected package python2-minimal.\n","(Reading database ... 129504 files and directories currently installed.)\n","Preparing to unpack .../python2-minimal_2.7.17-2ubuntu4_amd64.deb ...\n","Unpacking python2-minimal (2.7.17-2ubuntu4) ...\n","Selecting previously unselected package libpython2-stdlib:amd64.\n","Preparing to unpack .../libpython2-stdlib_2.7.17-2ubuntu4_amd64.deb ...\n","Unpacking libpython2-stdlib:amd64 (2.7.17-2ubuntu4) ...\n","Setting up python2-minimal (2.7.17-2ubuntu4) ...\n","Selecting previously unselected package python2.\n","(Reading database ... 129533 files and directories currently installed.)\n","Preparing to unpack .../python2_2.7.17-2ubuntu4_amd64.deb ...\n","Unpacking python2 (2.7.17-2ubuntu4) ...\n","Selecting previously unselected package freeglut3:amd64.\n","Preparing to unpack .../freeglut3_2.8.1-3_amd64.deb ...\n","Unpacking freeglut3:amd64 (2.8.1-3) ...\n","Selecting previously unselected package python-opengl.\n","Preparing to unpack .../python-opengl_3.1.0+dfsg-2build1_all.deb ...\n","Unpacking python-opengl (3.1.0+dfsg-2build1) ...\n","Setting up freeglut3:amd64 (2.8.1-3) ...\n","Setting up libpython2-stdlib:amd64 (2.7.17-2ubuntu4) ...\n","Setting up python2 (2.7.17-2ubuntu4) ...\n","Setting up python-opengl (3.1.0+dfsg-2build1) ...\n","Processing triggers for man-db (2.9.1-1) ...\n","Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following NEW packages will be installed:\n","  xvfb\n","0 upgraded, 1 newly installed, 0 to remove and 23 not upgraded.\n","Need to get 780 kB of archives.\n","After this operation, 2,271 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 xvfb amd64 2:1.20.13-1ubuntu1~20.04.5 [780 kB]\n","Fetched 780 kB in 1s (1,234 kB/s)\n","Selecting previously unselected package xvfb.\n","(Reading database ... 131921 files and directories currently installed.)\n","Preparing to unpack .../xvfb_2%3a1.20.13-1ubuntu1~20.04.5_amd64.deb ...\n","Unpacking xvfb (2:1.20.13-1ubuntu1~20.04.5) ...\n","Setting up xvfb (2:1.20.13-1ubuntu1~20.04.5) ...\n","Processing triggers for man-db (2.9.1-1) ...\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyvirtualdisplay\n","  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n","Installing collected packages: pyvirtualdisplay\n","Successfully installed pyvirtualdisplay-3.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting piglet\n","  Downloading piglet-1.0.0-py2.py3-none-any.whl (2.2 kB)\n","Collecting piglet-templates\n","  Downloading piglet_templates-1.3.0-py3-none-any.whl (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.5/67.5 KB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyparsing in /usr/local/lib/python3.8/dist-packages (from piglet-templates->piglet) (3.0.9)\n","Requirement already satisfied: astunparse in /usr/local/lib/python3.8/dist-packages (from piglet-templates->piglet) (1.6.3)\n","Requirement already satisfied: markupsafe in /usr/local/lib/python3.8/dist-packages (from piglet-templates->piglet) (2.0.1)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.8/dist-packages (from piglet-templates->piglet) (22.2.0)\n","Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.8/dist-packages (from astunparse->piglet-templates->piglet) (1.15.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse->piglet-templates->piglet) (0.38.4)\n","Installing collected packages: piglet-templates, piglet\n","Successfully installed piglet-1.0.0 piglet-templates-1.3.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gym[classic_control] in /usr/local/lib/python3.8/dist-packages (0.25.2)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym[classic_control]) (1.21.6)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym[classic_control]) (0.0.8)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym[classic_control]) (2.2.0)\n","Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym[classic_control]) (6.0.0)\n","Collecting pygame==2.1.0\n","  Downloading pygame-2.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym[classic_control]) (3.11.0)\n","Installing collected packages: pygame\n","Successfully installed pygame-2.1.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting keras-rl2\n","  Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (from keras-rl2) (2.9.2)\n","Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (1.12)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n","Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (2.9.1)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (1.14.1)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (3.19.6)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (1.21.6)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (2.2.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (21.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (57.4.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (15.0.6.1)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (0.29.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (0.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (4.4.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (1.3.0)\n","Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (2.9.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (1.51.1)\n","Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (2.9.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.38.4)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras-rl2) (0.6.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras-rl2) (3.4.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras-rl2) (1.0.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras-rl2) (2.16.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras-rl2) (1.8.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras-rl2) (0.4.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras-rl2) (2.25.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow->keras-rl2) (3.0.9)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->keras-rl2) (5.2.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->keras-rl2) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->keras-rl2) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow->keras-rl2) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow->keras-rl2) (6.0.0)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow->keras-rl2) (4.0.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow->keras-rl2) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow->keras-rl2) (2022.12.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow->keras-rl2) (2.10)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow->keras-rl2) (3.11.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->keras-rl2) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow->keras-rl2) (3.2.2)\n","Installing collected packages: keras-rl2\n","Successfully installed keras-rl2-1.0.5\n"]},{"output_type":"execute_result","data":{"text/plain":["<pyvirtualdisplay.display.Display at 0x7fb5f82fdc70>"]},"metadata":{},"execution_count":1}],"source":["!apt-get install python-opengl -y\n","!apt install xvfb -y\n","!pip install pyvirtualdisplay\n","!pip install piglet\n","!pip install gym[classic_control]\n","!pip install keras-rl2\n","\n","from pyvirtualdisplay import Display\n","Display().start()"]},{"cell_type":"code","source":["import numpy as np\n","import random\n","import gym\n","from IPython import display\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Activation, Flatten\n","from tensorflow.keras.optimizers import Adam\n","\n","from rl.agents.dqn import DQNAgent\n","%matplotlib inline"],"metadata":{"id":"tDEOMo7UTIUP","executionInfo":{"status":"ok","timestamp":1674170094780,"user_tz":360,"elapsed":5323,"user":{"displayName":"Austin Okoye","userId":"09681958171144788598"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["env_name = 'Acrobot-v1'\n","env = gym.make(env_name)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MywTw49tTIXS","executionInfo":{"status":"ok","timestamp":1674170155349,"user_tz":360,"elapsed":126,"user":{"displayName":"Austin Okoye","userId":"09681958171144788598"}},"outputId":"347cd069-4df9-4068-d99f-3c32d86ed95c"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n"]}]},{"cell_type":"code","source":["env.reset()\n","img = plt.imshow(env.render('rgb_array')) # only call this once\n","for _ in range(50):\n","  img.set_data(env.render('rgb_array')) # just update the data\n","  display.display(plt.gcf())\n","  display.clear_output(wait=True)\n","  action = env.action_space.sample()\n","  observation, reward, done, info = env.step(action)\n","  # if done:\n","  #   env.reset()\n","env.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":269},"id":"Dk8SOfQfTIaB","executionInfo":{"status":"ok","timestamp":1674170174703,"user_tz":360,"elapsed":9551,"user":{"displayName":"Austin Okoye","userId":"09681958171144788598"}},"outputId":"68849341-6f34-4ca4-fd96-11e1da420c3f"},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAUT0lEQVR4nO3de5CddX3H8fd3z2WvIZvLksQkJIHEIt6QbjGIrQIyRqqGVnSgjmRspplp6RQvMxbU2rFDR21njNBxaGmxgrWiRVtSijIxiRfaGrIRCJcIWTSYZEiyhGT3ZO+759s/zi/pSbLJPsmes+fy+7xmdvZ5fs9vd7/LHj55nuf3O8/P3B0RiVdDpQsQkcpSCIhETiEgEjmFgEjkFAIikVMIiESuLCFgZqvM7Hkz6zaz28rxM0SkNKzU8wTMLAW8AFwL7AW2ATe5+3Ml/UEiUhLlOBO4HOh291+6+wjwALC6DD9HREogXYbvuRDYU7S/F3jrmb5g7ty5vnTp0jKUIiLHbN++/RV37zi5vRwhkIiZrQPWAVxwwQV0dXVVqhSRKJjZSxO1l+NyYB+wuGh/UWg7gbvf4+6d7t7Z0XFKOInINClHCGwDVpjZMjPLAjcCG8rwc0SkBEp+OeDuY2b2p8CjQAr4mrs/W+qfIyKlUZZ7Au7+CPBIOb63iJSWZgyKRE4hIBI5hYBI5BQCIpFTCIhETiEgEjmFgEjkFAIikVMIiEROISASOYWASOQUAiKRUwiIRE4hIBI5hYBI5BQCIpFTCIhETiEgEjmFgEjkFAIikVMIiEROISASOYWASOQUAiKRUwiIRE4hIBI5hYBI5BQCIpFTCIhETiEgEjmFgEjkFAIikVMIiEROISASOYWASOQmDQEz+5qZHTSzZ4raZpvZRjPbFT7PCu1mZneZWbeZ7TCzy8pZvIhMXZIzga8Dq05quw3Y5O4rgE1hH+A9wIrwsQ64uzRliki5TBoC7v4T4NWTmlcD94Xt+4Dri9rv94KfAe1mtqBUxYpI6Z3rPYF57v5y2N4PzAvbC4E9Rf32hrZTmNk6M+sys66enp5zLENEpmrKNwbd3QE/h6+7x9073b2zo6NjqmWIyDk61xA4cOw0P3w+GNr3AYuL+i0KbSJSpc41BDYAa8L2GuChovabwyjBSqC36LJBRKpQerIOZvYt4J3AXDPbC/wl8EXgO2a2FngJ+FDo/ghwHdANDAAfLUPNIlJCk4aAu990mkPXTNDXgVumWpSITB/NGBSJnEJAJHIKAZHIKQREIqcQEImcQkAkcgoBkcgpBEQipxAQidykMwanw4EDB1i/fn2lyxCJkhVm+lZWZ2enb9u2rdJliNS1hoaG7e7eeXJ7VZwJAJhZpUsQiZLuCYhETiEgEjmFgEjkFAIikVMIiEROISASOYWASOQUAiKRUwiIRE4hIBI5hYBI5BQCIpFTCIhETiEgEjmFgEjkFAIikVMIiEROISASOYWASOQUAiKRUwiIRE4hIBK5SUPAzBab2RYze87MnjWzW0P7bDPbaGa7wudZod3M7C4z6zazHWZ2Wbl/CRE5d0nOBMaAT7r7JcBK4BYzuwS4Ddjk7iuATWEf4D3AivCxDri75FWLSMlMGgLu/rK7/zxs54CdwEJgNXBf6HYfcH3YXg3c7wU/A9rNbEHJKxeRkjirewJmthR4C7AVmOfuL4dD+4F5YXshsKfoy/aGNhGpQolDwMzagO8CH3P3vuJjXljQ8KwWNTSzdWbWZWZdPT09Z/OlIlJCiULAzDIUAuCb7v690Hzg2Gl++HwwtO8DFhd9+aLQdgJ3v8fdO929s6Oj41zrF5EpSjI6YMC9wE53/3LRoQ3AmrC9BnioqP3mMEqwEugtumwQkSqTZFXiK4GPAE+b2ZOh7dPAF4HvmNla4CXgQ+HYI8B1QDcwAHy0pBWLSElNGgLu/hhwunXDr5mgvwO3TLEuEZkmmjEoEjmFgEjkFAIikVMIiEROISASOYWASOQUAiKRUwiIRE4hIBI5hYBI5JK8d0AEAHdnfPxVhod34z5CJnM+2ewSIEXhfWZSixQCkkg+P8ShQ9/g4MG7GB7uxn2UdHouM2dex4IFnyWbXaYgqFEKAZlUPj/C/v1/y/79X8B98Hj72NgBDh36Z4aGfsGyZf+iIKhRuicgZ+Tu5HKbOXDgb04IgGL9/f/Lvn2fBsantzgpCYWAnNGI59l34O/I54+esV9v738yOPj0NFUlpaQQkDN6amCAHUd/PWm/fH6AfL5/GiqSUlMIyBk93NvLiJ/VM2SlxigE5LRG8nkeHxhgG52TPko6m11KNnvhtNQlpaUQkNPaMzLC1v4BHuQD7GL5aYNgiEZy7X9GJqM1ZmqRQkBOa1MuR258nP0s4K/5DC+wgnzR4yYdyNHG/fwhPW03Va5QmRLNE5AJjeTzbM7lwqCf8Sxv4ON8hVX8gE66aGaAX3EhP+DdHMr8Fne0zdEcgRqlEJAJHRkfZ0suV9RiHGQe93Mz3+AjAHg4K/iDGbNoT+ulVKv0l5NTuDvb+vvpHZ9o8o8d/58fIAVcNWOGXkg1TPcE5BQObM7lGE4wNNjc0MA72tp0KVDDFAJyisF8nu/39U3eEbispYUFmUyZK5JyUgjIKR4fGKB7eDhR399pa6O1QS+jWqa/npxie38/owkuBRqAVTNn6lKgxikE5AQj+Tz/1dubqO/yxkZWNDaWuSIpN4WAnODF4WG6BgYS9b28tZUODQ3WPIWAnOCHuRz9+fyk/RqA32tvL39BUnYKATluOJ9nU1/fpG8WAnhNJsNbW1t1P6AOKATkuJ6xMX589MwPDznmwsZGztfQYF1QCAhQmCW4tb+fgQSXAlC4FNDdgPqgEBAA8sCWXC7RA0TOa2jg6hkzdClQJxQCAkB/Ps/3Ew4Nnp/JsDibLXNFMl0mDQEzazKzx83sKTN71sw+H9qXmdlWM+s2s2+bWTa0N4b97nB8aXl/BZkqd+enR4+yZ3Q0Uf9V551HeypV5qpkuiQ5ExgGrnb3NwOXAqvMbCXwJWC9uy8HDgNrQ/+1wOHQvj70kyr35MBAolmCaeB97e26FKgjk4aAFxy7ZZwJHw5cDTwY2u8Drg/bq8M+4fg1pldMVRt15+GElwIXZLO8sbm5zBXJdEp0T8DMUmb2JHAQ2Ai8CBxx97HQZS+wMGwvBPYAhOO9wJwJvuc6M+sys66enp6p/RYyJc8NDbFjcOKFRU52SXMz8zRLsK4kCgF3H3f3S4FFwOXAxVP9we5+j7t3untnR0fHVL+dTMHGvr7EQ4MfaG+nQSd2deWsRgfc/QiwBbgCaDezY/8kLAL2he19wGKAcHwmcKgk1UrJjbrzWMIJQrNTKS5vbS1zRTLdkowOdJhZe9huBq4FdlIIgxtCtzXAQ2F7Q9gnHN/srtUrqtX+0VG29idbOeiCbJbletdg3UlycbcAuM/MUhRC4zvu/rCZPQc8YGZ3AE8A94b+9wLfMLNu4FXgxjLULSXy2NGjHBobm7wjsLq9nYwuBerOpCHg7juAt0zQ/ksK9wdObh8CPliS6qSsxtzZ1NdHkghoMeO39SzBuqQZgxHrGx/n0YTPEmxPp7mspaXMFUklKAQi5e48NTDAqxM+VvxU75oxgzbNEqxLCoGIbcnlEg0NHltbQPcD6pNCIFIjZzFLsD2V4qoZM8pckVSKQiBSTw4O8ouhoUR9V7a1aW2BOqYQiNTGvj4GE07f6GxpIau1BeqW/rIRGnXnxycsNnp6WTPeO3NmmSuSSlIIROjXIyM8kfCx4q9vauJ1TU1lrkgqSSEQoZ/mchxOODT4huZmWnQpUNf0143MqDsb+/pI8p5BrS0QB4VAZF4dG2NjwvsBi7JZrtRU4bqnEIiIu/PEwAC5hJcCb2ttZZYeIFL3FAIRcWBzLsdQgqFBo7DWoCKg/ikEIjJ0FrMEZzQ0cJ2WHY+Cgj4S7s7j/f28NDzERXRzDZtYwS6GaOLnXMYWruII7RTOAeBNLS0s1CzBKCgEIrJjIMe1/u/8MX/PnKInvr2bR1nNQ3yB23me3wCMNzc306qhwSjorxyJMaD3yAY+xp3M5RAGxz8acF7Ps3yWO5jPflLADbNm6VIgEgqBSPQM99A5eCczmPihogZczC+4gQdZ3pjVLMGIKAQiMTO/l/PHXzhjHwOu5L+5orWZ8zU0GA2FQCSSntgbzvs0KhAVhUAkRsgwzOSPCx+x85if0YrDMVEIROIllvIYb+dM04RGyPCv/vscTbYYkdQJhUAkXpNp4tHGj7OTiycMgjFS/AfXs4WrGNZaMVFRCERiVjrNSHoZf8Ed/JB30ct55DFGSfMy8/lH/oivcgvDNLFnZKTS5co00i3gSKSAtBkvsYTP8VcsZTfz2c8oGXazlAPMw8O/Cf/T38+fVLZcmUYKgYi8JpMBjFGy7OK17OK1E/Z7ZWwMd9cIQSR0ORCRdyR8bHh/Pq/7AhFRCESkI+EEoP2joxxJ+MwBqX0KgUiYGamEp/e7h4d5JeFKxVL7FAIReV1TU+LpwCO6HIiGQiAi8zMZZiRYVNSBfRomjIZCICJZM5KsKzxOYZhQ4qAQiIhB4jUFD4dhQql/CoGIpM14W1tbor658XE0PhCHxCFgZikze8LMHg77y8xsq5l1m9m3zSwb2hvDfnc4vrQ8pcvZMmBuwhuDvxoZYTCvdxLF4GzOBG4FdhbtfwlY7+7LgcPA2tC+Fjgc2teHflIFLOE9AYCdQ0MMKASikCgEzGwR8LvAP4V9A64GHgxd7gOuD9urwz7h+DWm+adVo7O1NdEDRJ3CkmVS/5KeCXwF+BQcX8JuDnDE3Y/NKNkLLAzbC4E9AOF4b+gvVeCCbJZsgkwec+fg6Og0VCSVNmkImNl7gYPuvr2UP9jM1plZl5l19fT0lPJbyxk0mSVK/sF8nm0Jly+X2pbk9XAl8H4z2w08QOEy4E6g3cyO3WVaBOwL2/uAxQDh+Ewoesh94O73uHunu3d2dHRM6ZeQ5DJmnJ9gmDAP9I2Pa5gwApOGgLvf7u6L3H0pcCOw2d0/DGwBbgjd1gAPhe0NYZ9wfLPrlVQ1WlMpLm1uTtS3d3z8jI8jk/owlXkCfw58wsy6KVzz3xva7wXmhPZPALdNrUQppTQwO+Ew4fNDQ5orEIGzeqiIu/8I+FHY/iVw+QR9hoAPlqA2KQMzI51wsObJwUHG3MlocKeuacZghK5sbU00XyDvzriu5OqeQiBCFzY20pDgX/chd17VcwXqnkIgQi0JVxvuGRvjmaGhMlcjlaYQiFBLQwNzEjxXYNSd/nxew4R1TiEQoY5MhuUJVx3u1bMG655CIELNZsxMcCYA8MzgYJmrkUpTCETIzBIP+3XpCUN1TyEQqd9O+HCRPP//rjGpTwqBSC1vnHyZcig8Yeio7gvUNYVApNoSDhO+ODzMbj15uK4pBCLVnk4nCoIhd4Y0RFjXFAKRWpLNMj/hk4dzuhyoawqBSJ2XSiV+zNgODRPWNYVApBog8TDhdj1hqK4pBCJlwNtaWxP1zbtr6nAdUwhEyii8mzCJV8fGdHOwjikEIpZkcVKAJwYHOaS3FNcthUCkzIzz02kaE9wX6M/nGdOZQN1SCETs4qYmZiQZIQhvKZb6pBCI2Jx0mmyCEBhz51kNE9YthUDEUgkfOjoKPK0QqFsKgYg1mvGbLS2J+uZBw4R1SiEQsYwZF2SzifoeHB3VGgR1SiEQMSP5uwl/cvQoQ7o5WJcUAhEzMxZls4leBP35vB4uUqfOagUiqT9vbG4mY8ZwuN7PADNTKWal08xKpbiwsZGLGhu5pLmZ5oRnDVJbFAKRu6ixkY/OmcOibJaLGhtZks0yJ51mdirFzHT6hBeIaTmyuqQQiNz8TIa7lyypdBlSQTq/E4mcQkAkcgoBkcgpBEQipxAQiZxCQCRyCgGRyCkERCKnEBCJnEJAJHJWDQ+KMLMc8Hyl6zgLc4FXKl1EQrVUK9RWvbVUK8ASd+84ubFa3jvwvLt3VrqIpMysq1bqraVaobbqraVaz0SXAyKRUwiIRK5aQuCeShdwlmqp3lqqFWqr3lqq9bSq4sagiFROtZwJiEiFVDwEzGyVmT1vZt1mdlsV1PM1MztoZs8Utc02s41mtit8nhXazczuCrXvMLPLKlDvYjPbYmbPmdmzZnZrtdZsZk1m9riZPRVq/XxoX2ZmW0NN3zazbGhvDPvd4fjS6aq1qOaUmT1hZg9Xe63nqqIhYGYp4KvAe4BLgJvM7JJK1gR8HVh1UtttwCZ3XwFsCvtQqHtF+FgH3D1NNRYbAz7p7pcAK4Fbwn/Daqx5GLja3d8MXAqsMrOVwJeA9e6+HDgMrA391wKHQ/v60G+63QrsLNqv5lrPjbtX7AO4Ani0aP924PZK1hTqWAo8U7T/PLAgbC+gMK8B4B+AmybqV8HaHwKurfaagRbg58BbKUy4SZ/8mgAeBa4I2+nQz6axxkUUAvRq4GEKSzVUZa1T+aj05cBCYE/R/t7QVm3mufvLYXs/MC9sV1X94RT0LcBWqrTmcHr9JHAQ2Ai8CBxx97EJ6jleazjeC8yZrlqBrwCfguNLLsyhems9Z5UOgZrjhaivuiEVM2sDvgt8zN37io9VU83uPu7ul1L4V/Zy4OIKlzQhM3svcNDdt1e6lnKrdAjsAxYX7S8KbdXmgJktAAifD4b2qqjfzDIUAuCb7v690FzVNbv7EWALhVPqdjM7NoW9uJ7jtYbjM4FD01TilcD7zWw38ACFS4I7q7TWKal0CGwDVoQ7rlngRmBDhWuayAZgTdheQ+G6+1j7zeGO+0qgt+gUfFpYYUWQe4Gd7v7lokNVV7OZdZhZe9hupnDvYieFMLjhNLUe+x1uADaHs5qyc/fb3X2Ruy+l8Lrc7O4frsZap6zSNyWA64AXKFwbfqYK6vkW8DIwSuGaby2Fa7tNwC7gh8Ds0NcojG68CDwNdFag3rdTONXfATwZPq6rxpqBNwFPhFqfAT4X2i8EHge6gX8DGkN7U9jvDscvrNBr4p3Aw7VQ67l8aMagSOQqfTkgIhWmEBCJnEJAJHIKAZHIKQREIqcQEImcQkAkcgoBkcj9H73BzKpuNAAAAAAAAElFTkSuQmCC\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["nb_actions = env.action_space.n\n","nb_actions"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ss5JZu-DTIcx","executionInfo":{"status":"ok","timestamp":1674170181847,"user_tz":360,"elapsed":127,"user":{"displayName":"Austin Okoye","userId":"09681958171144788598"}},"outputId":"71e3d5e4-7760-45fc-c40d-c3efe277387b"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["nb_obs = env.observation_space.shape\n","nb_obs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"imMTqMm2Tek2","executionInfo":{"status":"ok","timestamp":1674170184332,"user_tz":360,"elapsed":115,"user":{"displayName":"Austin Okoye","userId":"09681958171144788598"}},"outputId":"37f3279b-6b56-45b5-ed86-e907fa5def4d"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(6,)"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["model = Sequential()\n","\n","model.add(Flatten(input_shape=(1,)+nb_obs))\n","\n","model.add(Dense(64))\n","model.add(Activation('relu'))\n","\n","model.add(Dense(64))\n","model.add(Activation('relu'))\n","\n","model.add(Dense(64))\n","model.add(Activation('relu'))\n","\n","# Neurons == action_space\n","model.add(Dense(nb_actions))\n","model.add(Activation('linear'))"],"metadata":{"id":"XxZEBHjkTen2","executionInfo":{"status":"ok","timestamp":1674170297985,"user_tz":360,"elapsed":377,"user":{"displayName":"Austin Okoye","userId":"09681958171144788598"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["print(model.summary())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qXprh3IzTeq0","executionInfo":{"status":"ok","timestamp":1674170300073,"user_tz":360,"elapsed":115,"user":{"displayName":"Austin Okoye","userId":"09681958171144788598"}},"outputId":"87a08706-0e97-4ac8-f592-0eeb00521082"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," flatten (Flatten)           (None, 6)                 0         \n","                                                                 \n"," dense (Dense)               (None, 64)                448       \n","                                                                 \n"," activation (Activation)     (None, 64)                0         \n","                                                                 \n"," dense_1 (Dense)             (None, 64)                4160      \n","                                                                 \n"," activation_1 (Activation)   (None, 64)                0         \n","                                                                 \n"," dense_2 (Dense)             (None, 64)                4160      \n","                                                                 \n"," activation_2 (Activation)   (None, 64)                0         \n","                                                                 \n"," dense_3 (Dense)             (None, 3)                 195       \n","                                                                 \n"," activation_3 (Activation)   (None, 3)                 0         \n","                                                                 \n","=================================================================\n","Total params: 8,963\n","Trainable params: 8,963\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"]}]},{"cell_type":"code","source":["from rl.memory import SequentialMemory\n","from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy"],"metadata":{"id":"EMnWYQ2hTetl","executionInfo":{"status":"ok","timestamp":1674170361985,"user_tz":360,"elapsed":112,"user":{"displayName":"Austin Okoye","userId":"09681958171144788598"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["memory = SequentialMemory(limit=50000,window_length=1)"],"metadata":{"id":"fJMNenxdU-sq","executionInfo":{"status":"ok","timestamp":1674170363133,"user_tz":360,"elapsed":143,"user":{"displayName":"Austin Okoye","userId":"09681958171144788598"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps',\n","              value_max=1.0,\n","              value_min=0.1, \n","              value_test=0.05,\n","              nb_steps=150000\n","              )"],"metadata":{"id":"sTiNX3P1TIfi","executionInfo":{"status":"ok","timestamp":1674170406378,"user_tz":360,"elapsed":142,"user":{"displayName":"Austin Okoye","userId":"09681958171144788598"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["dqn = DQNAgent(model=model,nb_actions=nb_actions,\n","               memory=memory,nb_steps_warmup=1000,\n","                batch_size=32,gamma=0.99,\n","               target_model_update=1000,policy=policy)"],"metadata":{"id":"uX6KTBicTIiR","executionInfo":{"status":"ok","timestamp":1674170510334,"user_tz":360,"elapsed":112,"user":{"displayName":"Austin Okoye","userId":"09681958171144788598"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["dqn.compile(Adam(learning_rate=1e-3),metrics=['mae'])"],"metadata":{"id":"7okfjt9OTz4H","executionInfo":{"status":"ok","timestamp":1674170513042,"user_tz":360,"elapsed":869,"user":{"displayName":"Austin Okoye","userId":"09681958171144788598"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["dqn.fit(env,nb_steps=150000,visualize=False,verbose=2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AtFDpAHUTz7E","executionInfo":{"status":"ok","timestamp":1674171717658,"user_tz":360,"elapsed":1188050,"user":{"displayName":"Austin Okoye","userId":"09681958171144788598"}},"outputId":"0a8e3c9e-0bcc-43cd-a84d-a9245766485a"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Training for 150000 steps ...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates=self.state_updates,\n"]},{"output_type":"stream","name":"stdout","text":["    500/150000: episode: 1, duration: 0.602s, episode steps: 500, steps per second: 831, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.036 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n","   1000/150000: episode: 2, duration: 0.499s, episode steps: 500, steps per second: 1001, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.936 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates=self.state_updates,\n"]},{"output_type":"stream","name":"stdout","text":["   1500/150000: episode: 3, duration: 4.150s, episode steps: 500, steps per second: 120, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.996 [0.000, 2.000],  loss: 0.010887, mae: 0.482301, mean_q: -0.639197, mean_eps: 0.992500\n","   2000/150000: episode: 4, duration: 3.558s, episode steps: 500, steps per second: 141, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.048 [0.000, 2.000],  loss: 0.000462, mae: 0.473981, mean_q: -0.659870, mean_eps: 0.989503\n","   2500/150000: episode: 5, duration: 3.936s, episode steps: 500, steps per second: 127, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.022 [0.000, 2.000],  loss: 0.006998, mae: 1.212251, mean_q: -1.754884, mean_eps: 0.986503\n","   3000/150000: episode: 6, duration: 6.320s, episode steps: 500, steps per second:  79, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.022 [0.000, 2.000],  loss: 0.001146, mae: 1.201721, mean_q: -1.760188, mean_eps: 0.983503\n","   3500/150000: episode: 7, duration: 4.090s, episode steps: 500, steps per second: 122, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.010613, mae: 1.945366, mean_q: -2.853283, mean_eps: 0.980503\n","   4000/150000: episode: 8, duration: 3.554s, episode steps: 500, steps per second: 141, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.966 [0.000, 2.000],  loss: 0.004075, mae: 1.935889, mean_q: -2.857741, mean_eps: 0.977503\n","   4500/150000: episode: 9, duration: 3.566s, episode steps: 500, steps per second: 140, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.002 [0.000, 2.000],  loss: 0.014820, mae: 2.679125, mean_q: -3.945349, mean_eps: 0.974503\n","   5000/150000: episode: 10, duration: 3.549s, episode steps: 500, steps per second: 141, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.014 [0.000, 2.000],  loss: 0.008815, mae: 2.665439, mean_q: -3.941012, mean_eps: 0.971503\n","   5500/150000: episode: 11, duration: 3.515s, episode steps: 500, steps per second: 142, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.002 [0.000, 2.000],  loss: 0.020951, mae: 3.429589, mean_q: -5.055298, mean_eps: 0.968503\n","   6000/150000: episode: 12, duration: 4.450s, episode steps: 500, steps per second: 112, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.954 [0.000, 2.000],  loss: 0.017740, mae: 3.424803, mean_q: -5.052990, mean_eps: 0.965503\n","   6500/150000: episode: 13, duration: 3.885s, episode steps: 500, steps per second: 129, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.976 [0.000, 2.000],  loss: 0.035091, mae: 4.168402, mean_q: -6.132175, mean_eps: 0.962503\n","   7000/150000: episode: 14, duration: 3.520s, episode steps: 500, steps per second: 142, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.023029, mae: 4.163254, mean_q: -6.143105, mean_eps: 0.959503\n","   7500/150000: episode: 15, duration: 3.559s, episode steps: 500, steps per second: 140, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.030785, mae: 4.894474, mean_q: -7.222788, mean_eps: 0.956503\n","   8000/150000: episode: 16, duration: 3.543s, episode steps: 500, steps per second: 141, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.042024, mae: 4.887086, mean_q: -7.203026, mean_eps: 0.953503\n","   8500/150000: episode: 17, duration: 3.606s, episode steps: 500, steps per second: 139, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.006 [0.000, 2.000],  loss: 0.056973, mae: 5.612034, mean_q: -8.274220, mean_eps: 0.950503\n","   9000/150000: episode: 18, duration: 3.520s, episode steps: 500, steps per second: 142, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.018 [0.000, 2.000],  loss: 0.050013, mae: 5.600719, mean_q: -8.275184, mean_eps: 0.947503\n","   9500/150000: episode: 19, duration: 3.559s, episode steps: 500, steps per second: 140, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.024 [0.000, 2.000],  loss: 0.082563, mae: 6.206316, mean_q: -9.127523, mean_eps: 0.944503\n","  10000/150000: episode: 20, duration: 3.610s, episode steps: 500, steps per second: 138, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.976 [0.000, 2.000],  loss: 0.063355, mae: 6.200071, mean_q: -9.148412, mean_eps: 0.941503\n","  10500/150000: episode: 21, duration: 3.961s, episode steps: 500, steps per second: 126, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.073161, mae: 6.919660, mean_q: -10.222186, mean_eps: 0.938503\n","  11000/150000: episode: 22, duration: 4.833s, episode steps: 500, steps per second: 103, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.089380, mae: 6.908476, mean_q: -10.193014, mean_eps: 0.935503\n","  11500/150000: episode: 23, duration: 4.406s, episode steps: 500, steps per second: 113, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.115251, mae: 7.571089, mean_q: -11.159004, mean_eps: 0.932503\n","  12000/150000: episode: 24, duration: 3.548s, episode steps: 500, steps per second: 141, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.944 [0.000, 2.000],  loss: 0.091818, mae: 7.553452, mean_q: -11.141245, mean_eps: 0.929503\n","  12500/150000: episode: 25, duration: 3.672s, episode steps: 500, steps per second: 136, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.978 [0.000, 2.000],  loss: 0.139000, mae: 8.080774, mean_q: -11.892273, mean_eps: 0.926503\n","  13000/150000: episode: 26, duration: 3.623s, episode steps: 500, steps per second: 138, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.956 [0.000, 2.000],  loss: 0.105132, mae: 8.072349, mean_q: -11.899021, mean_eps: 0.923503\n","  13500/150000: episode: 27, duration: 3.571s, episode steps: 500, steps per second: 140, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.924 [0.000, 2.000],  loss: 0.150219, mae: 8.709394, mean_q: -12.832793, mean_eps: 0.920503\n","  14000/150000: episode: 28, duration: 3.675s, episode steps: 500, steps per second: 136, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.992 [0.000, 2.000],  loss: 0.148551, mae: 8.692577, mean_q: -12.819308, mean_eps: 0.917503\n","  14500/150000: episode: 29, duration: 4.290s, episode steps: 500, steps per second: 117, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.942 [0.000, 2.000],  loss: 0.177827, mae: 9.369969, mean_q: -13.799175, mean_eps: 0.914503\n","  15000/150000: episode: 30, duration: 3.654s, episode steps: 500, steps per second: 137, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.032 [0.000, 2.000],  loss: 0.219160, mae: 9.364916, mean_q: -13.804110, mean_eps: 0.911503\n","  15500/150000: episode: 31, duration: 3.602s, episode steps: 500, steps per second: 139, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.046 [0.000, 2.000],  loss: 0.165609, mae: 9.967735, mean_q: -14.711873, mean_eps: 0.908503\n","  15939/150000: episode: 32, duration: 3.207s, episode steps: 439, steps per second: 137, episode reward: -438.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.923 [0.000, 2.000],  loss: 0.177683, mae: 9.967179, mean_q: -14.712416, mean_eps: 0.905686\n","  16439/150000: episode: 33, duration: 3.635s, episode steps: 500, steps per second: 138, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.998 [0.000, 2.000],  loss: 0.201395, mae: 10.410136, mean_q: -15.347723, mean_eps: 0.902869\n","  16928/150000: episode: 34, duration: 3.568s, episode steps: 489, steps per second: 137, episode reward: -488.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.969 [0.000, 2.000],  loss: 0.238827, mae: 10.458496, mean_q: -15.425040, mean_eps: 0.899902\n","  17428/150000: episode: 35, duration: 3.670s, episode steps: 500, steps per second: 136, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.968 [0.000, 2.000],  loss: 0.234086, mae: 10.821093, mean_q: -15.959134, mean_eps: 0.896935\n","  17928/150000: episode: 36, duration: 3.716s, episode steps: 500, steps per second: 135, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.224775, mae: 10.811721, mean_q: -15.934778, mean_eps: 0.893935\n","  18391/150000: episode: 37, duration: 3.426s, episode steps: 463, steps per second: 135, episode reward: -462.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.924 [0.000, 2.000],  loss: 0.299419, mae: 11.277308, mean_q: -16.586451, mean_eps: 0.891046\n","  18869/150000: episode: 38, duration: 3.562s, episode steps: 478, steps per second: 134, episode reward: -477.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.979 [0.000, 2.000],  loss: 0.285530, mae: 11.348059, mean_q: -16.695251, mean_eps: 0.888223\n","  19369/150000: episode: 39, duration: 3.748s, episode steps: 500, steps per second: 133, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.974 [0.000, 2.000],  loss: 0.399257, mae: 11.639261, mean_q: -17.083239, mean_eps: 0.885289\n","  19763/150000: episode: 40, duration: 2.850s, episode steps: 394, steps per second: 138, episode reward: -393.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.033 [0.000, 2.000],  loss: 0.236245, mae: 11.729792, mean_q: -17.273205, mean_eps: 0.882607\n","  20263/150000: episode: 41, duration: 3.692s, episode steps: 500, steps per second: 135, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.996 [0.000, 2.000],  loss: 0.323372, mae: 11.825125, mean_q: -17.381541, mean_eps: 0.879925\n","  20763/150000: episode: 42, duration: 3.773s, episode steps: 500, steps per second: 133, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.072 [0.000, 2.000],  loss: 0.247531, mae: 11.870662, mean_q: -17.458425, mean_eps: 0.876925\n","  21263/150000: episode: 43, duration: 5.265s, episode steps: 500, steps per second:  95, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.958 [0.000, 2.000],  loss: 0.311125, mae: 12.126552, mean_q: -17.815627, mean_eps: 0.873925\n","  21726/150000: episode: 44, duration: 3.464s, episode steps: 463, steps per second: 134, episode reward: -462.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.004 [0.000, 2.000],  loss: 0.364096, mae: 12.289869, mean_q: -18.066136, mean_eps: 0.871036\n","  22226/150000: episode: 45, duration: 3.739s, episode steps: 500, steps per second: 134, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.377290, mae: 12.484866, mean_q: -18.351456, mean_eps: 0.868147\n","  22726/150000: episode: 46, duration: 3.714s, episode steps: 500, steps per second: 135, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.932 [0.000, 2.000],  loss: 0.361480, mae: 12.693503, mean_q: -18.682582, mean_eps: 0.865147\n","  23226/150000: episode: 47, duration: 3.670s, episode steps: 500, steps per second: 136, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.988 [0.000, 2.000],  loss: 0.395429, mae: 12.916385, mean_q: -18.985204, mean_eps: 0.862147\n","  23711/150000: episode: 48, duration: 3.551s, episode steps: 485, steps per second: 137, episode reward: -484.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.998 [0.000, 2.000],  loss: 0.420579, mae: 13.182737, mean_q: -19.395937, mean_eps: 0.859192\n","  24130/150000: episode: 49, duration: 3.087s, episode steps: 419, steps per second: 136, episode reward: -418.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.048 [0.000, 2.000],  loss: 0.323575, mae: 13.297398, mean_q: -19.585918, mean_eps: 0.856480\n","  24414/150000: episode: 50, duration: 2.150s, episode steps: 284, steps per second: 132, episode reward: -283.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 0.388089, mae: 13.547349, mean_q: -19.931615, mean_eps: 0.854371\n","  24813/150000: episode: 51, duration: 2.895s, episode steps: 399, steps per second: 138, episode reward: -398.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.020 [0.000, 2.000],  loss: 0.443099, mae: 13.562404, mean_q: -19.965675, mean_eps: 0.852322\n","  25313/150000: episode: 52, duration: 3.638s, episode steps: 500, steps per second: 137, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.964 [0.000, 2.000],  loss: 0.294410, mae: 13.692670, mean_q: -20.154335, mean_eps: 0.849625\n","  25813/150000: episode: 53, duration: 3.689s, episode steps: 500, steps per second: 136, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.982 [0.000, 2.000],  loss: 0.282080, mae: 13.835840, mean_q: -20.384684, mean_eps: 0.846625\n","  26313/150000: episode: 54, duration: 3.703s, episode steps: 500, steps per second: 135, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.958 [0.000, 2.000],  loss: 0.375270, mae: 14.139675, mean_q: -20.799054, mean_eps: 0.843625\n","  26612/150000: episode: 55, duration: 2.202s, episode steps: 299, steps per second: 136, episode reward: -298.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.913 [0.000, 2.000],  loss: 0.305943, mae: 14.283746, mean_q: -21.025975, mean_eps: 0.841228\n","  26889/150000: episode: 56, duration: 2.037s, episode steps: 277, steps per second: 136, episode reward: -276.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.982 [0.000, 2.000],  loss: 0.287947, mae: 14.256076, mean_q: -20.982253, mean_eps: 0.839500\n","  27389/150000: episode: 57, duration: 3.681s, episode steps: 500, steps per second: 136, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.312070, mae: 14.646353, mean_q: -21.547829, mean_eps: 0.837169\n","  27889/150000: episode: 58, duration: 3.742s, episode steps: 500, steps per second: 134, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.996 [0.000, 2.000],  loss: 0.393315, mae: 14.692709, mean_q: -21.609483, mean_eps: 0.834169\n","  28306/150000: episode: 59, duration: 3.153s, episode steps: 417, steps per second: 132, episode reward: -416.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.005 [0.000, 2.000],  loss: 0.470746, mae: 14.954420, mean_q: -21.963807, mean_eps: 0.831418\n","  28712/150000: episode: 60, duration: 4.195s, episode steps: 406, steps per second:  97, episode reward: -405.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.983 [0.000, 2.000],  loss: 0.491051, mae: 14.993888, mean_q: -22.045568, mean_eps: 0.828949\n","  29212/150000: episode: 61, duration: 4.070s, episode steps: 500, steps per second: 123, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.054 [0.000, 2.000],  loss: 0.589221, mae: 15.044068, mean_q: -22.084469, mean_eps: 0.826231\n","  29712/150000: episode: 62, duration: 3.758s, episode steps: 500, steps per second: 133, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.392415, mae: 15.139323, mean_q: -22.305526, mean_eps: 0.823231\n","  30049/150000: episode: 63, duration: 2.594s, episode steps: 337, steps per second: 130, episode reward: -336.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 0.455957, mae: 15.212304, mean_q: -22.395503, mean_eps: 0.820720\n","  30492/150000: episode: 64, duration: 3.366s, episode steps: 443, steps per second: 132, episode reward: -442.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.047 [0.000, 2.000],  loss: 0.329878, mae: 15.499468, mean_q: -22.848105, mean_eps: 0.818380\n","  30989/150000: episode: 65, duration: 3.794s, episode steps: 497, steps per second: 131, episode reward: -496.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.413102, mae: 15.477066, mean_q: -22.797929, mean_eps: 0.815560\n","  31359/150000: episode: 66, duration: 2.791s, episode steps: 370, steps per second: 133, episode reward: -369.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.884 [0.000, 2.000],  loss: 0.479720, mae: 15.898433, mean_q: -23.390250, mean_eps: 0.812959\n","  31675/150000: episode: 67, duration: 2.407s, episode steps: 316, steps per second: 131, episode reward: -315.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.082 [0.000, 2.000],  loss: 0.437991, mae: 15.861007, mean_q: -23.381538, mean_eps: 0.810901\n","  31892/150000: episode: 68, duration: 1.690s, episode steps: 217, steps per second: 128, episode reward: -216.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.166 [0.000, 2.000],  loss: 0.198822, mae: 15.890561, mean_q: -23.464218, mean_eps: 0.809302\n","  32177/150000: episode: 69, duration: 2.191s, episode steps: 285, steps per second: 130, episode reward: -284.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.895 [0.000, 2.000],  loss: 0.526526, mae: 16.190056, mean_q: -23.845690, mean_eps: 0.807796\n","  32517/150000: episode: 70, duration: 2.593s, episode steps: 340, steps per second: 131, episode reward: -339.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.053 [0.000, 2.000],  loss: 0.366055, mae: 16.317029, mean_q: -24.027839, mean_eps: 0.805921\n","  32916/150000: episode: 71, duration: 2.951s, episode steps: 399, steps per second: 135, episode reward: -398.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.955 [0.000, 2.000],  loss: 0.422016, mae: 16.293326, mean_q: -24.001393, mean_eps: 0.803704\n","  33299/150000: episode: 72, duration: 2.860s, episode steps: 383, steps per second: 134, episode reward: -382.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.945 [0.000, 2.000],  loss: 0.504384, mae: 16.444104, mean_q: -24.198899, mean_eps: 0.801358\n","  33726/150000: episode: 73, duration: 3.369s, episode steps: 427, steps per second: 127, episode reward: -426.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.970 [0.000, 2.000],  loss: 0.440898, mae: 16.437305, mean_q: -24.177564, mean_eps: 0.798928\n","  34226/150000: episode: 74, duration: 3.739s, episode steps: 500, steps per second: 134, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.982 [0.000, 2.000],  loss: 0.432985, mae: 16.706752, mean_q: -24.609033, mean_eps: 0.796147\n","  34696/150000: episode: 75, duration: 3.479s, episode steps: 470, steps per second: 135, episode reward: -469.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.055 [0.000, 2.000],  loss: 0.456561, mae: 16.942606, mean_q: -24.945186, mean_eps: 0.793237\n","  35196/150000: episode: 76, duration: 3.836s, episode steps: 500, steps per second: 130, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.998 [0.000, 2.000],  loss: 0.580347, mae: 17.075055, mean_q: -25.099913, mean_eps: 0.790327\n","  35524/150000: episode: 77, duration: 2.510s, episode steps: 328, steps per second: 131, episode reward: -327.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.957 [0.000, 2.000],  loss: 0.520245, mae: 17.275885, mean_q: -25.427355, mean_eps: 0.787843\n","  35913/150000: episode: 78, duration: 2.956s, episode steps: 389, steps per second: 132, episode reward: -388.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.967 [0.000, 2.000],  loss: 0.509430, mae: 17.167928, mean_q: -25.289847, mean_eps: 0.785692\n","  36213/150000: episode: 79, duration: 2.256s, episode steps: 300, steps per second: 133, episode reward: -299.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.943 [0.000, 2.000],  loss: 0.609097, mae: 17.550545, mean_q: -25.818543, mean_eps: 0.783625\n","  36704/150000: episode: 80, duration: 3.725s, episode steps: 491, steps per second: 132, episode reward: -490.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.951 [0.000, 2.000],  loss: 0.631880, mae: 17.673637, mean_q: -25.993438, mean_eps: 0.781252\n","  37022/150000: episode: 81, duration: 2.402s, episode steps: 318, steps per second: 132, episode reward: -317.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.019 [0.000, 2.000],  loss: 0.452285, mae: 17.652272, mean_q: -25.999272, mean_eps: 0.778825\n","  37382/150000: episode: 82, duration: 2.753s, episode steps: 360, steps per second: 131, episode reward: -359.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.858 [0.000, 2.000],  loss: 0.564454, mae: 17.983719, mean_q: -26.448592, mean_eps: 0.776791\n","  37739/150000: episode: 83, duration: 2.766s, episode steps: 357, steps per second: 129, episode reward: -356.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.964 [0.000, 2.000],  loss: 0.566717, mae: 17.882156, mean_q: -26.290497, mean_eps: 0.774640\n","  37949/150000: episode: 84, duration: 1.588s, episode steps: 210, steps per second: 132, episode reward: -209.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.943 [0.000, 2.000],  loss: 0.288834, mae: 17.949098, mean_q: -26.445502, mean_eps: 0.772939\n","  38176/150000: episode: 85, duration: 1.721s, episode steps: 227, steps per second: 132, episode reward: -226.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.943 [0.000, 2.000],  loss: 0.646410, mae: 18.090992, mean_q: -26.596023, mean_eps: 0.771628\n","  38635/150000: episode: 86, duration: 3.899s, episode steps: 459, steps per second: 118, episode reward: -458.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 0.621601, mae: 18.109155, mean_q: -26.638768, mean_eps: 0.769570\n","  38938/150000: episode: 87, duration: 3.450s, episode steps: 303, steps per second:  88, episode reward: -302.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.917 [0.000, 2.000],  loss: 0.610443, mae: 18.209171, mean_q: -26.792273, mean_eps: 0.767284\n","  39381/150000: episode: 88, duration: 3.354s, episode steps: 443, steps per second: 132, episode reward: -442.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 0.608501, mae: 18.372827, mean_q: -27.012569, mean_eps: 0.765046\n","  39675/150000: episode: 89, duration: 2.233s, episode steps: 294, steps per second: 132, episode reward: -293.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.959 [0.000, 2.000],  loss: 0.612542, mae: 18.431639, mean_q: -27.137844, mean_eps: 0.762835\n","  39911/150000: episode: 90, duration: 1.823s, episode steps: 236, steps per second: 129, episode reward: -235.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.898 [0.000, 2.000],  loss: 0.368907, mae: 18.320038, mean_q: -26.978973, mean_eps: 0.761245\n","  40256/150000: episode: 91, duration: 2.624s, episode steps: 345, steps per second: 131, episode reward: -344.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.910 [0.000, 2.000],  loss: 0.621914, mae: 18.575905, mean_q: -27.299254, mean_eps: 0.759502\n","  40729/150000: episode: 92, duration: 3.540s, episode steps: 473, steps per second: 134, episode reward: -472.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.032 [0.000, 2.000],  loss: 0.621101, mae: 18.681406, mean_q: -27.489508, mean_eps: 0.757048\n","  40955/150000: episode: 93, duration: 1.733s, episode steps: 226, steps per second: 130, episode reward: -225.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.956 [0.000, 2.000],  loss: 0.422805, mae: 18.624615, mean_q: -27.423438, mean_eps: 0.754951\n","  41376/150000: episode: 94, duration: 3.226s, episode steps: 421, steps per second: 130, episode reward: -420.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.029 [0.000, 2.000],  loss: 0.543119, mae: 18.680411, mean_q: -27.487777, mean_eps: 0.753010\n","  41675/150000: episode: 95, duration: 2.313s, episode steps: 299, steps per second: 129, episode reward: -298.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.064 [0.000, 2.000],  loss: 0.470780, mae: 18.700382, mean_q: -27.527156, mean_eps: 0.750850\n","  41881/150000: episode: 96, duration: 1.579s, episode steps: 206, steps per second: 130, episode reward: -205.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.005 [0.000, 2.000],  loss: 0.472203, mae: 18.645835, mean_q: -27.457966, mean_eps: 0.749335\n","  42283/150000: episode: 97, duration: 3.025s, episode steps: 402, steps per second: 133, episode reward: -401.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.985 [0.000, 2.000],  loss: 0.566115, mae: 18.828863, mean_q: -27.675623, mean_eps: 0.747511\n","  42546/150000: episode: 98, duration: 2.000s, episode steps: 263, steps per second: 131, episode reward: -262.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.985 [0.000, 2.000],  loss: 0.649545, mae: 18.965239, mean_q: -27.860879, mean_eps: 0.745516\n","  42851/150000: episode: 99, duration: 2.373s, episode steps: 305, steps per second: 129, episode reward: -304.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.600506, mae: 18.903171, mean_q: -27.791414, mean_eps: 0.743812\n","  43290/150000: episode: 100, duration: 3.366s, episode steps: 439, steps per second: 130, episode reward: -438.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.052 [0.000, 2.000],  loss: 0.563737, mae: 19.151379, mean_q: -28.147269, mean_eps: 0.741580\n","  43696/150000: episode: 101, duration: 3.099s, episode steps: 406, steps per second: 131, episode reward: -405.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.074 [0.000, 2.000],  loss: 0.614559, mae: 19.182373, mean_q: -28.169829, mean_eps: 0.739045\n","  43907/150000: episode: 102, duration: 1.625s, episode steps: 211, steps per second: 130, episode reward: -210.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 0.609222, mae: 19.081184, mean_q: -28.021476, mean_eps: 0.737194\n","  44210/150000: episode: 103, duration: 2.346s, episode steps: 303, steps per second: 129, episode reward: -302.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.515111, mae: 19.392522, mean_q: -28.481707, mean_eps: 0.735652\n","  44511/150000: episode: 104, duration: 2.275s, episode steps: 301, steps per second: 132, episode reward: -300.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.186 [0.000, 2.000],  loss: 0.483169, mae: 19.357464, mean_q: -28.431172, mean_eps: 0.733840\n","  44805/150000: episode: 105, duration: 2.215s, episode steps: 294, steps per second: 133, episode reward: -293.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.078 [0.000, 2.000],  loss: 0.265928, mae: 19.369108, mean_q: -28.481438, mean_eps: 0.732055\n","  45116/150000: episode: 106, duration: 2.357s, episode steps: 311, steps per second: 132, episode reward: -310.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 0.857827, mae: 19.511640, mean_q: -28.634067, mean_eps: 0.730240\n","  45562/150000: episode: 107, duration: 3.444s, episode steps: 446, steps per second: 129, episode reward: -445.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.928 [0.000, 2.000],  loss: 0.724275, mae: 19.559345, mean_q: -28.707635, mean_eps: 0.727969\n","  45916/150000: episode: 108, duration: 2.831s, episode steps: 354, steps per second: 125, episode reward: -353.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 0.660142, mae: 19.503355, mean_q: -28.655294, mean_eps: 0.725569\n","  46257/150000: episode: 109, duration: 3.825s, episode steps: 341, steps per second:  89, episode reward: -340.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.062 [0.000, 2.000],  loss: 0.702900, mae: 19.801160, mean_q: -29.110347, mean_eps: 0.723484\n","  46599/150000: episode: 110, duration: 2.834s, episode steps: 342, steps per second: 121, episode reward: -341.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.886 [0.000, 2.000],  loss: 0.774020, mae: 19.678906, mean_q: -28.878314, mean_eps: 0.721435\n","  46928/150000: episode: 111, duration: 2.569s, episode steps: 329, steps per second: 128, episode reward: -328.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.085 [0.000, 2.000],  loss: 0.592575, mae: 19.741221, mean_q: -29.006631, mean_eps: 0.719422\n","  47164/150000: episode: 112, duration: 1.823s, episode steps: 236, steps per second: 129, episode reward: -235.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.059 [0.000, 2.000],  loss: 0.719141, mae: 19.816216, mean_q: -29.070731, mean_eps: 0.717727\n","  47444/150000: episode: 113, duration: 2.157s, episode steps: 280, steps per second: 130, episode reward: -279.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.093 [0.000, 2.000],  loss: 0.614257, mae: 19.802928, mean_q: -29.078377, mean_eps: 0.716179\n","  47944/150000: episode: 114, duration: 3.815s, episode steps: 500, steps per second: 131, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.906 [0.000, 2.000],  loss: 0.671606, mae: 19.726995, mean_q: -28.938695, mean_eps: 0.713839\n","  48168/150000: episode: 115, duration: 1.737s, episode steps: 224, steps per second: 129, episode reward: -223.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.982 [0.000, 2.000],  loss: 0.671362, mae: 19.939925, mean_q: -29.255930, mean_eps: 0.711667\n","  48378/150000: episode: 116, duration: 1.585s, episode steps: 210, steps per second: 133, episode reward: -209.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.890 [0.000, 2.000],  loss: 0.696820, mae: 19.917212, mean_q: -29.186629, mean_eps: 0.710365\n","  48595/150000: episode: 117, duration: 1.664s, episode steps: 217, steps per second: 130, episode reward: -216.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 0.627952, mae: 19.967046, mean_q: -29.274121, mean_eps: 0.709084\n","  48812/150000: episode: 118, duration: 1.658s, episode steps: 217, steps per second: 131, episode reward: -216.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.903 [0.000, 2.000],  loss: 0.583619, mae: 19.985404, mean_q: -29.291300, mean_eps: 0.707782\n","  49257/150000: episode: 119, duration: 3.464s, episode steps: 445, steps per second: 128, episode reward: -444.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.906 [0.000, 2.000],  loss: 0.769049, mae: 19.842547, mean_q: -29.052623, mean_eps: 0.705796\n","  49541/150000: episode: 120, duration: 2.213s, episode steps: 284, steps per second: 128, episode reward: -283.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.887 [0.000, 2.000],  loss: 0.332501, mae: 19.642274, mean_q: -28.802751, mean_eps: 0.703609\n","  49959/150000: episode: 121, duration: 3.238s, episode steps: 418, steps per second: 129, episode reward: -417.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.842 [0.000, 2.000],  loss: 0.554299, mae: 19.606029, mean_q: -28.715203, mean_eps: 0.701503\n","  50120/150000: episode: 122, duration: 1.267s, episode steps: 161, steps per second: 127, episode reward: -160.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.882 [0.000, 2.000],  loss: 0.490882, mae: 19.805905, mean_q: -29.006372, mean_eps: 0.699766\n","  50372/150000: episode: 123, duration: 1.953s, episode steps: 252, steps per second: 129, episode reward: -251.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.829 [0.000, 2.000],  loss: 0.930115, mae: 19.781631, mean_q: -28.935739, mean_eps: 0.698527\n","  50660/150000: episode: 124, duration: 2.223s, episode steps: 288, steps per second: 130, episode reward: -287.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.851 [0.000, 2.000],  loss: 0.525906, mae: 19.845747, mean_q: -29.119389, mean_eps: 0.696907\n","  50873/150000: episode: 125, duration: 1.677s, episode steps: 213, steps per second: 127, episode reward: -212.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.911 [0.000, 2.000],  loss: 0.617031, mae: 19.661700, mean_q: -28.839735, mean_eps: 0.695404\n","  51217/150000: episode: 126, duration: 2.704s, episode steps: 344, steps per second: 127, episode reward: -343.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.006 [0.000, 2.000],  loss: 0.854943, mae: 19.807624, mean_q: -29.021514, mean_eps: 0.693733\n","  51595/150000: episode: 127, duration: 2.978s, episode steps: 378, steps per second: 127, episode reward: -377.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.905 [0.000, 2.000],  loss: 0.570840, mae: 19.883997, mean_q: -29.197254, mean_eps: 0.691567\n","  52030/150000: episode: 128, duration: 3.457s, episode steps: 435, steps per second: 126, episode reward: -434.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.864 [0.000, 2.000],  loss: 0.603230, mae: 19.903268, mean_q: -29.225330, mean_eps: 0.689128\n","  52333/150000: episode: 129, duration: 2.367s, episode steps: 303, steps per second: 128, episode reward: -302.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.842 [0.000, 2.000],  loss: 0.565203, mae: 19.990831, mean_q: -29.356993, mean_eps: 0.686914\n","  52570/150000: episode: 130, duration: 1.807s, episode steps: 237, steps per second: 131, episode reward: -236.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.577662, mae: 19.924604, mean_q: -29.266407, mean_eps: 0.685294\n","  52843/150000: episode: 131, duration: 2.175s, episode steps: 273, steps per second: 126, episode reward: -272.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.956 [0.000, 2.000],  loss: 0.388367, mae: 19.981189, mean_q: -29.379411, mean_eps: 0.683764\n","  53123/150000: episode: 132, duration: 2.191s, episode steps: 280, steps per second: 128, episode reward: -279.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.907 [0.000, 2.000],  loss: 0.845779, mae: 19.998957, mean_q: -29.300885, mean_eps: 0.682105\n","  53434/150000: episode: 133, duration: 2.435s, episode steps: 311, steps per second: 128, episode reward: -310.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.061 [0.000, 2.000],  loss: 0.579267, mae: 19.961528, mean_q: -29.260765, mean_eps: 0.680332\n","  53608/150000: episode: 134, duration: 1.338s, episode steps: 174, steps per second: 130, episode reward: -173.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.069 [0.000, 2.000],  loss: 0.763549, mae: 19.940687, mean_q: -29.240321, mean_eps: 0.678877\n","  53840/150000: episode: 135, duration: 1.768s, episode steps: 232, steps per second: 131, episode reward: -231.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.918 [0.000, 2.000],  loss: 0.658339, mae: 20.039670, mean_q: -29.429677, mean_eps: 0.677659\n","  54116/150000: episode: 136, duration: 2.079s, episode steps: 276, steps per second: 133, episode reward: -275.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.946 [0.000, 2.000],  loss: 0.594272, mae: 20.035019, mean_q: -29.360262, mean_eps: 0.676135\n","  54372/150000: episode: 137, duration: 1.946s, episode steps: 256, steps per second: 132, episode reward: -255.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 0.408873, mae: 20.199154, mean_q: -29.631677, mean_eps: 0.674539\n","  54689/150000: episode: 138, duration: 2.459s, episode steps: 317, steps per second: 129, episode reward: -316.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.953 [0.000, 2.000],  loss: 0.849363, mae: 20.121664, mean_q: -29.422660, mean_eps: 0.672820\n","  54900/150000: episode: 139, duration: 1.645s, episode steps: 211, steps per second: 128, episode reward: -210.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.185 [0.000, 2.000],  loss: 0.633669, mae: 20.117200, mean_q: -29.488789, mean_eps: 0.671236\n","  55064/150000: episode: 140, duration: 1.270s, episode steps: 164, steps per second: 129, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.976 [0.000, 2.000],  loss: 0.677785, mae: 20.171528, mean_q: -29.507790, mean_eps: 0.670111\n","  55298/150000: episode: 141, duration: 1.849s, episode steps: 234, steps per second: 127, episode reward: -233.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.115 [0.000, 2.000],  loss: 0.542883, mae: 20.332704, mean_q: -29.774821, mean_eps: 0.668917\n","  55569/150000: episode: 142, duration: 2.089s, episode steps: 271, steps per second: 130, episode reward: -270.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.801244, mae: 20.359303, mean_q: -29.781472, mean_eps: 0.667402\n","  55853/150000: episode: 143, duration: 2.636s, episode steps: 284, steps per second: 108, episode reward: -283.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.123 [0.000, 2.000],  loss: 0.685752, mae: 20.353585, mean_q: -29.796462, mean_eps: 0.665737\n","  56053/150000: episode: 144, duration: 2.259s, episode steps: 200, steps per second:  89, episode reward: -199.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.100 [0.000, 2.000],  loss: 0.490200, mae: 20.344388, mean_q: -29.803510, mean_eps: 0.664285\n","  56285/150000: episode: 145, duration: 2.181s, episode steps: 232, steps per second: 106, episode reward: -231.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.082 [0.000, 2.000],  loss: 0.739369, mae: 20.498500, mean_q: -30.001169, mean_eps: 0.662989\n","  56525/150000: episode: 146, duration: 1.875s, episode steps: 240, steps per second: 128, episode reward: -239.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.008 [0.000, 2.000],  loss: 0.674563, mae: 20.386229, mean_q: -29.849194, mean_eps: 0.661573\n","  56755/150000: episode: 147, duration: 1.794s, episode steps: 230, steps per second: 128, episode reward: -229.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.283 [0.000, 2.000],  loss: 0.576282, mae: 20.442356, mean_q: -29.953454, mean_eps: 0.660163\n","  57034/150000: episode: 148, duration: 2.209s, episode steps: 279, steps per second: 126, episode reward: -278.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.097 [0.000, 2.000],  loss: 0.426659, mae: 20.382128, mean_q: -29.882174, mean_eps: 0.658636\n","  57328/150000: episode: 149, duration: 2.264s, episode steps: 294, steps per second: 130, episode reward: -293.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.020 [0.000, 2.000],  loss: 0.813776, mae: 20.627219, mean_q: -30.183316, mean_eps: 0.656917\n","  57637/150000: episode: 150, duration: 2.384s, episode steps: 309, steps per second: 130, episode reward: -308.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.129 [0.000, 2.000],  loss: 0.619485, mae: 20.630711, mean_q: -30.210535, mean_eps: 0.655108\n","  57886/150000: episode: 151, duration: 1.933s, episode steps: 249, steps per second: 129, episode reward: -248.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.036 [0.000, 2.000],  loss: 0.535545, mae: 20.571181, mean_q: -30.139316, mean_eps: 0.653434\n","  58238/150000: episode: 152, duration: 2.706s, episode steps: 352, steps per second: 130, episode reward: -351.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.125 [0.000, 2.000],  loss: 0.738227, mae: 20.833323, mean_q: -30.480802, mean_eps: 0.651631\n","  58433/150000: episode: 153, duration: 1.532s, episode steps: 195, steps per second: 127, episode reward: -194.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.041 [0.000, 2.000],  loss: 0.536303, mae: 20.917424, mean_q: -30.647696, mean_eps: 0.649990\n","  58672/150000: episode: 154, duration: 1.879s, episode steps: 239, steps per second: 127, episode reward: -238.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.538295, mae: 20.852134, mean_q: -30.529037, mean_eps: 0.648688\n","  58875/150000: episode: 155, duration: 1.589s, episode steps: 203, steps per second: 128, episode reward: -202.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.034 [0.000, 2.000],  loss: 0.453975, mae: 20.825157, mean_q: -30.524325, mean_eps: 0.647362\n","  59068/150000: episode: 156, duration: 1.556s, episode steps: 193, steps per second: 124, episode reward: -192.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.041 [0.000, 2.000],  loss: 0.797868, mae: 20.814441, mean_q: -30.449354, mean_eps: 0.646174\n","  59275/150000: episode: 157, duration: 1.614s, episode steps: 207, steps per second: 128, episode reward: -206.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.072 [0.000, 2.000],  loss: 0.939555, mae: 20.611067, mean_q: -30.053159, mean_eps: 0.644974\n","  59592/150000: episode: 158, duration: 2.478s, episode steps: 317, steps per second: 128, episode reward: -316.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.981 [0.000, 2.000],  loss: 0.627214, mae: 20.607893, mean_q: -30.114240, mean_eps: 0.643402\n","  59756/150000: episode: 159, duration: 1.269s, episode steps: 164, steps per second: 129, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.957 [0.000, 2.000],  loss: 0.595465, mae: 20.585690, mean_q: -30.065023, mean_eps: 0.641959\n","  59896/150000: episode: 160, duration: 1.087s, episode steps: 140, steps per second: 129, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.964 [0.000, 2.000],  loss: 0.985458, mae: 20.552041, mean_q: -29.992832, mean_eps: 0.641047\n","  60131/150000: episode: 161, duration: 1.848s, episode steps: 235, steps per second: 127, episode reward: -234.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.923 [0.000, 2.000],  loss: 0.590909, mae: 20.633697, mean_q: -30.104990, mean_eps: 0.639922\n","  60297/150000: episode: 162, duration: 1.362s, episode steps: 166, steps per second: 122, episode reward: -165.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.982 [0.000, 2.000],  loss: 0.567023, mae: 20.742691, mean_q: -30.330696, mean_eps: 0.638719\n","  60442/150000: episode: 163, duration: 1.166s, episode steps: 145, steps per second: 124, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.945 [0.000, 2.000],  loss: 0.664716, mae: 20.893112, mean_q: -30.521497, mean_eps: 0.637786\n","  60607/150000: episode: 164, duration: 1.315s, episode steps: 165, steps per second: 126, episode reward: -164.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.994 [0.000, 2.000],  loss: 0.692642, mae: 20.690162, mean_q: -30.205135, mean_eps: 0.636856\n","  60805/150000: episode: 165, duration: 1.567s, episode steps: 198, steps per second: 126, episode reward: -197.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.985 [0.000, 2.000],  loss: 0.831851, mae: 20.720931, mean_q: -30.266289, mean_eps: 0.635767\n","  60959/150000: episode: 166, duration: 1.189s, episode steps: 154, steps per second: 130, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.974 [0.000, 2.000],  loss: 0.614623, mae: 20.595160, mean_q: -30.037488, mean_eps: 0.634711\n","  61166/150000: episode: 167, duration: 1.603s, episode steps: 207, steps per second: 129, episode reward: -206.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 0.947698, mae: 20.829086, mean_q: -30.392869, mean_eps: 0.633628\n","  61375/150000: episode: 168, duration: 1.616s, episode steps: 209, steps per second: 129, episode reward: -208.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.828 [0.000, 2.000],  loss: 0.518685, mae: 20.845902, mean_q: -30.514290, mean_eps: 0.632380\n","  61612/150000: episode: 169, duration: 1.848s, episode steps: 237, steps per second: 128, episode reward: -236.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.004 [0.000, 2.000],  loss: 0.442520, mae: 20.779660, mean_q: -30.404560, mean_eps: 0.631042\n","  61856/150000: episode: 170, duration: 1.951s, episode steps: 244, steps per second: 125, episode reward: -243.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.004 [0.000, 2.000],  loss: 0.479098, mae: 20.668140, mean_q: -30.227631, mean_eps: 0.629599\n","  62067/150000: episode: 171, duration: 1.652s, episode steps: 211, steps per second: 128, episode reward: -210.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.981 [0.000, 2.000],  loss: 0.713720, mae: 20.801064, mean_q: -30.374107, mean_eps: 0.628234\n","  62306/150000: episode: 172, duration: 1.963s, episode steps: 239, steps per second: 122, episode reward: -238.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.967 [0.000, 2.000],  loss: 0.470203, mae: 20.840528, mean_q: -30.437622, mean_eps: 0.626884\n","  62512/150000: episode: 173, duration: 1.610s, episode steps: 206, steps per second: 128, episode reward: -205.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.883 [0.000, 2.000],  loss: 0.532543, mae: 20.817847, mean_q: -30.428151, mean_eps: 0.625549\n","  62770/150000: episode: 174, duration: 2.025s, episode steps: 258, steps per second: 127, episode reward: -257.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 0.522459, mae: 20.842702, mean_q: -30.493384, mean_eps: 0.624157\n","  62938/150000: episode: 175, duration: 1.720s, episode steps: 168, steps per second:  98, episode reward: -167.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.946 [0.000, 2.000],  loss: 0.479534, mae: 20.928064, mean_q: -30.594698, mean_eps: 0.622879\n","  63149/150000: episode: 176, duration: 2.433s, episode steps: 211, steps per second:  87, episode reward: -210.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 0.721125, mae: 20.999319, mean_q: -30.683295, mean_eps: 0.621742\n","  63339/150000: episode: 177, duration: 1.850s, episode steps: 190, steps per second: 103, episode reward: -189.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.847 [0.000, 2.000],  loss: 0.514804, mae: 21.038701, mean_q: -30.787796, mean_eps: 0.620539\n","  63474/150000: episode: 178, duration: 1.049s, episode steps: 135, steps per second: 129, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.800 [0.000, 2.000],  loss: 0.449463, mae: 21.006730, mean_q: -30.738638, mean_eps: 0.619564\n","  63689/150000: episode: 179, duration: 1.664s, episode steps: 215, steps per second: 129, episode reward: -214.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.888 [0.000, 2.000],  loss: 0.633762, mae: 20.973426, mean_q: -30.653242, mean_eps: 0.618514\n","  63880/150000: episode: 180, duration: 1.500s, episode steps: 191, steps per second: 127, episode reward: -190.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.880 [0.000, 2.000],  loss: 0.720232, mae: 20.927163, mean_q: -30.594937, mean_eps: 0.617296\n","  64054/150000: episode: 181, duration: 1.426s, episode steps: 174, steps per second: 122, episode reward: -173.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.983 [0.000, 2.000],  loss: 0.560251, mae: 21.044075, mean_q: -30.796512, mean_eps: 0.616201\n","  64257/150000: episode: 182, duration: 1.610s, episode steps: 203, steps per second: 126, episode reward: -202.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.995 [0.000, 2.000],  loss: 0.813018, mae: 21.059055, mean_q: -30.775828, mean_eps: 0.615070\n","  64600/150000: episode: 183, duration: 2.721s, episode steps: 343, steps per second: 126, episode reward: -342.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.974 [0.000, 2.000],  loss: 0.620828, mae: 21.083084, mean_q: -30.842663, mean_eps: 0.613432\n","  64792/150000: episode: 184, duration: 1.500s, episode steps: 192, steps per second: 128, episode reward: -191.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.953 [0.000, 2.000],  loss: 0.460275, mae: 21.104564, mean_q: -30.882977, mean_eps: 0.611827\n","  65011/150000: episode: 185, duration: 1.695s, episode steps: 219, steps per second: 129, episode reward: -218.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.009 [0.000, 2.000],  loss: 0.736904, mae: 21.112386, mean_q: -30.826501, mean_eps: 0.610594\n","  65193/150000: episode: 186, duration: 1.475s, episode steps: 182, steps per second: 123, episode reward: -181.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.896 [0.000, 2.000],  loss: 0.715219, mae: 21.163621, mean_q: -30.922278, mean_eps: 0.609391\n","  65362/150000: episode: 187, duration: 1.394s, episode steps: 169, steps per second: 121, episode reward: -168.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.917 [0.000, 2.000],  loss: 0.423044, mae: 21.242777, mean_q: -31.068610, mean_eps: 0.608338\n","  65584/150000: episode: 188, duration: 1.745s, episode steps: 222, steps per second: 127, episode reward: -221.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.968 [0.000, 2.000],  loss: 0.454425, mae: 21.160933, mean_q: -30.967288, mean_eps: 0.607165\n","  65855/150000: episode: 189, duration: 2.213s, episode steps: 271, steps per second: 122, episode reward: -270.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.948 [0.000, 2.000],  loss: 0.592857, mae: 21.268117, mean_q: -31.092179, mean_eps: 0.605686\n","  66091/150000: episode: 190, duration: 1.888s, episode steps: 236, steps per second: 125, episode reward: -235.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.979 [0.000, 2.000],  loss: 0.826686, mae: 21.193128, mean_q: -30.946753, mean_eps: 0.604165\n","  66338/150000: episode: 191, duration: 1.948s, episode steps: 247, steps per second: 127, episode reward: -246.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.024 [0.000, 2.000],  loss: 0.619826, mae: 21.363743, mean_q: -31.217977, mean_eps: 0.602716\n","  66592/150000: episode: 192, duration: 2.013s, episode steps: 254, steps per second: 126, episode reward: -253.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.028 [0.000, 2.000],  loss: 0.452260, mae: 21.270327, mean_q: -31.101239, mean_eps: 0.601213\n","  66774/150000: episode: 193, duration: 1.441s, episode steps: 182, steps per second: 126, episode reward: -181.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.099 [0.000, 2.000],  loss: 0.482488, mae: 21.302191, mean_q: -31.165286, mean_eps: 0.599905\n","  66971/150000: episode: 194, duration: 1.565s, episode steps: 197, steps per second: 126, episode reward: -196.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.056 [0.000, 2.000],  loss: 0.505614, mae: 21.354879, mean_q: -31.232623, mean_eps: 0.598768\n","  67116/150000: episode: 195, duration: 1.168s, episode steps: 145, steps per second: 124, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.959 [0.000, 2.000],  loss: 0.510008, mae: 21.458697, mean_q: -31.393134, mean_eps: 0.597742\n","  67343/150000: episode: 196, duration: 1.784s, episode steps: 227, steps per second: 127, episode reward: -226.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.132 [0.000, 2.000],  loss: 0.606781, mae: 21.410011, mean_q: -31.280850, mean_eps: 0.596626\n","  67520/150000: episode: 197, duration: 1.397s, episode steps: 177, steps per second: 127, episode reward: -176.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.147 [0.000, 2.000],  loss: 0.504568, mae: 21.340990, mean_q: -31.185235, mean_eps: 0.595414\n","  67679/150000: episode: 198, duration: 1.301s, episode steps: 159, steps per second: 122, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.044 [0.000, 2.000],  loss: 0.766771, mae: 21.542178, mean_q: -31.447984, mean_eps: 0.594406\n","  67909/150000: episode: 199, duration: 1.824s, episode steps: 230, steps per second: 126, episode reward: -229.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.104 [0.000, 2.000],  loss: 0.586327, mae: 21.502894, mean_q: -31.430664, mean_eps: 0.593239\n","  68121/150000: episode: 200, duration: 1.650s, episode steps: 212, steps per second: 129, episode reward: -211.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500184, mae: 21.506644, mean_q: -31.422505, mean_eps: 0.591913\n","  68314/150000: episode: 201, duration: 1.541s, episode steps: 193, steps per second: 125, episode reward: -192.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.855 [0.000, 2.000],  loss: 0.510379, mae: 21.526332, mean_q: -31.454373, mean_eps: 0.590698\n","  68479/150000: episode: 202, duration: 1.283s, episode steps: 165, steps per second: 129, episode reward: -164.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.073 [0.000, 2.000],  loss: 0.526333, mae: 21.705830, mean_q: -31.716194, mean_eps: 0.589624\n","  68700/150000: episode: 203, duration: 1.701s, episode steps: 221, steps per second: 130, episode reward: -220.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.086 [0.000, 2.000],  loss: 0.588835, mae: 21.583004, mean_q: -31.536474, mean_eps: 0.588466\n","  68915/150000: episode: 204, duration: 1.678s, episode steps: 215, steps per second: 128, episode reward: -214.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.130 [0.000, 2.000],  loss: 0.616687, mae: 21.620787, mean_q: -31.557696, mean_eps: 0.587158\n","  69064/150000: episode: 205, duration: 1.190s, episode steps: 149, steps per second: 125, episode reward: -148.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.067 [0.000, 2.000],  loss: 0.531880, mae: 21.518628, mean_q: -31.402744, mean_eps: 0.586066\n","  69234/150000: episode: 206, duration: 1.365s, episode steps: 170, steps per second: 125, episode reward: -169.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.947 [0.000, 2.000],  loss: 0.561145, mae: 21.728385, mean_q: -31.753237, mean_eps: 0.585109\n","  69408/150000: episode: 207, duration: 1.343s, episode steps: 174, steps per second: 130, episode reward: -173.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.057 [0.000, 2.000],  loss: 0.536842, mae: 21.523610, mean_q: -31.427456, mean_eps: 0.584077\n","  69579/150000: episode: 208, duration: 1.380s, episode steps: 171, steps per second: 124, episode reward: -170.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.082 [0.000, 2.000],  loss: 0.637999, mae: 21.537343, mean_q: -31.446567, mean_eps: 0.583042\n","  69795/150000: episode: 209, duration: 1.692s, episode steps: 216, steps per second: 128, episode reward: -215.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.060 [0.000, 2.000],  loss: 0.526020, mae: 21.617847, mean_q: -31.580093, mean_eps: 0.581881\n","  69992/150000: episode: 210, duration: 1.556s, episode steps: 197, steps per second: 127, episode reward: -196.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.659152, mae: 21.601508, mean_q: -31.567695, mean_eps: 0.580642\n","  70208/150000: episode: 211, duration: 1.704s, episode steps: 216, steps per second: 127, episode reward: -215.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.995 [0.000, 2.000],  loss: 0.684595, mae: 22.117511, mean_q: -32.264585, mean_eps: 0.579403\n","  70439/150000: episode: 212, duration: 1.796s, episode steps: 231, steps per second: 129, episode reward: -230.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.726254, mae: 21.827149, mean_q: -31.830428, mean_eps: 0.578062\n","  70646/150000: episode: 213, duration: 1.617s, episode steps: 207, steps per second: 128, episode reward: -206.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.092 [0.000, 2.000],  loss: 0.665080, mae: 21.784523, mean_q: -31.787221, mean_eps: 0.576748\n","  70806/150000: episode: 214, duration: 1.262s, episode steps: 160, steps per second: 127, episode reward: -159.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.119 [0.000, 2.000],  loss: 0.645169, mae: 22.060479, mean_q: -32.230472, mean_eps: 0.575647\n","  71045/150000: episode: 215, duration: 1.897s, episode steps: 239, steps per second: 126, episode reward: -238.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.054 [0.000, 2.000],  loss: 0.645281, mae: 22.011943, mean_q: -32.118429, mean_eps: 0.574450\n","  71217/150000: episode: 216, duration: 1.382s, episode steps: 172, steps per second: 124, episode reward: -171.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.047 [0.000, 2.000],  loss: 0.783854, mae: 22.040957, mean_q: -32.143156, mean_eps: 0.573217\n","  71411/150000: episode: 217, duration: 1.544s, episode steps: 194, steps per second: 126, episode reward: -193.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.088 [0.000, 2.000],  loss: 0.692022, mae: 22.099133, mean_q: -32.219167, mean_eps: 0.572119\n","  71660/150000: episode: 218, duration: 2.018s, episode steps: 249, steps per second: 123, episode reward: -248.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.020 [0.000, 2.000],  loss: 0.656418, mae: 22.273652, mean_q: -32.546757, mean_eps: 0.570790\n","  71788/150000: episode: 219, duration: 1.018s, episode steps: 128, steps per second: 126, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.969 [0.000, 2.000],  loss: 0.815737, mae: 22.010598, mean_q: -32.081180, mean_eps: 0.569659\n","  71954/150000: episode: 220, duration: 1.318s, episode steps: 166, steps per second: 126, episode reward: -165.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 0.623178, mae: 22.026312, mean_q: -32.133967, mean_eps: 0.568777\n","  72144/150000: episode: 221, duration: 1.514s, episode steps: 190, steps per second: 125, episode reward: -189.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.905 [0.000, 2.000],  loss: 0.723151, mae: 21.979249, mean_q: -32.020779, mean_eps: 0.567709\n","  72312/150000: episode: 222, duration: 1.325s, episode steps: 168, steps per second: 127, episode reward: -167.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.946 [0.000, 2.000],  loss: 0.653353, mae: 22.004633, mean_q: -32.125405, mean_eps: 0.566635\n","  72590/150000: episode: 223, duration: 2.357s, episode steps: 278, steps per second: 118, episode reward: -277.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.036 [0.000, 2.000],  loss: 0.741899, mae: 21.923830, mean_q: -31.986231, mean_eps: 0.565297\n","  72821/150000: episode: 224, duration: 2.664s, episode steps: 231, steps per second:  87, episode reward: -230.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.905 [0.000, 2.000],  loss: 0.654095, mae: 21.907064, mean_q: -31.953005, mean_eps: 0.563770\n","  73026/150000: episode: 225, duration: 2.134s, episode steps: 205, steps per second:  96, episode reward: -204.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.854 [0.000, 2.000],  loss: 0.667975, mae: 21.860248, mean_q: -31.885831, mean_eps: 0.562462\n","  73199/150000: episode: 226, duration: 1.355s, episode steps: 173, steps per second: 128, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.960 [0.000, 2.000],  loss: 0.759074, mae: 21.903423, mean_q: -31.962276, mean_eps: 0.561328\n","  73347/150000: episode: 227, duration: 1.171s, episode steps: 148, steps per second: 126, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.878 [0.000, 2.000],  loss: 0.685810, mae: 21.877942, mean_q: -31.938849, mean_eps: 0.560365\n","  73580/150000: episode: 228, duration: 1.814s, episode steps: 233, steps per second: 128, episode reward: -232.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 0.703699, mae: 21.886533, mean_q: -31.949301, mean_eps: 0.559222\n","  73745/150000: episode: 229, duration: 1.293s, episode steps: 165, steps per second: 128, episode reward: -164.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.042 [0.000, 2.000],  loss: 0.544856, mae: 21.792921, mean_q: -31.818649, mean_eps: 0.558028\n","  73941/150000: episode: 230, duration: 1.531s, episode steps: 196, steps per second: 128, episode reward: -195.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.005 [0.000, 2.000],  loss: 0.514562, mae: 21.887962, mean_q: -31.995150, mean_eps: 0.556945\n","  74079/150000: episode: 231, duration: 1.115s, episode steps: 138, steps per second: 124, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.804 [0.000, 2.000],  loss: 0.710615, mae: 22.069623, mean_q: -32.211462, mean_eps: 0.555943\n","  74325/150000: episode: 232, duration: 1.939s, episode steps: 246, steps per second: 127, episode reward: -245.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.862 [0.000, 2.000],  loss: 0.825224, mae: 22.292337, mean_q: -32.539907, mean_eps: 0.554791\n","  74509/150000: episode: 233, duration: 1.475s, episode steps: 184, steps per second: 125, episode reward: -183.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.962 [0.000, 2.000],  loss: 0.697564, mae: 22.312219, mean_q: -32.577530, mean_eps: 0.553501\n","  74673/150000: episode: 234, duration: 1.307s, episode steps: 164, steps per second: 126, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.945 [0.000, 2.000],  loss: 0.743038, mae: 22.117176, mean_q: -32.239321, mean_eps: 0.552457\n","  74873/150000: episode: 235, duration: 1.582s, episode steps: 200, steps per second: 126, episode reward: -199.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 0.640479, mae: 22.336825, mean_q: -32.606417, mean_eps: 0.551365\n","  74980/150000: episode: 236, duration: 0.864s, episode steps: 107, steps per second: 124, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.841 [0.000, 2.000],  loss: 0.583266, mae: 22.036897, mean_q: -32.153088, mean_eps: 0.550444\n","  75157/150000: episode: 237, duration: 1.364s, episode steps: 177, steps per second: 130, episode reward: -176.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.808 [0.000, 2.000],  loss: 0.798333, mae: 22.288105, mean_q: -32.480124, mean_eps: 0.549592\n","  75357/150000: episode: 238, duration: 1.546s, episode steps: 200, steps per second: 129, episode reward: -199.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 0.704872, mae: 22.164327, mean_q: -32.317894, mean_eps: 0.548461\n","  75548/150000: episode: 239, duration: 1.482s, episode steps: 191, steps per second: 129, episode reward: -190.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 0.604956, mae: 22.078414, mean_q: -32.212858, mean_eps: 0.547288\n","  75740/150000: episode: 240, duration: 1.504s, episode steps: 192, steps per second: 128, episode reward: -191.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.932 [0.000, 2.000],  loss: 0.706228, mae: 22.190434, mean_q: -32.373483, mean_eps: 0.546139\n","  75965/150000: episode: 241, duration: 1.761s, episode steps: 225, steps per second: 128, episode reward: -224.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.062 [0.000, 2.000],  loss: 0.690706, mae: 22.124231, mean_q: -32.249197, mean_eps: 0.544888\n","  76105/150000: episode: 242, duration: 1.104s, episode steps: 140, steps per second: 127, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.871 [0.000, 2.000],  loss: 0.702932, mae: 21.905004, mean_q: -31.872881, mean_eps: 0.543793\n","  76260/150000: episode: 243, duration: 1.215s, episode steps: 155, steps per second: 128, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.691773, mae: 22.183603, mean_q: -32.303293, mean_eps: 0.542908\n","  76464/150000: episode: 244, duration: 1.644s, episode steps: 204, steps per second: 124, episode reward: -203.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.917 [0.000, 2.000],  loss: 0.581492, mae: 22.310452, mean_q: -32.499074, mean_eps: 0.541831\n","  76665/150000: episode: 245, duration: 1.633s, episode steps: 201, steps per second: 123, episode reward: -200.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.851 [0.000, 2.000],  loss: 0.597121, mae: 22.173970, mean_q: -32.335435, mean_eps: 0.540616\n","  76821/150000: episode: 246, duration: 1.225s, episode steps: 156, steps per second: 127, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.974 [0.000, 2.000],  loss: 0.713123, mae: 22.140546, mean_q: -32.234012, mean_eps: 0.539545\n","  76968/150000: episode: 247, duration: 1.193s, episode steps: 147, steps per second: 123, episode reward: -146.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.864 [0.000, 2.000],  loss: 0.657983, mae: 22.319364, mean_q: -32.526713, mean_eps: 0.538636\n","  77131/150000: episode: 248, duration: 1.350s, episode steps: 163, steps per second: 121, episode reward: -162.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.877 [0.000, 2.000],  loss: 0.756119, mae: 22.312400, mean_q: -32.445528, mean_eps: 0.537706\n","  77267/150000: episode: 249, duration: 1.074s, episode steps: 136, steps per second: 127, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.074 [0.000, 2.000],  loss: 0.653598, mae: 22.412237, mean_q: -32.653011, mean_eps: 0.536809\n","  77437/150000: episode: 250, duration: 1.343s, episode steps: 170, steps per second: 127, episode reward: -169.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 0.580099, mae: 22.385737, mean_q: -32.591549, mean_eps: 0.535891\n","  77637/150000: episode: 251, duration: 1.584s, episode steps: 200, steps per second: 126, episode reward: -199.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.065 [0.000, 2.000],  loss: 0.620470, mae: 22.425269, mean_q: -32.696547, mean_eps: 0.534781\n","  77801/150000: episode: 252, duration: 1.277s, episode steps: 164, steps per second: 128, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 0.639650, mae: 22.252084, mean_q: -32.421186, mean_eps: 0.533689\n","  77956/150000: episode: 253, duration: 1.194s, episode steps: 155, steps per second: 130, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.019 [0.000, 2.000],  loss: 0.704927, mae: 22.248220, mean_q: -32.361313, mean_eps: 0.532732\n","  78210/150000: episode: 254, duration: 1.945s, episode steps: 254, steps per second: 131, episode reward: -253.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.921 [0.000, 2.000],  loss: 0.769321, mae: 22.392810, mean_q: -32.592021, mean_eps: 0.531505\n","  78373/150000: episode: 255, duration: 1.294s, episode steps: 163, steps per second: 126, episode reward: -162.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.037 [0.000, 2.000],  loss: 0.597887, mae: 22.458046, mean_q: -32.709395, mean_eps: 0.530254\n","  78517/150000: episode: 256, duration: 1.127s, episode steps: 144, steps per second: 128, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.993 [0.000, 2.000],  loss: 0.573910, mae: 22.364071, mean_q: -32.580055, mean_eps: 0.529333\n","  78785/150000: episode: 257, duration: 2.067s, episode steps: 268, steps per second: 130, episode reward: -267.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.605510, mae: 22.257657, mean_q: -32.411582, mean_eps: 0.528097\n","  78946/150000: episode: 258, duration: 1.263s, episode steps: 161, steps per second: 127, episode reward: -160.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.894 [0.000, 2.000],  loss: 0.729351, mae: 22.291101, mean_q: -32.458158, mean_eps: 0.526810\n","  79114/150000: episode: 259, duration: 1.350s, episode steps: 168, steps per second: 124, episode reward: -167.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 0.765029, mae: 22.618290, mean_q: -32.880544, mean_eps: 0.525823\n","  79316/150000: episode: 260, duration: 1.563s, episode steps: 202, steps per second: 129, episode reward: -201.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.030 [0.000, 2.000],  loss: 0.774462, mae: 22.651037, mean_q: -32.932574, mean_eps: 0.524713\n","  79532/150000: episode: 261, duration: 1.665s, episode steps: 216, steps per second: 130, episode reward: -215.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.986 [0.000, 2.000],  loss: 0.752133, mae: 22.724599, mean_q: -33.076976, mean_eps: 0.523459\n","  79664/150000: episode: 262, duration: 1.042s, episode steps: 132, steps per second: 127, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.955 [0.000, 2.000],  loss: 0.644694, mae: 22.497000, mean_q: -32.733406, mean_eps: 0.522415\n","  79829/150000: episode: 263, duration: 1.296s, episode steps: 165, steps per second: 127, episode reward: -164.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 0.618644, mae: 22.552863, mean_q: -32.818724, mean_eps: 0.521524\n","  79977/150000: episode: 264, duration: 1.132s, episode steps: 148, steps per second: 131, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.108 [0.000, 2.000],  loss: 0.619557, mae: 22.570703, mean_q: -32.853070, mean_eps: 0.520585\n","  80150/150000: episode: 265, duration: 1.349s, episode steps: 173, steps per second: 128, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.104 [0.000, 2.000],  loss: 0.885343, mae: 22.463497, mean_q: -32.608603, mean_eps: 0.519622\n","  80296/150000: episode: 266, duration: 1.180s, episode steps: 146, steps per second: 124, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.021 [0.000, 2.000],  loss: 0.686115, mae: 22.241326, mean_q: -32.320773, mean_eps: 0.518665\n","  80411/150000: episode: 267, duration: 0.925s, episode steps: 115, steps per second: 124, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.130 [0.000, 2.000],  loss: 0.720260, mae: 22.610917, mean_q: -32.860603, mean_eps: 0.517882\n","  80599/150000: episode: 268, duration: 1.465s, episode steps: 188, steps per second: 128, episode reward: -187.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.117 [0.000, 2.000],  loss: 0.789927, mae: 22.524209, mean_q: -32.721084, mean_eps: 0.516973\n","  80792/150000: episode: 269, duration: 1.540s, episode steps: 193, steps per second: 125, episode reward: -192.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.104 [0.000, 2.000],  loss: 0.680587, mae: 22.561208, mean_q: -32.849807, mean_eps: 0.515830\n","  80975/150000: episode: 270, duration: 1.495s, episode steps: 183, steps per second: 122, episode reward: -182.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.044 [0.000, 2.000],  loss: 0.752311, mae: 22.419302, mean_q: -32.610505, mean_eps: 0.514702\n","  81134/150000: episode: 271, duration: 1.242s, episode steps: 159, steps per second: 128, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.912 [0.000, 2.000],  loss: 0.733359, mae: 22.665065, mean_q: -32.892286, mean_eps: 0.513676\n","  81278/150000: episode: 272, duration: 1.151s, episode steps: 144, steps per second: 125, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.021 [0.000, 2.000],  loss: 0.783315, mae: 22.450752, mean_q: -32.619139, mean_eps: 0.512767\n","  81430/150000: episode: 273, duration: 1.201s, episode steps: 152, steps per second: 127, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.099 [0.000, 2.000],  loss: 0.769248, mae: 22.382288, mean_q: -32.500191, mean_eps: 0.511879\n","  81569/150000: episode: 274, duration: 1.139s, episode steps: 139, steps per second: 122, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.007 [0.000, 2.000],  loss: 0.665501, mae: 22.265542, mean_q: -32.359591, mean_eps: 0.511006\n","  81772/150000: episode: 275, duration: 1.608s, episode steps: 203, steps per second: 126, episode reward: -202.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.956 [0.000, 2.000],  loss: 0.623343, mae: 22.402718, mean_q: -32.594828, mean_eps: 0.509980\n","  81930/150000: episode: 276, duration: 1.257s, episode steps: 158, steps per second: 126, episode reward: -157.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 0.698794, mae: 22.464558, mean_q: -32.647504, mean_eps: 0.508897\n","  82088/150000: episode: 277, duration: 1.251s, episode steps: 158, steps per second: 126, episode reward: -157.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.146 [0.000, 2.000],  loss: 0.771253, mae: 22.391480, mean_q: -32.535541, mean_eps: 0.507949\n","  82252/150000: episode: 278, duration: 1.292s, episode steps: 164, steps per second: 127, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.006 [0.000, 2.000],  loss: 0.640406, mae: 22.499939, mean_q: -32.716395, mean_eps: 0.506983\n","  82468/150000: episode: 279, duration: 1.685s, episode steps: 216, steps per second: 128, episode reward: -215.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.931 [0.000, 2.000],  loss: 0.619501, mae: 22.501545, mean_q: -32.713189, mean_eps: 0.505843\n","  82580/150000: episode: 280, duration: 0.918s, episode steps: 112, steps per second: 122, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 0.700167, mae: 22.122374, mean_q: -32.140695, mean_eps: 0.504859\n","  82702/150000: episode: 281, duration: 0.981s, episode steps: 122, steps per second: 124, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.066 [0.000, 2.000],  loss: 0.587188, mae: 22.467607, mean_q: -32.686499, mean_eps: 0.504157\n","  82862/150000: episode: 282, duration: 1.278s, episode steps: 160, steps per second: 125, episode reward: -159.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.181 [0.000, 2.000],  loss: 0.645977, mae: 22.322977, mean_q: -32.485717, mean_eps: 0.503311\n","  83018/150000: episode: 283, duration: 1.232s, episode steps: 156, steps per second: 127, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.962 [0.000, 2.000],  loss: 0.680695, mae: 22.236727, mean_q: -32.320089, mean_eps: 0.502363\n","  83200/150000: episode: 284, duration: 1.424s, episode steps: 182, steps per second: 128, episode reward: -181.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.055 [0.000, 2.000],  loss: 0.654374, mae: 22.606886, mean_q: -32.912021, mean_eps: 0.501349\n","  83376/150000: episode: 285, duration: 1.382s, episode steps: 176, steps per second: 127, episode reward: -175.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.655000, mae: 22.410601, mean_q: -32.631087, mean_eps: 0.500275\n","  83541/150000: episode: 286, duration: 1.301s, episode steps: 165, steps per second: 127, episode reward: -164.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.036 [0.000, 2.000],  loss: 0.633888, mae: 22.313596, mean_q: -32.477124, mean_eps: 0.499252\n","  83650/150000: episode: 287, duration: 0.845s, episode steps: 109, steps per second: 129, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.853 [0.000, 2.000],  loss: 0.631298, mae: 22.628680, mean_q: -32.964988, mean_eps: 0.498430\n","  83779/150000: episode: 288, duration: 0.992s, episode steps: 129, steps per second: 130, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.140 [0.000, 2.000],  loss: 0.663315, mae: 22.528861, mean_q: -32.789105, mean_eps: 0.497716\n","  83942/150000: episode: 289, duration: 1.247s, episode steps: 163, steps per second: 131, episode reward: -162.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.933 [0.000, 2.000],  loss: 0.700903, mae: 22.638767, mean_q: -32.944812, mean_eps: 0.496840\n","  84084/150000: episode: 290, duration: 1.116s, episode steps: 142, steps per second: 127, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.070 [0.000, 2.000],  loss: 0.610336, mae: 22.723948, mean_q: -33.078508, mean_eps: 0.495925\n","  84249/150000: episode: 291, duration: 1.283s, episode steps: 165, steps per second: 129, episode reward: -164.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.982 [0.000, 2.000],  loss: 0.801213, mae: 22.632084, mean_q: -32.913080, mean_eps: 0.495004\n","  84390/150000: episode: 292, duration: 1.094s, episode steps: 141, steps per second: 129, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.879 [0.000, 2.000],  loss: 0.659975, mae: 22.723317, mean_q: -33.094176, mean_eps: 0.494086\n","  84522/150000: episode: 293, duration: 1.033s, episode steps: 132, steps per second: 128, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.917 [0.000, 2.000],  loss: 0.566123, mae: 22.594830, mean_q: -32.940112, mean_eps: 0.493267\n","  84738/150000: episode: 294, duration: 1.700s, episode steps: 216, steps per second: 127, episode reward: -215.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.023 [0.000, 2.000],  loss: 0.652241, mae: 22.665350, mean_q: -32.996917, mean_eps: 0.492223\n","  84913/150000: episode: 295, duration: 1.353s, episode steps: 175, steps per second: 129, episode reward: -174.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.063 [0.000, 2.000],  loss: 0.632272, mae: 22.632016, mean_q: -32.963301, mean_eps: 0.491050\n","  85089/150000: episode: 296, duration: 1.381s, episode steps: 176, steps per second: 127, episode reward: -175.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.972 [0.000, 2.000],  loss: 0.633732, mae: 22.642251, mean_q: -32.959281, mean_eps: 0.489997\n","  85241/150000: episode: 297, duration: 1.191s, episode steps: 152, steps per second: 128, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.882 [0.000, 2.000],  loss: 0.720224, mae: 22.558300, mean_q: -32.793467, mean_eps: 0.489013\n","  85368/150000: episode: 298, duration: 0.985s, episode steps: 127, steps per second: 129, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.961 [0.000, 2.000],  loss: 0.624159, mae: 22.693959, mean_q: -33.024159, mean_eps: 0.488176\n","  85523/150000: episode: 299, duration: 1.186s, episode steps: 155, steps per second: 131, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.884 [0.000, 2.000],  loss: 0.710559, mae: 22.451971, mean_q: -32.661659, mean_eps: 0.487330\n","  85702/150000: episode: 300, duration: 1.378s, episode steps: 179, steps per second: 130, episode reward: -178.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.860 [0.000, 2.000],  loss: 0.668824, mae: 22.546021, mean_q: -32.800559, mean_eps: 0.486328\n","  85863/150000: episode: 301, duration: 1.244s, episode steps: 161, steps per second: 129, episode reward: -160.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.857 [0.000, 2.000],  loss: 0.657517, mae: 22.439173, mean_q: -32.614170, mean_eps: 0.485308\n","  86037/150000: episode: 302, duration: 1.339s, episode steps: 174, steps per second: 130, episode reward: -173.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.983 [0.000, 2.000],  loss: 0.591799, mae: 22.681536, mean_q: -32.980165, mean_eps: 0.484303\n","  86156/150000: episode: 303, duration: 0.946s, episode steps: 119, steps per second: 126, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.025 [0.000, 2.000],  loss: 0.604366, mae: 22.600254, mean_q: -32.859449, mean_eps: 0.483424\n","  86293/150000: episode: 304, duration: 1.037s, episode steps: 137, steps per second: 132, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.015 [0.000, 2.000],  loss: 0.577922, mae: 22.760705, mean_q: -33.136509, mean_eps: 0.482656\n","  86441/150000: episode: 305, duration: 1.146s, episode steps: 148, steps per second: 129, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.027 [0.000, 2.000],  loss: 0.672908, mae: 22.458589, mean_q: -32.643033, mean_eps: 0.481801\n","  86566/150000: episode: 306, duration: 0.982s, episode steps: 125, steps per second: 127, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.872 [0.000, 2.000],  loss: 0.580943, mae: 22.657640, mean_q: -32.967997, mean_eps: 0.480982\n","  86692/150000: episode: 307, duration: 0.987s, episode steps: 126, steps per second: 128, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.849 [0.000, 2.000],  loss: 0.646011, mae: 22.655996, mean_q: -32.961020, mean_eps: 0.480229\n","  86808/150000: episode: 308, duration: 0.929s, episode steps: 116, steps per second: 125, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 0.555699, mae: 22.593469, mean_q: -32.854841, mean_eps: 0.479503\n","  86950/150000: episode: 309, duration: 1.103s, episode steps: 142, steps per second: 129, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.944 [0.000, 2.000],  loss: 0.681467, mae: 22.706427, mean_q: -33.032248, mean_eps: 0.478729\n","  87182/150000: episode: 310, duration: 1.807s, episode steps: 232, steps per second: 128, episode reward: -231.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.823 [0.000, 2.000],  loss: 0.688163, mae: 22.701193, mean_q: -33.004661, mean_eps: 0.477607\n","  87329/150000: episode: 311, duration: 1.137s, episode steps: 147, steps per second: 129, episode reward: -146.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.871 [0.000, 2.000],  loss: 0.623826, mae: 22.821838, mean_q: -33.221123, mean_eps: 0.476470\n","  87458/150000: episode: 312, duration: 1.032s, episode steps: 129, steps per second: 125, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.922 [0.000, 2.000],  loss: 0.657025, mae: 22.764031, mean_q: -33.074032, mean_eps: 0.475642\n","  87642/150000: episode: 313, duration: 1.414s, episode steps: 184, steps per second: 130, episode reward: -183.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.804 [0.000, 2.000],  loss: 0.636704, mae: 22.580028, mean_q: -32.828566, mean_eps: 0.474703\n","  87808/150000: episode: 314, duration: 1.278s, episode steps: 166, steps per second: 130, episode reward: -165.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.952 [0.000, 2.000],  loss: 0.593342, mae: 22.556389, mean_q: -32.804385, mean_eps: 0.473653\n","  87968/150000: episode: 315, duration: 1.235s, episode steps: 160, steps per second: 130, episode reward: -159.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.144 [0.000, 2.000],  loss: 0.620142, mae: 22.698395, mean_q: -33.021703, mean_eps: 0.472675\n","  88136/150000: episode: 316, duration: 1.294s, episode steps: 168, steps per second: 130, episode reward: -167.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.976 [0.000, 2.000],  loss: 0.633589, mae: 22.745765, mean_q: -33.038757, mean_eps: 0.471691\n","  88253/150000: episode: 317, duration: 0.907s, episode steps: 117, steps per second: 129, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.678956, mae: 22.552059, mean_q: -32.745613, mean_eps: 0.470836\n","  88407/150000: episode: 318, duration: 1.201s, episode steps: 154, steps per second: 128, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.961 [0.000, 2.000],  loss: 0.627454, mae: 22.684500, mean_q: -32.993994, mean_eps: 0.470023\n","  88543/150000: episode: 319, duration: 1.069s, episode steps: 136, steps per second: 127, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.985 [0.000, 2.000],  loss: 0.570718, mae: 22.796009, mean_q: -33.151997, mean_eps: 0.469153\n","  88697/150000: episode: 320, duration: 1.201s, episode steps: 154, steps per second: 128, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.948 [0.000, 2.000],  loss: 0.771112, mae: 22.592753, mean_q: -32.791734, mean_eps: 0.468283\n","  88797/150000: episode: 321, duration: 0.788s, episode steps: 100, steps per second: 127, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.830 [0.000, 2.000],  loss: 0.659559, mae: 22.657288, mean_q: -32.863735, mean_eps: 0.467521\n","  88921/150000: episode: 322, duration: 0.977s, episode steps: 124, steps per second: 127, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.048 [0.000, 2.000],  loss: 0.693548, mae: 22.455971, mean_q: -32.604244, mean_eps: 0.466849\n","  89092/150000: episode: 323, duration: 1.356s, episode steps: 171, steps per second: 126, episode reward: -170.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.865 [0.000, 2.000],  loss: 0.732940, mae: 22.712089, mean_q: -32.990340, mean_eps: 0.465964\n","  89217/150000: episode: 324, duration: 0.983s, episode steps: 125, steps per second: 127, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.984 [0.000, 2.000],  loss: 0.766253, mae: 22.506178, mean_q: -32.639912, mean_eps: 0.465076\n","  89369/150000: episode: 325, duration: 1.189s, episode steps: 152, steps per second: 128, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: 0.602027, mae: 22.711869, mean_q: -32.978738, mean_eps: 0.464245\n","  89501/150000: episode: 326, duration: 1.034s, episode steps: 132, steps per second: 128, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.182 [0.000, 2.000],  loss: 0.685335, mae: 22.899481, mean_q: -33.278472, mean_eps: 0.463393\n","  89601/150000: episode: 327, duration: 0.783s, episode steps: 100, steps per second: 128, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.030 [0.000, 2.000],  loss: 0.697033, mae: 22.695241, mean_q: -32.974994, mean_eps: 0.462697\n","  89747/150000: episode: 328, duration: 1.139s, episode steps: 146, steps per second: 128, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.068 [0.000, 2.000],  loss: 0.676692, mae: 22.526580, mean_q: -32.692341, mean_eps: 0.461959\n","  89854/150000: episode: 329, duration: 0.875s, episode steps: 107, steps per second: 122, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.178 [0.000, 2.000],  loss: 0.630975, mae: 22.699552, mean_q: -33.008973, mean_eps: 0.461200\n","  90050/150000: episode: 330, duration: 2.102s, episode steps: 196, steps per second:  93, episode reward: -195.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.097 [0.000, 2.000],  loss: 0.620326, mae: 22.520040, mean_q: -32.734539, mean_eps: 0.460291\n","  90173/150000: episode: 331, duration: 1.455s, episode steps: 123, steps per second:  85, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.951 [0.000, 2.000],  loss: 0.714598, mae: 22.669654, mean_q: -32.912960, mean_eps: 0.459334\n","  90333/150000: episode: 332, duration: 1.954s, episode steps: 160, steps per second:  82, episode reward: -159.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.925 [0.000, 2.000],  loss: 0.721046, mae: 22.647897, mean_q: -32.886425, mean_eps: 0.458485\n","  90471/150000: episode: 333, duration: 1.617s, episode steps: 138, steps per second:  85, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.116 [0.000, 2.000],  loss: 0.648511, mae: 22.633137, mean_q: -32.892722, mean_eps: 0.457591\n","  90602/150000: episode: 334, duration: 1.544s, episode steps: 131, steps per second:  85, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.069 [0.000, 2.000],  loss: 0.702102, mae: 22.549303, mean_q: -32.738807, mean_eps: 0.456784\n","  90751/150000: episode: 335, duration: 1.651s, episode steps: 149, steps per second:  90, episode reward: -148.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.081 [0.000, 2.000],  loss: 0.656767, mae: 22.510612, mean_q: -32.715762, mean_eps: 0.455944\n","  90874/150000: episode: 336, duration: 0.962s, episode steps: 123, steps per second: 128, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.959 [0.000, 2.000],  loss: 0.603531, mae: 22.665403, mean_q: -32.957837, mean_eps: 0.455128\n","  90989/150000: episode: 337, duration: 0.928s, episode steps: 115, steps per second: 124, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.087 [0.000, 2.000],  loss: 0.569425, mae: 22.353520, mean_q: -32.481403, mean_eps: 0.454414\n","  91137/150000: episode: 338, duration: 1.138s, episode steps: 148, steps per second: 130, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.149 [0.000, 2.000],  loss: 0.696895, mae: 22.689559, mean_q: -32.871122, mean_eps: 0.453625\n","  91268/150000: episode: 339, duration: 1.024s, episode steps: 131, steps per second: 128, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.023 [0.000, 2.000],  loss: 0.601210, mae: 22.798725, mean_q: -33.097524, mean_eps: 0.452788\n","  91459/150000: episode: 340, duration: 1.492s, episode steps: 191, steps per second: 128, episode reward: -190.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.136 [0.000, 2.000],  loss: 0.624038, mae: 22.688064, mean_q: -32.916062, mean_eps: 0.451822\n","  91610/150000: episode: 341, duration: 1.182s, episode steps: 151, steps per second: 128, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.132 [0.000, 2.000],  loss: 0.621306, mae: 22.593334, mean_q: -32.775639, mean_eps: 0.450796\n","  91748/150000: episode: 342, duration: 1.072s, episode steps: 138, steps per second: 129, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.101 [0.000, 2.000],  loss: 0.682251, mae: 22.495289, mean_q: -32.621627, mean_eps: 0.449929\n","  91885/150000: episode: 343, duration: 1.084s, episode steps: 137, steps per second: 126, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.095 [0.000, 2.000],  loss: 0.582545, mae: 22.491214, mean_q: -32.633031, mean_eps: 0.449104\n","  92013/150000: episode: 344, duration: 1.023s, episode steps: 128, steps per second: 125, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.109 [0.000, 2.000],  loss: 0.619160, mae: 22.596952, mean_q: -32.791073, mean_eps: 0.448309\n","  92163/150000: episode: 345, duration: 1.167s, episode steps: 150, steps per second: 129, episode reward: -149.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.960 [0.000, 2.000],  loss: 0.675083, mae: 22.634732, mean_q: -32.796506, mean_eps: 0.447475\n","  92307/150000: episode: 346, duration: 1.167s, episode steps: 144, steps per second: 123, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.951 [0.000, 2.000],  loss: 0.582515, mae: 22.448673, mean_q: -32.549005, mean_eps: 0.446593\n","  92437/150000: episode: 347, duration: 1.025s, episode steps: 130, steps per second: 127, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.138 [0.000, 2.000],  loss: 0.618872, mae: 22.406636, mean_q: -32.470007, mean_eps: 0.445771\n","  92573/150000: episode: 348, duration: 1.060s, episode steps: 136, steps per second: 128, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.007 [0.000, 2.000],  loss: 0.579888, mae: 22.669195, mean_q: -32.882817, mean_eps: 0.444973\n","  92750/150000: episode: 349, duration: 1.392s, episode steps: 177, steps per second: 127, episode reward: -176.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.881 [0.000, 2.000],  loss: 0.621158, mae: 22.461869, mean_q: -32.574550, mean_eps: 0.444034\n","  92893/150000: episode: 350, duration: 1.104s, episode steps: 143, steps per second: 130, episode reward: -142.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.965 [0.000, 2.000],  loss: 0.611666, mae: 22.396265, mean_q: -32.474095, mean_eps: 0.443074\n","  92995/150000: episode: 351, duration: 0.802s, episode steps: 102, steps per second: 127, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.049 [0.000, 2.000],  loss: 0.548072, mae: 22.100502, mean_q: -32.034357, mean_eps: 0.442339\n","  93170/150000: episode: 352, duration: 1.367s, episode steps: 175, steps per second: 128, episode reward: -174.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.086 [0.000, 2.000],  loss: 0.738413, mae: 22.516352, mean_q: -32.631085, mean_eps: 0.441508\n","  93280/150000: episode: 353, duration: 0.849s, episode steps: 110, steps per second: 129, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.955 [0.000, 2.000],  loss: 0.639402, mae: 22.713991, mean_q: -32.974558, mean_eps: 0.440653\n","  93422/150000: episode: 354, duration: 1.099s, episode steps: 142, steps per second: 129, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.937 [0.000, 2.000],  loss: 0.666992, mae: 22.386470, mean_q: -32.431438, mean_eps: 0.439897\n","  93563/150000: episode: 355, duration: 1.121s, episode steps: 141, steps per second: 126, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.661466, mae: 22.418484, mean_q: -32.506668, mean_eps: 0.439048\n","  93700/150000: episode: 356, duration: 1.081s, episode steps: 137, steps per second: 127, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.883 [0.000, 2.000],  loss: 0.588830, mae: 22.402357, mean_q: -32.494704, mean_eps: 0.438214\n","  93864/150000: episode: 357, duration: 1.313s, episode steps: 164, steps per second: 125, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 0.596905, mae: 22.189016, mean_q: -32.174021, mean_eps: 0.437311\n","  94044/150000: episode: 358, duration: 1.424s, episode steps: 180, steps per second: 126, episode reward: -179.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.994 [0.000, 2.000],  loss: 0.627315, mae: 22.304663, mean_q: -32.364027, mean_eps: 0.436279\n","  94178/150000: episode: 359, duration: 1.047s, episode steps: 134, steps per second: 128, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.067 [0.000, 2.000],  loss: 0.712108, mae: 22.479896, mean_q: -32.595923, mean_eps: 0.435337\n","  94317/150000: episode: 360, duration: 1.099s, episode steps: 139, steps per second: 126, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 0.623753, mae: 22.387062, mean_q: -32.482625, mean_eps: 0.434518\n","  94430/150000: episode: 361, duration: 0.884s, episode steps: 113, steps per second: 128, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.965 [0.000, 2.000],  loss: 0.585669, mae: 22.400296, mean_q: -32.554330, mean_eps: 0.433762\n","  94581/150000: episode: 362, duration: 1.170s, episode steps: 151, steps per second: 129, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.007 [0.000, 2.000],  loss: 0.633543, mae: 22.413471, mean_q: -32.537293, mean_eps: 0.432970\n","  94741/150000: episode: 363, duration: 1.255s, episode steps: 160, steps per second: 128, episode reward: -159.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.969 [0.000, 2.000],  loss: 0.625136, mae: 22.430988, mean_q: -32.580740, mean_eps: 0.432037\n","  94858/150000: episode: 364, duration: 0.936s, episode steps: 117, steps per second: 125, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 0.614934, mae: 22.366367, mean_q: -32.480740, mean_eps: 0.431206\n","  94982/150000: episode: 365, duration: 0.961s, episode steps: 124, steps per second: 129, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.984 [0.000, 2.000],  loss: 0.681841, mae: 22.263537, mean_q: -32.334391, mean_eps: 0.430483\n","  95142/150000: episode: 366, duration: 1.248s, episode steps: 160, steps per second: 128, episode reward: -159.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.081 [0.000, 2.000],  loss: 0.728266, mae: 22.393368, mean_q: -32.484754, mean_eps: 0.429631\n","  95276/150000: episode: 367, duration: 1.028s, episode steps: 134, steps per second: 130, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.007 [0.000, 2.000],  loss: 0.668811, mae: 22.576710, mean_q: -32.801800, mean_eps: 0.428749\n","  95439/150000: episode: 368, duration: 1.269s, episode steps: 163, steps per second: 128, episode reward: -162.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.969 [0.000, 2.000],  loss: 0.590596, mae: 22.758660, mean_q: -33.064673, mean_eps: 0.427858\n","  95590/150000: episode: 369, duration: 1.216s, episode steps: 151, steps per second: 124, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.815 [0.000, 2.000],  loss: 0.595688, mae: 22.521685, mean_q: -32.711547, mean_eps: 0.426916\n","  95712/150000: episode: 370, duration: 1.007s, episode steps: 122, steps per second: 121, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.934 [0.000, 2.000],  loss: 0.566216, mae: 22.682115, mean_q: -32.973327, mean_eps: 0.426097\n","  95849/150000: episode: 371, duration: 1.081s, episode steps: 137, steps per second: 127, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.066 [0.000, 2.000],  loss: 0.628740, mae: 22.547555, mean_q: -32.782533, mean_eps: 0.425320\n","  95974/150000: episode: 372, duration: 0.989s, episode steps: 125, steps per second: 126, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.056 [0.000, 2.000],  loss: 0.565787, mae: 22.508395, mean_q: -32.692982, mean_eps: 0.424534\n","  96089/150000: episode: 373, duration: 0.929s, episode steps: 115, steps per second: 124, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 0.612298, mae: 22.878849, mean_q: -33.253427, mean_eps: 0.423814\n","  96199/150000: episode: 374, duration: 0.874s, episode steps: 110, steps per second: 126, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.873 [0.000, 2.000],  loss: 0.664502, mae: 22.458785, mean_q: -32.588474, mean_eps: 0.423139\n","  96323/150000: episode: 375, duration: 0.995s, episode steps: 124, steps per second: 125, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.952 [0.000, 2.000],  loss: 0.687584, mae: 22.459005, mean_q: -32.572124, mean_eps: 0.422437\n","  96471/150000: episode: 376, duration: 1.182s, episode steps: 148, steps per second: 125, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.081 [0.000, 2.000],  loss: 0.540016, mae: 22.427321, mean_q: -32.560946, mean_eps: 0.421621\n","  96609/150000: episode: 377, duration: 1.082s, episode steps: 138, steps per second: 128, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.812 [0.000, 2.000],  loss: 0.517086, mae: 22.456034, mean_q: -32.606174, mean_eps: 0.420763\n","  96741/150000: episode: 378, duration: 1.057s, episode steps: 132, steps per second: 125, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.886 [0.000, 2.000],  loss: 0.524941, mae: 22.361152, mean_q: -32.485451, mean_eps: 0.419953\n","  96859/150000: episode: 379, duration: 0.928s, episode steps: 118, steps per second: 127, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.915 [0.000, 2.000],  loss: 0.510974, mae: 22.575558, mean_q: -32.797566, mean_eps: 0.419203\n","  97007/150000: episode: 380, duration: 1.156s, episode steps: 148, steps per second: 128, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.858 [0.000, 2.000],  loss: 0.529387, mae: 22.425553, mean_q: -32.580751, mean_eps: 0.418405\n","  97160/150000: episode: 381, duration: 1.220s, episode steps: 153, steps per second: 125, episode reward: -152.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.078 [0.000, 2.000],  loss: 0.754539, mae: 22.536720, mean_q: -32.660221, mean_eps: 0.417502\n","  97330/150000: episode: 382, duration: 1.367s, episode steps: 170, steps per second: 124, episode reward: -169.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.706 [0.000, 2.000],  loss: 0.543643, mae: 22.455528, mean_q: -32.597631, mean_eps: 0.416533\n","  97462/150000: episode: 383, duration: 1.073s, episode steps: 132, steps per second: 123, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.939 [0.000, 2.000],  loss: 0.544142, mae: 22.462256, mean_q: -32.629281, mean_eps: 0.415627\n","  97591/150000: episode: 384, duration: 1.025s, episode steps: 129, steps per second: 126, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.829 [0.000, 2.000],  loss: 0.598381, mae: 22.350426, mean_q: -32.435086, mean_eps: 0.414844\n","  97758/150000: episode: 385, duration: 1.329s, episode steps: 167, steps per second: 126, episode reward: -166.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.796 [0.000, 2.000],  loss: 0.558873, mae: 22.698135, mean_q: -32.997833, mean_eps: 0.413956\n","  97979/150000: episode: 386, duration: 1.726s, episode steps: 221, steps per second: 128, episode reward: -220.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.837 [0.000, 2.000],  loss: 0.515877, mae: 22.506966, mean_q: -32.696953, mean_eps: 0.412792\n","  98143/150000: episode: 387, duration: 1.283s, episode steps: 164, steps per second: 128, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.896 [0.000, 2.000],  loss: 0.602464, mae: 22.718376, mean_q: -32.961884, mean_eps: 0.411637\n","  98271/150000: episode: 388, duration: 1.010s, episode steps: 128, steps per second: 127, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.148 [0.000, 2.000],  loss: 0.560879, mae: 22.614104, mean_q: -32.838311, mean_eps: 0.410761\n","  98424/150000: episode: 389, duration: 1.176s, episode steps: 153, steps per second: 130, episode reward: -152.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.020 [0.000, 2.000],  loss: 0.619397, mae: 22.567693, mean_q: -32.733693, mean_eps: 0.409918\n","  98686/150000: episode: 390, duration: 2.033s, episode steps: 262, steps per second: 129, episode reward: -261.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.069 [0.000, 2.000],  loss: 0.493302, mae: 22.485492, mean_q: -32.657381, mean_eps: 0.408673\n","  98859/150000: episode: 391, duration: 1.337s, episode steps: 173, steps per second: 129, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.098 [0.000, 2.000],  loss: 0.507275, mae: 22.576660, mean_q: -32.764656, mean_eps: 0.407368\n","  99005/150000: episode: 392, duration: 1.131s, episode steps: 146, steps per second: 129, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.021 [0.000, 2.000],  loss: 0.524629, mae: 22.548618, mean_q: -32.741847, mean_eps: 0.406411\n","  99177/150000: episode: 393, duration: 1.338s, episode steps: 172, steps per second: 129, episode reward: -171.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.948 [0.000, 2.000],  loss: 0.567652, mae: 22.716570, mean_q: -32.988492, mean_eps: 0.405457\n","  99311/150000: episode: 394, duration: 1.048s, episode steps: 134, steps per second: 128, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.970 [0.000, 2.000],  loss: 0.562269, mae: 22.962387, mean_q: -33.361603, mean_eps: 0.404539\n","  99474/150000: episode: 395, duration: 1.316s, episode steps: 163, steps per second: 124, episode reward: -162.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 0.529917, mae: 22.783038, mean_q: -33.138540, mean_eps: 0.403648\n","  99660/150000: episode: 396, duration: 1.455s, episode steps: 186, steps per second: 128, episode reward: -185.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.995 [0.000, 2.000],  loss: 0.540212, mae: 22.927284, mean_q: -33.321487, mean_eps: 0.402601\n","  99766/150000: episode: 397, duration: 0.819s, episode steps: 106, steps per second: 129, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.934 [0.000, 2.000],  loss: 0.633444, mae: 22.748800, mean_q: -33.001882, mean_eps: 0.401725\n","  99858/150000: episode: 398, duration: 0.735s, episode steps:  92, steps per second: 125, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.913 [0.000, 2.000],  loss: 0.501175, mae: 22.796079, mean_q: -33.115642, mean_eps: 0.401131\n"," 100031/150000: episode: 399, duration: 1.363s, episode steps: 173, steps per second: 127, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.867 [0.000, 2.000],  loss: 0.563584, mae: 22.493922, mean_q: -32.618269, mean_eps: 0.400336\n"," 100133/150000: episode: 400, duration: 0.801s, episode steps: 102, steps per second: 127, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 0.616598, mae: 22.488153, mean_q: -32.626609, mean_eps: 0.399511\n"," 100274/150000: episode: 401, duration: 1.080s, episode steps: 141, steps per second: 130, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.979 [0.000, 2.000],  loss: 0.620046, mae: 22.670622, mean_q: -32.946028, mean_eps: 0.398782\n"," 100396/150000: episode: 402, duration: 0.952s, episode steps: 122, steps per second: 128, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.213 [0.000, 2.000],  loss: 0.518768, mae: 22.693829, mean_q: -32.999755, mean_eps: 0.397993\n"," 100548/150000: episode: 403, duration: 1.159s, episode steps: 152, steps per second: 131, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.974 [0.000, 2.000],  loss: 0.522908, mae: 22.603124, mean_q: -32.812487, mean_eps: 0.397171\n"," 100704/150000: episode: 404, duration: 1.204s, episode steps: 156, steps per second: 130, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.942 [0.000, 2.000],  loss: 0.510422, mae: 22.745904, mean_q: -33.067165, mean_eps: 0.396247\n"," 100844/150000: episode: 405, duration: 1.095s, episode steps: 140, steps per second: 128, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.057 [0.000, 2.000],  loss: 0.493476, mae: 22.685535, mean_q: -32.986310, mean_eps: 0.395359\n"," 100956/150000: episode: 406, duration: 0.861s, episode steps: 112, steps per second: 130, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.964 [0.000, 2.000],  loss: 0.504078, mae: 22.759878, mean_q: -33.096420, mean_eps: 0.394603\n"," 101069/150000: episode: 407, duration: 0.889s, episode steps: 113, steps per second: 127, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.080 [0.000, 2.000],  loss: 0.719561, mae: 22.596774, mean_q: -32.814595, mean_eps: 0.393928\n"," 101214/150000: episode: 408, duration: 1.163s, episode steps: 145, steps per second: 125, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.103 [0.000, 2.000],  loss: 0.569207, mae: 22.656514, mean_q: -32.948396, mean_eps: 0.393154\n"," 101316/150000: episode: 409, duration: 0.812s, episode steps: 102, steps per second: 126, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.069 [0.000, 2.000],  loss: 0.646453, mae: 22.624238, mean_q: -32.820756, mean_eps: 0.392413\n"," 101413/150000: episode: 410, duration: 0.789s, episode steps:  97, steps per second: 123, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.062 [0.000, 2.000],  loss: 0.568198, mae: 22.772229, mean_q: -33.109793, mean_eps: 0.391816\n"," 101525/150000: episode: 411, duration: 0.884s, episode steps: 112, steps per second: 127, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.045 [0.000, 2.000],  loss: 0.451162, mae: 22.747171, mean_q: -33.108299, mean_eps: 0.391189\n"," 101647/150000: episode: 412, duration: 0.971s, episode steps: 122, steps per second: 126, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.066 [0.000, 2.000],  loss: 0.537117, mae: 22.748968, mean_q: -33.022835, mean_eps: 0.390487\n"," 101781/150000: episode: 413, duration: 1.034s, episode steps: 134, steps per second: 130, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.097 [0.000, 2.000],  loss: 0.465223, mae: 22.608440, mean_q: -32.869648, mean_eps: 0.389719\n"," 101929/150000: episode: 414, duration: 1.176s, episode steps: 148, steps per second: 126, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.034 [0.000, 2.000],  loss: 0.489879, mae: 22.866749, mean_q: -33.288656, mean_eps: 0.388873\n"," 102099/150000: episode: 415, duration: 1.307s, episode steps: 170, steps per second: 130, episode reward: -169.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.094 [0.000, 2.000],  loss: 0.605615, mae: 22.764377, mean_q: -33.067667, mean_eps: 0.387919\n"," 102215/150000: episode: 416, duration: 0.894s, episode steps: 116, steps per second: 130, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.078 [0.000, 2.000],  loss: 0.559651, mae: 22.920611, mean_q: -33.313851, mean_eps: 0.387061\n"," 102325/150000: episode: 417, duration: 0.860s, episode steps: 110, steps per second: 128, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.136 [0.000, 2.000],  loss: 0.643458, mae: 22.754788, mean_q: -33.003339, mean_eps: 0.386383\n"," 102446/150000: episode: 418, duration: 0.941s, episode steps: 121, steps per second: 129, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.149 [0.000, 2.000],  loss: 0.533829, mae: 22.551023, mean_q: -32.756566, mean_eps: 0.385690\n"," 102577/150000: episode: 419, duration: 1.042s, episode steps: 131, steps per second: 126, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.221 [0.000, 2.000],  loss: 0.490336, mae: 22.645541, mean_q: -32.907315, mean_eps: 0.384934\n"," 102698/150000: episode: 420, duration: 0.929s, episode steps: 121, steps per second: 130, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.215 [0.000, 2.000],  loss: 0.554788, mae: 22.909386, mean_q: -33.290739, mean_eps: 0.384178\n"," 102799/150000: episode: 421, duration: 0.805s, episode steps: 101, steps per second: 125, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.139 [0.000, 2.000],  loss: 0.598637, mae: 22.665709, mean_q: -32.868141, mean_eps: 0.383512\n"," 102954/150000: episode: 422, duration: 1.194s, episode steps: 155, steps per second: 130, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.123 [0.000, 2.000],  loss: 0.586157, mae: 22.835150, mean_q: -33.121788, mean_eps: 0.382744\n"," 103112/150000: episode: 423, duration: 1.231s, episode steps: 158, steps per second: 128, episode reward: -157.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.742794, mae: 22.587508, mean_q: -32.691366, mean_eps: 0.381805\n"," 103248/150000: episode: 424, duration: 1.084s, episode steps: 136, steps per second: 125, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.235 [0.000, 2.000],  loss: 0.619169, mae: 22.656569, mean_q: -32.814896, mean_eps: 0.380923\n"," 103369/150000: episode: 425, duration: 0.968s, episode steps: 121, steps per second: 125, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.074 [0.000, 2.000],  loss: 0.617300, mae: 22.545051, mean_q: -32.652400, mean_eps: 0.380152\n"," 103496/150000: episode: 426, duration: 1.011s, episode steps: 127, steps per second: 126, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.142 [0.000, 2.000],  loss: 0.596058, mae: 22.847521, mean_q: -33.105374, mean_eps: 0.379408\n"," 103594/150000: episode: 427, duration: 0.782s, episode steps:  98, steps per second: 125, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.041 [0.000, 2.000],  loss: 0.617264, mae: 22.654093, mean_q: -32.793292, mean_eps: 0.378733\n"," 103772/150000: episode: 428, duration: 1.403s, episode steps: 178, steps per second: 127, episode reward: -177.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.253 [0.000, 2.000],  loss: 0.598697, mae: 22.507682, mean_q: -32.654526, mean_eps: 0.377905\n"," 103897/150000: episode: 429, duration: 1.018s, episode steps: 125, steps per second: 123, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.056 [0.000, 2.000],  loss: 0.633240, mae: 22.702549, mean_q: -32.883650, mean_eps: 0.376996\n"," 103991/150000: episode: 430, duration: 0.748s, episode steps:  94, steps per second: 126, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 0.646042, mae: 22.406053, mean_q: -32.433755, mean_eps: 0.376339\n"," 104150/150000: episode: 431, duration: 1.244s, episode steps: 159, steps per second: 128, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.138 [0.000, 2.000],  loss: 0.719543, mae: 22.712945, mean_q: -32.884838, mean_eps: 0.375580\n"," 104272/150000: episode: 432, duration: 0.952s, episode steps: 122, steps per second: 128, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.967 [0.000, 2.000],  loss: 0.732259, mae: 22.688954, mean_q: -32.878275, mean_eps: 0.374737\n"," 104412/150000: episode: 433, duration: 1.081s, episode steps: 140, steps per second: 129, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.057 [0.000, 2.000],  loss: 0.649025, mae: 22.413361, mean_q: -32.510744, mean_eps: 0.373951\n"," 104553/150000: episode: 434, duration: 1.086s, episode steps: 141, steps per second: 130, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.950 [0.000, 2.000],  loss: 0.589874, mae: 22.337496, mean_q: -32.360052, mean_eps: 0.373108\n"," 104684/150000: episode: 435, duration: 1.046s, episode steps: 131, steps per second: 125, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.122 [0.000, 2.000],  loss: 0.533486, mae: 22.850879, mean_q: -33.166624, mean_eps: 0.372292\n"," 104779/150000: episode: 436, duration: 0.740s, episode steps:  95, steps per second: 128, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.053 [0.000, 2.000],  loss: 0.543631, mae: 22.707021, mean_q: -32.930523, mean_eps: 0.371614\n"," 104909/150000: episode: 437, duration: 1.023s, episode steps: 130, steps per second: 127, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.062 [0.000, 2.000],  loss: 0.625862, mae: 22.665853, mean_q: -32.878660, mean_eps: 0.370939\n"," 105087/150000: episode: 438, duration: 1.413s, episode steps: 178, steps per second: 126, episode reward: -177.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.062 [0.000, 2.000],  loss: 0.677627, mae: 22.795932, mean_q: -33.007995, mean_eps: 0.370015\n"," 105194/150000: episode: 439, duration: 0.846s, episode steps: 107, steps per second: 126, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.615019, mae: 22.618658, mean_q: -32.735401, mean_eps: 0.369160\n"," 105327/150000: episode: 440, duration: 1.019s, episode steps: 133, steps per second: 130, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.985 [0.000, 2.000],  loss: 0.561554, mae: 22.660354, mean_q: -32.833998, mean_eps: 0.368440\n"," 105464/150000: episode: 441, duration: 1.067s, episode steps: 137, steps per second: 128, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.561767, mae: 22.536424, mean_q: -32.682266, mean_eps: 0.367630\n"," 105547/150000: episode: 442, duration: 0.653s, episode steps:  83, steps per second: 127, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.133 [0.000, 2.000],  loss: 0.582273, mae: 22.505171, mean_q: -32.641426, mean_eps: 0.366970\n"," 105667/150000: episode: 443, duration: 0.967s, episode steps: 120, steps per second: 124, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.142 [0.000, 2.000],  loss: 0.558185, mae: 22.528595, mean_q: -32.686938, mean_eps: 0.366361\n"," 105812/150000: episode: 444, duration: 1.139s, episode steps: 145, steps per second: 127, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.910 [0.000, 2.000],  loss: 0.506461, mae: 22.588898, mean_q: -32.769100, mean_eps: 0.365566\n"," 105934/150000: episode: 445, duration: 0.972s, episode steps: 122, steps per second: 126, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.115 [0.000, 2.000],  loss: 0.614499, mae: 22.682527, mean_q: -32.854057, mean_eps: 0.364765\n"," 106088/150000: episode: 446, duration: 1.211s, episode steps: 154, steps per second: 127, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.981 [0.000, 2.000],  loss: 0.685614, mae: 22.455546, mean_q: -32.478435, mean_eps: 0.363937\n"," 106199/150000: episode: 447, duration: 0.852s, episode steps: 111, steps per second: 130, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.072 [0.000, 2.000],  loss: 0.608140, mae: 22.781324, mean_q: -33.007855, mean_eps: 0.363142\n"," 106306/150000: episode: 448, duration: 0.841s, episode steps: 107, steps per second: 127, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.972 [0.000, 2.000],  loss: 0.577894, mae: 22.660263, mean_q: -32.852683, mean_eps: 0.362488\n"," 106475/150000: episode: 449, duration: 1.339s, episode steps: 169, steps per second: 126, episode reward: -168.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.024 [0.000, 2.000],  loss: 0.549487, mae: 22.635556, mean_q: -32.839310, mean_eps: 0.361660\n"," 106625/150000: episode: 450, duration: 1.200s, episode steps: 150, steps per second: 125, episode reward: -149.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.893 [0.000, 2.000],  loss: 0.598867, mae: 22.808214, mean_q: -33.075385, mean_eps: 0.360703\n"," 106765/150000: episode: 451, duration: 1.097s, episode steps: 140, steps per second: 128, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.850 [0.000, 2.000],  loss: 0.544788, mae: 22.668508, mean_q: -32.854588, mean_eps: 0.359833\n"," 106959/150000: episode: 452, duration: 1.503s, episode steps: 194, steps per second: 129, episode reward: -193.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.974 [0.000, 2.000],  loss: 0.575927, mae: 22.780471, mean_q: -33.023963, mean_eps: 0.358831\n"," 107100/150000: episode: 453, duration: 1.150s, episode steps: 141, steps per second: 123, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.936 [0.000, 2.000],  loss: 0.586487, mae: 22.480544, mean_q: -32.543285, mean_eps: 0.357826\n"," 107224/150000: episode: 454, duration: 1.003s, episode steps: 124, steps per second: 124, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.927 [0.000, 2.000],  loss: 0.627106, mae: 22.303798, mean_q: -32.282231, mean_eps: 0.357031\n"," 107369/150000: episode: 455, duration: 1.542s, episode steps: 145, steps per second:  94, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.883 [0.000, 2.000],  loss: 0.584239, mae: 22.369616, mean_q: -32.425742, mean_eps: 0.356224\n"," 107486/150000: episode: 456, duration: 1.320s, episode steps: 117, steps per second:  89, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.009 [0.000, 2.000],  loss: 0.519803, mae: 22.466069, mean_q: -32.568635, mean_eps: 0.355438\n"," 107598/150000: episode: 457, duration: 1.342s, episode steps: 112, steps per second:  83, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.955 [0.000, 2.000],  loss: 0.508157, mae: 22.302132, mean_q: -32.355991, mean_eps: 0.354751\n"," 107705/150000: episode: 458, duration: 1.294s, episode steps: 107, steps per second:  83, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.794 [0.000, 2.000],  loss: 0.507706, mae: 22.365904, mean_q: -32.432581, mean_eps: 0.354094\n"," 107831/150000: episode: 459, duration: 1.675s, episode steps: 126, steps per second:  75, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.754 [0.000, 2.000],  loss: 0.486933, mae: 22.257398, mean_q: -32.239064, mean_eps: 0.353395\n"," 107933/150000: episode: 460, duration: 1.200s, episode steps: 102, steps per second:  85, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.922 [0.000, 2.000],  loss: 0.475273, mae: 22.156973, mean_q: -32.137941, mean_eps: 0.352711\n"," 108064/150000: episode: 461, duration: 1.469s, episode steps: 131, steps per second:  89, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.962 [0.000, 2.000],  loss: 0.684661, mae: 22.576663, mean_q: -32.687864, mean_eps: 0.352012\n"," 108193/150000: episode: 462, duration: 1.002s, episode steps: 129, steps per second: 129, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.899 [0.000, 2.000],  loss: 0.566227, mae: 22.394286, mean_q: -32.442578, mean_eps: 0.351232\n"," 108342/150000: episode: 463, duration: 1.134s, episode steps: 149, steps per second: 131, episode reward: -148.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.839 [0.000, 2.000],  loss: 0.571669, mae: 22.231926, mean_q: -32.197097, mean_eps: 0.350398\n"," 108449/150000: episode: 464, duration: 0.839s, episode steps: 107, steps per second: 127, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.953 [0.000, 2.000],  loss: 0.620962, mae: 22.301765, mean_q: -32.285686, mean_eps: 0.349630\n"," 108556/150000: episode: 465, duration: 0.854s, episode steps: 107, steps per second: 125, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.056 [0.000, 2.000],  loss: 0.589880, mae: 22.599829, mean_q: -32.741826, mean_eps: 0.348988\n"," 108649/150000: episode: 466, duration: 0.738s, episode steps:  93, steps per second: 126, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 0.615665, mae: 22.499385, mean_q: -32.614068, mean_eps: 0.348388\n"," 108765/150000: episode: 467, duration: 0.921s, episode steps: 116, steps per second: 126, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.871 [0.000, 2.000],  loss: 0.485480, mae: 22.394103, mean_q: -32.453283, mean_eps: 0.347761\n"," 108963/150000: episode: 468, duration: 1.518s, episode steps: 198, steps per second: 130, episode reward: -197.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.854 [0.000, 2.000],  loss: 0.480155, mae: 22.506411, mean_q: -32.627603, mean_eps: 0.346819\n"," 109060/150000: episode: 469, duration: 0.760s, episode steps:  97, steps per second: 128, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.656670, mae: 22.237599, mean_q: -32.161083, mean_eps: 0.345934\n"," 109152/150000: episode: 470, duration: 0.727s, episode steps:  92, steps per second: 127, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.826 [0.000, 2.000],  loss: 0.689070, mae: 22.390686, mean_q: -32.370025, mean_eps: 0.345367\n"," 109272/150000: episode: 471, duration: 0.936s, episode steps: 120, steps per second: 128, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.033 [0.000, 2.000],  loss: 0.588105, mae: 22.434987, mean_q: -32.474545, mean_eps: 0.344731\n"," 109374/150000: episode: 472, duration: 0.816s, episode steps: 102, steps per second: 125, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.853 [0.000, 2.000],  loss: 0.575535, mae: 22.371685, mean_q: -32.400717, mean_eps: 0.344065\n"," 109501/150000: episode: 473, duration: 0.981s, episode steps: 127, steps per second: 129, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.945 [0.000, 2.000],  loss: 0.550183, mae: 22.426803, mean_q: -32.467228, mean_eps: 0.343378\n"," 109663/150000: episode: 474, duration: 1.237s, episode steps: 162, steps per second: 131, episode reward: -161.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.932 [0.000, 2.000],  loss: 0.588265, mae: 22.189240, mean_q: -32.126633, mean_eps: 0.342511\n"," 109783/150000: episode: 475, duration: 0.926s, episode steps: 120, steps per second: 130, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.783 [0.000, 2.000],  loss: 0.582709, mae: 22.293257, mean_q: -32.265915, mean_eps: 0.341665\n"," 109884/150000: episode: 476, duration: 0.817s, episode steps: 101, steps per second: 124, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.812 [0.000, 2.000],  loss: 0.588212, mae: 22.400994, mean_q: -32.411704, mean_eps: 0.341002\n"," 110039/150000: episode: 477, duration: 1.232s, episode steps: 155, steps per second: 126, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.845 [0.000, 2.000],  loss: 0.562833, mae: 22.588984, mean_q: -32.734381, mean_eps: 0.340234\n"," 110155/150000: episode: 478, duration: 0.935s, episode steps: 116, steps per second: 124, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.983 [0.000, 2.000],  loss: 0.571233, mae: 22.736447, mean_q: -32.966984, mean_eps: 0.339421\n"," 110309/150000: episode: 479, duration: 1.209s, episode steps: 154, steps per second: 127, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.753 [0.000, 2.000],  loss: 0.556806, mae: 22.575959, mean_q: -32.709103, mean_eps: 0.338611\n"," 110429/150000: episode: 480, duration: 0.977s, episode steps: 120, steps per second: 123, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.783 [0.000, 2.000],  loss: 0.601394, mae: 22.564033, mean_q: -32.665561, mean_eps: 0.337789\n"," 110541/150000: episode: 481, duration: 0.916s, episode steps: 112, steps per second: 122, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.098 [0.000, 2.000],  loss: 0.483857, mae: 22.384999, mean_q: -32.446019, mean_eps: 0.337093\n"," 110618/150000: episode: 482, duration: 0.648s, episode steps:  77, steps per second: 119, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.566754, mae: 22.343796, mean_q: -32.377734, mean_eps: 0.336526\n"," 110715/150000: episode: 483, duration: 0.786s, episode steps:  97, steps per second: 123, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.031 [0.000, 2.000],  loss: 0.563688, mae: 22.395824, mean_q: -32.447269, mean_eps: 0.336004\n"," 110848/150000: episode: 484, duration: 1.069s, episode steps: 133, steps per second: 124, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.962 [0.000, 2.000],  loss: 0.494318, mae: 22.486947, mean_q: -32.572835, mean_eps: 0.335314\n"," 110967/150000: episode: 485, duration: 0.999s, episode steps: 119, steps per second: 119, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.924 [0.000, 2.000],  loss: 0.495073, mae: 22.416784, mean_q: -32.487101, mean_eps: 0.334558\n"," 111078/150000: episode: 486, duration: 0.922s, episode steps: 111, steps per second: 120, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.090 [0.000, 2.000],  loss: 0.573986, mae: 22.467528, mean_q: -32.519572, mean_eps: 0.333868\n"," 111190/150000: episode: 487, duration: 0.938s, episode steps: 112, steps per second: 119, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.920 [0.000, 2.000],  loss: 0.618230, mae: 22.693182, mean_q: -32.793307, mean_eps: 0.333199\n"," 111303/150000: episode: 488, duration: 0.914s, episode steps: 113, steps per second: 124, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.947 [0.000, 2.000],  loss: 0.589919, mae: 22.395000, mean_q: -32.417846, mean_eps: 0.332524\n"," 111403/150000: episode: 489, duration: 0.829s, episode steps: 100, steps per second: 121, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.120 [0.000, 2.000],  loss: 0.627962, mae: 22.528768, mean_q: -32.559788, mean_eps: 0.331885\n"," 111510/150000: episode: 490, duration: 0.893s, episode steps: 107, steps per second: 120, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.907 [0.000, 2.000],  loss: 0.620001, mae: 22.346491, mean_q: -32.328820, mean_eps: 0.331264\n"," 111633/150000: episode: 491, duration: 1.012s, episode steps: 123, steps per second: 122, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.098 [0.000, 2.000],  loss: 0.611327, mae: 22.459643, mean_q: -32.432992, mean_eps: 0.330574\n"," 111767/150000: episode: 492, duration: 1.114s, episode steps: 134, steps per second: 120, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.940 [0.000, 2.000],  loss: 0.522353, mae: 22.568050, mean_q: -32.696638, mean_eps: 0.329803\n"," 111864/150000: episode: 493, duration: 0.793s, episode steps:  97, steps per second: 122, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.897 [0.000, 2.000],  loss: 0.507105, mae: 22.406246, mean_q: -32.391995, mean_eps: 0.329110\n"," 111990/150000: episode: 494, duration: 1.010s, episode steps: 126, steps per second: 125, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.929 [0.000, 2.000],  loss: 0.575574, mae: 22.591898, mean_q: -32.699924, mean_eps: 0.328441\n"," 112149/150000: episode: 495, duration: 1.272s, episode steps: 159, steps per second: 125, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.830 [0.000, 2.000],  loss: 0.752957, mae: 22.510090, mean_q: -32.540413, mean_eps: 0.327586\n"," 112266/150000: episode: 496, duration: 0.966s, episode steps: 117, steps per second: 121, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.068 [0.000, 2.000],  loss: 0.599275, mae: 22.419009, mean_q: -32.486569, mean_eps: 0.326758\n"," 112334/150000: episode: 497, duration: 0.597s, episode steps:  68, steps per second: 114, episode reward: -67.000, mean reward: -0.985 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.712299, mae: 22.268927, mean_q: -32.208575, mean_eps: 0.326203\n"," 112430/150000: episode: 498, duration: 0.763s, episode steps:  96, steps per second: 126, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.781 [0.000, 2.000],  loss: 0.682913, mae: 22.439175, mean_q: -32.452202, mean_eps: 0.325711\n"," 112528/150000: episode: 499, duration: 0.768s, episode steps:  98, steps per second: 128, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.898 [0.000, 2.000],  loss: 0.576085, mae: 22.514540, mean_q: -32.556538, mean_eps: 0.325129\n"," 112629/150000: episode: 500, duration: 0.823s, episode steps: 101, steps per second: 123, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.931 [0.000, 2.000],  loss: 0.522525, mae: 22.267173, mean_q: -32.225292, mean_eps: 0.324532\n"," 112773/150000: episode: 501, duration: 1.155s, episode steps: 144, steps per second: 125, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.021 [0.000, 2.000],  loss: 0.539592, mae: 22.548053, mean_q: -32.623049, mean_eps: 0.323797\n"," 112907/150000: episode: 502, duration: 1.041s, episode steps: 134, steps per second: 129, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.784 [0.000, 2.000],  loss: 0.640561, mae: 22.429721, mean_q: -32.460377, mean_eps: 0.322963\n"," 113024/150000: episode: 503, duration: 0.915s, episode steps: 117, steps per second: 128, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.128 [0.000, 2.000],  loss: 0.590589, mae: 22.413215, mean_q: -32.454385, mean_eps: 0.322210\n"," 113125/150000: episode: 504, duration: 0.807s, episode steps: 101, steps per second: 125, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.109 [0.000, 2.000],  loss: 0.730219, mae: 22.459520, mean_q: -32.487561, mean_eps: 0.321556\n"," 113273/150000: episode: 505, duration: 1.157s, episode steps: 148, steps per second: 128, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.189 [0.000, 2.000],  loss: 0.606534, mae: 22.228394, mean_q: -32.172882, mean_eps: 0.320809\n"," 113378/150000: episode: 506, duration: 0.828s, episode steps: 105, steps per second: 127, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.971 [0.000, 2.000],  loss: 0.658132, mae: 22.210656, mean_q: -32.118781, mean_eps: 0.320050\n"," 113482/150000: episode: 507, duration: 0.825s, episode steps: 104, steps per second: 126, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.904 [0.000, 2.000],  loss: 0.628642, mae: 22.054567, mean_q: -31.900760, mean_eps: 0.319423\n"," 113593/150000: episode: 508, duration: 0.889s, episode steps: 111, steps per second: 125, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.598220, mae: 22.333251, mean_q: -32.355275, mean_eps: 0.318778\n"," 113718/150000: episode: 509, duration: 1.013s, episode steps: 125, steps per second: 123, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.088 [0.000, 2.000],  loss: 0.498978, mae: 22.358828, mean_q: -32.390122, mean_eps: 0.318070\n"," 113815/150000: episode: 510, duration: 0.757s, episode steps:  97, steps per second: 128, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.979 [0.000, 2.000],  loss: 0.554136, mae: 22.011339, mean_q: -31.820618, mean_eps: 0.317404\n"," 113914/150000: episode: 511, duration: 0.764s, episode steps:  99, steps per second: 130, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.071 [0.000, 2.000],  loss: 0.555475, mae: 22.415780, mean_q: -32.382303, mean_eps: 0.316816\n"," 114047/150000: episode: 512, duration: 1.017s, episode steps: 133, steps per second: 131, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.030 [0.000, 2.000],  loss: 0.568949, mae: 22.532223, mean_q: -32.580590, mean_eps: 0.316120\n"," 114135/150000: episode: 513, duration: 0.717s, episode steps:  88, steps per second: 123, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 0.615301, mae: 22.495966, mean_q: -32.546921, mean_eps: 0.315457\n"," 114286/150000: episode: 514, duration: 1.177s, episode steps: 151, steps per second: 128, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.113 [0.000, 2.000],  loss: 0.538603, mae: 22.408505, mean_q: -32.470708, mean_eps: 0.314740\n"," 114443/150000: episode: 515, duration: 1.224s, episode steps: 157, steps per second: 128, episode reward: -156.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: 0.516518, mae: 22.348895, mean_q: -32.391351, mean_eps: 0.313816\n"," 114529/150000: episode: 516, duration: 0.661s, episode steps:  86, steps per second: 130, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.163 [0.000, 2.000],  loss: 0.514397, mae: 22.022807, mean_q: -31.911444, mean_eps: 0.313087\n"," 114664/150000: episode: 517, duration: 1.039s, episode steps: 135, steps per second: 130, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.067 [0.000, 2.000],  loss: 0.586311, mae: 21.965711, mean_q: -31.787120, mean_eps: 0.312424\n"," 114810/150000: episode: 518, duration: 1.122s, episode steps: 146, steps per second: 130, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.240 [0.000, 2.000],  loss: 0.536625, mae: 21.984267, mean_q: -31.845236, mean_eps: 0.311581\n"," 114920/150000: episode: 519, duration: 0.880s, episode steps: 110, steps per second: 125, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.109 [0.000, 2.000],  loss: 0.560195, mae: 22.091685, mean_q: -31.960186, mean_eps: 0.310813\n"," 115022/150000: episode: 520, duration: 0.805s, episode steps: 102, steps per second: 127, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.127 [0.000, 2.000],  loss: 0.607207, mae: 22.031284, mean_q: -31.835330, mean_eps: 0.310177\n"," 115139/150000: episode: 521, duration: 0.902s, episode steps: 117, steps per second: 130, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.085 [0.000, 2.000],  loss: 0.704172, mae: 22.057754, mean_q: -31.921698, mean_eps: 0.309520\n"," 115228/150000: episode: 522, duration: 0.699s, episode steps:  89, steps per second: 127, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.112 [0.000, 2.000],  loss: 0.631633, mae: 21.824538, mean_q: -31.537613, mean_eps: 0.308902\n"," 115329/150000: episode: 523, duration: 0.789s, episode steps: 101, steps per second: 128, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.129 [0.000, 2.000],  loss: 0.481634, mae: 21.963629, mean_q: -31.821742, mean_eps: 0.308332\n"," 115424/150000: episode: 524, duration: 0.748s, episode steps:  95, steps per second: 127, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 0.525326, mae: 21.884977, mean_q: -31.682045, mean_eps: 0.307744\n"," 115578/150000: episode: 525, duration: 1.197s, episode steps: 154, steps per second: 129, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.195 [0.000, 2.000],  loss: 0.532402, mae: 21.962516, mean_q: -31.816910, mean_eps: 0.306997\n"," 115740/150000: episode: 526, duration: 1.262s, episode steps: 162, steps per second: 128, episode reward: -161.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.006 [0.000, 2.000],  loss: 0.535324, mae: 22.052095, mean_q: -31.913954, mean_eps: 0.306049\n"," 115833/150000: episode: 527, duration: 0.729s, episode steps:  93, steps per second: 128, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.097 [0.000, 2.000],  loss: 0.571299, mae: 21.685582, mean_q: -31.339850, mean_eps: 0.305284\n"," 115914/150000: episode: 528, duration: 0.625s, episode steps:  81, steps per second: 130, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.037 [0.000, 2.000],  loss: 0.569104, mae: 22.368729, mean_q: -32.379026, mean_eps: 0.304762\n"," 116026/150000: episode: 529, duration: 0.911s, episode steps: 112, steps per second: 123, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.116 [0.000, 2.000],  loss: 0.604877, mae: 21.926457, mean_q: -31.708737, mean_eps: 0.304183\n"," 116129/150000: episode: 530, duration: 0.816s, episode steps: 103, steps per second: 126, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 0.626319, mae: 22.094837, mean_q: -31.968461, mean_eps: 0.303538\n"," 116228/150000: episode: 531, duration: 0.810s, episode steps:  99, steps per second: 122, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 0.574163, mae: 21.830934, mean_q: -31.605868, mean_eps: 0.302932\n"," 116310/150000: episode: 532, duration: 0.629s, episode steps:  82, steps per second: 130, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.037 [0.000, 2.000],  loss: 0.540790, mae: 21.965687, mean_q: -31.806725, mean_eps: 0.302389\n"," 116435/150000: episode: 533, duration: 0.983s, episode steps: 125, steps per second: 127, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.008 [0.000, 2.000],  loss: 0.497996, mae: 22.053036, mean_q: -31.908170, mean_eps: 0.301768\n"," 116544/150000: episode: 534, duration: 0.854s, episode steps: 109, steps per second: 128, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.046 [0.000, 2.000],  loss: 0.511612, mae: 22.474300, mean_q: -32.534165, mean_eps: 0.301066\n"," 116652/150000: episode: 535, duration: 0.866s, episode steps: 108, steps per second: 125, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.194 [0.000, 2.000],  loss: 0.504783, mae: 22.156719, mean_q: -32.026705, mean_eps: 0.300415\n"," 116743/150000: episode: 536, duration: 0.711s, episode steps:  91, steps per second: 128, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.510289, mae: 22.157549, mean_q: -32.087574, mean_eps: 0.299818\n"," 116842/150000: episode: 537, duration: 0.769s, episode steps:  99, steps per second: 129, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 0.460904, mae: 22.110314, mean_q: -32.011396, mean_eps: 0.299248\n"," 116956/150000: episode: 538, duration: 0.885s, episode steps: 114, steps per second: 129, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.035 [0.000, 2.000],  loss: 0.466172, mae: 22.060833, mean_q: -31.910412, mean_eps: 0.298609\n"," 117050/150000: episode: 539, duration: 0.741s, episode steps:  94, steps per second: 127, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.508483, mae: 22.128698, mean_q: -32.060227, mean_eps: 0.297985\n"," 117158/150000: episode: 540, duration: 0.854s, episode steps: 108, steps per second: 127, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.046 [0.000, 2.000],  loss: 0.561557, mae: 21.966310, mean_q: -31.793932, mean_eps: 0.297379\n"," 117268/150000: episode: 541, duration: 0.874s, episode steps: 110, steps per second: 126, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.009 [0.000, 2.000],  loss: 0.564241, mae: 21.931700, mean_q: -31.735815, mean_eps: 0.296725\n"," 117385/150000: episode: 542, duration: 0.908s, episode steps: 117, steps per second: 129, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 0.541407, mae: 21.981627, mean_q: -31.799368, mean_eps: 0.296044\n"," 117525/150000: episode: 543, duration: 1.108s, episode steps: 140, steps per second: 126, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.114 [0.000, 2.000],  loss: 0.509122, mae: 21.918206, mean_q: -31.746091, mean_eps: 0.295273\n"," 117603/150000: episode: 544, duration: 0.595s, episode steps:  78, steps per second: 131, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.179 [0.000, 2.000],  loss: 0.510177, mae: 22.383519, mean_q: -32.485211, mean_eps: 0.294619\n"," 117699/150000: episode: 545, duration: 0.769s, episode steps:  96, steps per second: 125, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.062 [0.000, 2.000],  loss: 0.488053, mae: 22.268473, mean_q: -32.245018, mean_eps: 0.294097\n"," 117819/150000: episode: 546, duration: 0.931s, episode steps: 120, steps per second: 129, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.983 [0.000, 2.000],  loss: 0.520839, mae: 22.164871, mean_q: -32.130843, mean_eps: 0.293449\n"," 117895/150000: episode: 547, duration: 0.623s, episode steps:  76, steps per second: 122, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.053 [0.000, 2.000],  loss: 0.510236, mae: 21.788680, mean_q: -31.543603, mean_eps: 0.292861\n"," 118007/150000: episode: 548, duration: 0.884s, episode steps: 112, steps per second: 127, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.036 [0.000, 2.000],  loss: 0.487448, mae: 21.829560, mean_q: -31.579493, mean_eps: 0.292297\n"," 118124/150000: episode: 549, duration: 0.913s, episode steps: 117, steps per second: 128, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.923 [0.000, 2.000],  loss: 0.574608, mae: 21.802113, mean_q: -31.482546, mean_eps: 0.291610\n"," 118229/150000: episode: 550, duration: 0.835s, episode steps: 105, steps per second: 126, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.143 [0.000, 2.000],  loss: 0.635220, mae: 21.878577, mean_q: -31.619651, mean_eps: 0.290944\n"," 118347/150000: episode: 551, duration: 0.912s, episode steps: 118, steps per second: 129, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.127 [0.000, 2.000],  loss: 0.546078, mae: 21.743436, mean_q: -31.447537, mean_eps: 0.290275\n"," 118451/150000: episode: 552, duration: 0.817s, episode steps: 104, steps per second: 127, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 0.555753, mae: 21.675585, mean_q: -31.356237, mean_eps: 0.289609\n"," 118544/150000: episode: 553, duration: 0.741s, episode steps:  93, steps per second: 125, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.204 [0.000, 2.000],  loss: 0.688131, mae: 21.632501, mean_q: -31.216119, mean_eps: 0.289018\n"," 118667/150000: episode: 554, duration: 0.956s, episode steps: 123, steps per second: 129, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.878 [0.000, 2.000],  loss: 0.503692, mae: 21.827915, mean_q: -31.561640, mean_eps: 0.288370\n"," 118770/150000: episode: 555, duration: 0.824s, episode steps: 103, steps per second: 125, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.932 [0.000, 2.000],  loss: 0.521551, mae: 21.669504, mean_q: -31.333206, mean_eps: 0.287692\n"," 118876/150000: episode: 556, duration: 0.849s, episode steps: 106, steps per second: 125, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.925 [0.000, 2.000],  loss: 0.485240, mae: 21.854439, mean_q: -31.621791, mean_eps: 0.287065\n"," 118979/150000: episode: 557, duration: 0.813s, episode steps: 103, steps per second: 127, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.942 [0.000, 2.000],  loss: 0.452229, mae: 22.049679, mean_q: -31.955898, mean_eps: 0.286438\n"," 119094/150000: episode: 558, duration: 0.915s, episode steps: 115, steps per second: 126, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.017 [0.000, 2.000],  loss: 0.481192, mae: 21.599945, mean_q: -31.240528, mean_eps: 0.285784\n"," 119171/150000: episode: 559, duration: 0.610s, episode steps:  77, steps per second: 126, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.909 [0.000, 2.000],  loss: 0.521570, mae: 21.588186, mean_q: -31.227887, mean_eps: 0.285208\n"," 119282/150000: episode: 560, duration: 0.867s, episode steps: 111, steps per second: 128, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.982 [0.000, 2.000],  loss: 0.464799, mae: 21.444941, mean_q: -31.056945, mean_eps: 0.284644\n"," 119404/150000: episode: 561, duration: 0.950s, episode steps: 122, steps per second: 128, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.959 [0.000, 2.000],  loss: 0.485173, mae: 21.520187, mean_q: -31.152605, mean_eps: 0.283945\n"," 119493/150000: episode: 562, duration: 0.714s, episode steps:  89, steps per second: 125, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 0.494110, mae: 21.687133, mean_q: -31.364408, mean_eps: 0.283312\n"," 119625/150000: episode: 563, duration: 1.090s, episode steps: 132, steps per second: 121, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.962 [0.000, 2.000],  loss: 0.580811, mae: 21.447429, mean_q: -30.968208, mean_eps: 0.282649\n"," 119702/150000: episode: 564, duration: 0.622s, episode steps:  77, steps per second: 124, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.961 [0.000, 2.000],  loss: 0.551036, mae: 21.373183, mean_q: -30.884816, mean_eps: 0.282022\n"," 119853/150000: episode: 565, duration: 1.224s, episode steps: 151, steps per second: 123, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.947 [0.000, 2.000],  loss: 0.554926, mae: 21.604666, mean_q: -31.241501, mean_eps: 0.281338\n"," 119930/150000: episode: 566, duration: 0.621s, episode steps:  77, steps per second: 124, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.831 [0.000, 2.000],  loss: 0.569537, mae: 21.665997, mean_q: -31.274948, mean_eps: 0.280654\n"," 120012/150000: episode: 567, duration: 0.652s, episode steps:  82, steps per second: 126, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.159 [0.000, 2.000],  loss: 0.564305, mae: 21.686245, mean_q: -31.361156, mean_eps: 0.280177\n"," 120101/150000: episode: 568, duration: 0.717s, episode steps:  89, steps per second: 124, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 0.561737, mae: 21.670272, mean_q: -31.293013, mean_eps: 0.279664\n"," 120203/150000: episode: 569, duration: 0.820s, episode steps: 102, steps per second: 124, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.824 [0.000, 2.000],  loss: 0.579505, mae: 21.726821, mean_q: -31.413883, mean_eps: 0.279091\n"," 120310/150000: episode: 570, duration: 0.838s, episode steps: 107, steps per second: 128, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.944 [0.000, 2.000],  loss: 0.508008, mae: 21.409229, mean_q: -30.960119, mean_eps: 0.278464\n"," 120447/150000: episode: 571, duration: 1.088s, episode steps: 137, steps per second: 126, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.956 [0.000, 2.000],  loss: 0.519624, mae: 21.404063, mean_q: -30.972417, mean_eps: 0.277732\n"," 120557/150000: episode: 572, duration: 0.875s, episode steps: 110, steps per second: 126, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.973 [0.000, 2.000],  loss: 0.515420, mae: 21.920694, mean_q: -31.716223, mean_eps: 0.276991\n"," 120646/150000: episode: 573, duration: 0.712s, episode steps:  89, steps per second: 125, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.910 [0.000, 2.000],  loss: 0.470258, mae: 21.513087, mean_q: -31.120396, mean_eps: 0.276394\n"," 120754/150000: episode: 574, duration: 0.842s, episode steps: 108, steps per second: 128, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.898 [0.000, 2.000],  loss: 0.510424, mae: 21.564344, mean_q: -31.214810, mean_eps: 0.275803\n"," 120862/150000: episode: 575, duration: 0.862s, episode steps: 108, steps per second: 125, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.917 [0.000, 2.000],  loss: 0.505248, mae: 21.526930, mean_q: -31.177438, mean_eps: 0.275155\n"," 120952/150000: episode: 576, duration: 0.719s, episode steps:  90, steps per second: 125, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.922 [0.000, 2.000],  loss: 0.519908, mae: 21.458383, mean_q: -31.049371, mean_eps: 0.274561\n"," 121052/150000: episode: 577, duration: 0.781s, episode steps: 100, steps per second: 128, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.050 [0.000, 2.000],  loss: 0.541842, mae: 21.243441, mean_q: -30.694061, mean_eps: 0.273991\n"," 121163/150000: episode: 578, duration: 0.885s, episode steps: 111, steps per second: 125, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.027 [0.000, 2.000],  loss: 0.544156, mae: 21.580766, mean_q: -31.183371, mean_eps: 0.273358\n"," 121262/150000: episode: 579, duration: 0.789s, episode steps:  99, steps per second: 125, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.980 [0.000, 2.000],  loss: 0.465477, mae: 21.655284, mean_q: -31.343343, mean_eps: 0.272728\n"," 121353/150000: episode: 580, duration: 0.725s, episode steps:  91, steps per second: 126, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.868 [0.000, 2.000],  loss: 0.559215, mae: 21.067239, mean_q: -30.418383, mean_eps: 0.272158\n"," 121468/150000: episode: 581, duration: 0.890s, episode steps: 115, steps per second: 129, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.491617, mae: 21.663342, mean_q: -31.364043, mean_eps: 0.271540\n"," 121562/150000: episode: 582, duration: 0.735s, episode steps:  94, steps per second: 128, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.936 [0.000, 2.000],  loss: 0.486677, mae: 21.684447, mean_q: -31.387178, mean_eps: 0.270913\n"," 121670/150000: episode: 583, duration: 0.857s, episode steps: 108, steps per second: 126, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.972 [0.000, 2.000],  loss: 0.531659, mae: 21.167199, mean_q: -30.581441, mean_eps: 0.270307\n"," 121766/150000: episode: 584, duration: 0.771s, episode steps:  96, steps per second: 124, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.062 [0.000, 2.000],  loss: 0.528807, mae: 21.497783, mean_q: -31.081025, mean_eps: 0.269695\n"," 121896/150000: episode: 585, duration: 1.032s, episode steps: 130, steps per second: 126, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.900 [0.000, 2.000],  loss: 0.546716, mae: 21.523218, mean_q: -31.123314, mean_eps: 0.269017\n"," 122007/150000: episode: 586, duration: 0.885s, episode steps: 111, steps per second: 125, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.009 [0.000, 2.000],  loss: 0.524562, mae: 21.651545, mean_q: -31.298062, mean_eps: 0.268294\n"," 122122/150000: episode: 587, duration: 0.897s, episode steps: 115, steps per second: 128, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.965 [0.000, 2.000],  loss: 0.514318, mae: 21.677569, mean_q: -31.345853, mean_eps: 0.267616\n"," 122257/150000: episode: 588, duration: 1.067s, episode steps: 135, steps per second: 127, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.970 [0.000, 2.000],  loss: 0.523758, mae: 21.526349, mean_q: -31.139565, mean_eps: 0.266866\n"," 122395/150000: episode: 589, duration: 1.060s, episode steps: 138, steps per second: 130, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.029 [0.000, 2.000],  loss: 0.503365, mae: 21.859366, mean_q: -31.626180, mean_eps: 0.266047\n"," 122506/150000: episode: 590, duration: 0.865s, episode steps: 111, steps per second: 128, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.946 [0.000, 2.000],  loss: 0.501447, mae: 21.589089, mean_q: -31.255251, mean_eps: 0.265300\n"," 122644/150000: episode: 591, duration: 1.084s, episode steps: 138, steps per second: 127, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.717 [0.000, 2.000],  loss: 0.482718, mae: 21.524861, mean_q: -31.147140, mean_eps: 0.264553\n"," 122748/150000: episode: 592, duration: 0.817s, episode steps: 104, steps per second: 127, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.962 [0.000, 2.000],  loss: 0.422641, mae: 21.699151, mean_q: -31.409453, mean_eps: 0.263827\n"," 122859/150000: episode: 593, duration: 0.856s, episode steps: 111, steps per second: 130, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 0.424056, mae: 21.504947, mean_q: -31.106600, mean_eps: 0.263182\n"," 122973/150000: episode: 594, duration: 0.885s, episode steps: 114, steps per second: 129, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.807 [0.000, 2.000],  loss: 0.483511, mae: 21.460991, mean_q: -31.018909, mean_eps: 0.262507\n"," 123107/150000: episode: 595, duration: 1.031s, episode steps: 134, steps per second: 130, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.925 [0.000, 2.000],  loss: 0.570115, mae: 21.564964, mean_q: -31.138426, mean_eps: 0.261763\n"," 123200/150000: episode: 596, duration: 0.756s, episode steps:  93, steps per second: 123, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 0.494368, mae: 21.464818, mean_q: -31.047326, mean_eps: 0.261082\n"," 123302/150000: episode: 597, duration: 0.812s, episode steps: 102, steps per second: 126, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.049 [0.000, 2.000],  loss: 0.639685, mae: 21.337737, mean_q: -30.838370, mean_eps: 0.260497\n"," 123418/150000: episode: 598, duration: 0.872s, episode steps: 116, steps per second: 133, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.853 [0.000, 2.000],  loss: 0.562009, mae: 21.641722, mean_q: -31.320728, mean_eps: 0.259843\n"," 123503/150000: episode: 599, duration: 0.673s, episode steps:  85, steps per second: 126, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 0.584169, mae: 21.454271, mean_q: -31.011662, mean_eps: 0.259240\n"," 123634/150000: episode: 600, duration: 1.041s, episode steps: 131, steps per second: 126, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.947 [0.000, 2.000],  loss: 0.502271, mae: 21.385947, mean_q: -30.924263, mean_eps: 0.258592\n"," 123732/150000: episode: 601, duration: 0.790s, episode steps:  98, steps per second: 124, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.071 [0.000, 2.000],  loss: 0.585116, mae: 21.252744, mean_q: -30.713096, mean_eps: 0.257905\n"," 123833/150000: episode: 602, duration: 0.808s, episode steps: 101, steps per second: 125, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.030 [0.000, 2.000],  loss: 0.494369, mae: 21.422137, mean_q: -31.000732, mean_eps: 0.257308\n"," 123918/150000: episode: 603, duration: 0.681s, episode steps:  85, steps per second: 125, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.035 [0.000, 2.000],  loss: 0.468555, mae: 21.469591, mean_q: -31.115766, mean_eps: 0.256750\n"," 124052/150000: episode: 604, duration: 1.077s, episode steps: 134, steps per second: 124, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.903 [0.000, 2.000],  loss: 0.507319, mae: 21.652423, mean_q: -31.329975, mean_eps: 0.256093\n"," 124157/150000: episode: 605, duration: 0.824s, episode steps: 105, steps per second: 127, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.943 [0.000, 2.000],  loss: 0.588583, mae: 21.406801, mean_q: -30.943234, mean_eps: 0.255376\n"," 124267/150000: episode: 606, duration: 0.882s, episode steps: 110, steps per second: 125, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.009 [0.000, 2.000],  loss: 0.471609, mae: 21.508819, mean_q: -31.055092, mean_eps: 0.254731\n"," 124386/150000: episode: 607, duration: 0.960s, episode steps: 119, steps per second: 124, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.067 [0.000, 2.000],  loss: 0.529890, mae: 21.617182, mean_q: -31.260847, mean_eps: 0.254044\n"," 124480/150000: episode: 608, duration: 0.768s, episode steps:  94, steps per second: 122, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 0.563338, mae: 21.664137, mean_q: -31.302105, mean_eps: 0.253405\n"," 124578/150000: episode: 609, duration: 0.764s, episode steps:  98, steps per second: 128, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.204 [0.000, 2.000],  loss: 0.594278, mae: 21.626260, mean_q: -31.241459, mean_eps: 0.252829\n"," 124674/150000: episode: 610, duration: 0.759s, episode steps:  96, steps per second: 126, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.833 [0.000, 2.000],  loss: 0.522806, mae: 21.551016, mean_q: -31.155543, mean_eps: 0.252247\n"," 124801/150000: episode: 611, duration: 0.991s, episode steps: 127, steps per second: 128, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.984 [0.000, 2.000],  loss: 0.540332, mae: 21.392446, mean_q: -30.902894, mean_eps: 0.251578\n"," 124888/150000: episode: 612, duration: 0.683s, episode steps:  87, steps per second: 127, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.115 [0.000, 2.000],  loss: 0.553780, mae: 21.560098, mean_q: -31.167004, mean_eps: 0.250936\n"," 125044/150000: episode: 613, duration: 1.209s, episode steps: 156, steps per second: 129, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.942 [0.000, 2.000],  loss: 0.561345, mae: 21.539087, mean_q: -31.121441, mean_eps: 0.250207\n"," 125133/150000: episode: 614, duration: 0.696s, episode steps:  89, steps per second: 128, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.124 [0.000, 2.000],  loss: 0.585160, mae: 21.799648, mean_q: -31.494063, mean_eps: 0.249472\n"," 125286/150000: episode: 615, duration: 1.189s, episode steps: 153, steps per second: 129, episode reward: -152.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.078 [0.000, 2.000],  loss: 0.552016, mae: 21.636206, mean_q: -31.270373, mean_eps: 0.248746\n"," 125406/150000: episode: 616, duration: 0.919s, episode steps: 120, steps per second: 131, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.524678, mae: 21.602189, mean_q: -31.249186, mean_eps: 0.247927\n"," 125499/150000: episode: 617, duration: 0.742s, episode steps:  93, steps per second: 125, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.075 [0.000, 2.000],  loss: 0.539621, mae: 21.795030, mean_q: -31.525729, mean_eps: 0.247288\n"," 125576/150000: episode: 618, duration: 0.625s, episode steps:  77, steps per second: 123, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.117 [0.000, 2.000],  loss: 0.535496, mae: 21.685011, mean_q: -31.371579, mean_eps: 0.246778\n"," 125660/150000: episode: 619, duration: 0.663s, episode steps:  84, steps per second: 127, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.929 [0.000, 2.000],  loss: 0.505397, mae: 21.487835, mean_q: -31.061311, mean_eps: 0.246295\n"," 125789/150000: episode: 620, duration: 1.036s, episode steps: 129, steps per second: 125, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.047 [0.000, 2.000],  loss: 0.525217, mae: 21.331668, mean_q: -30.836835, mean_eps: 0.245656\n"," 125895/150000: episode: 621, duration: 0.853s, episode steps: 106, steps per second: 124, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.123 [0.000, 2.000],  loss: 0.532396, mae: 21.678229, mean_q: -31.365842, mean_eps: 0.244951\n"," 125993/150000: episode: 622, duration: 0.807s, episode steps:  98, steps per second: 121, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.143 [0.000, 2.000],  loss: 0.497388, mae: 21.625826, mean_q: -31.294026, mean_eps: 0.244339\n"," 126094/150000: episode: 623, duration: 1.044s, episode steps: 101, steps per second:  97, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.020 [0.000, 2.000],  loss: 0.597260, mae: 21.381615, mean_q: -30.877958, mean_eps: 0.243742\n"," 126169/150000: episode: 624, duration: 0.930s, episode steps:  75, steps per second:  81, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.880 [0.000, 2.000],  loss: 0.583391, mae: 21.421736, mean_q: -30.961797, mean_eps: 0.243214\n"," 126297/150000: episode: 625, duration: 1.813s, episode steps: 128, steps per second:  71, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.086 [0.000, 2.000],  loss: 0.633376, mae: 21.543316, mean_q: -31.162082, mean_eps: 0.242605\n"," 126387/150000: episode: 626, duration: 1.304s, episode steps:  90, steps per second:  69, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.100 [0.000, 2.000],  loss: 0.543956, mae: 21.372478, mean_q: -30.884826, mean_eps: 0.241951\n"," 126504/150000: episode: 627, duration: 1.519s, episode steps: 117, steps per second:  77, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.068 [0.000, 2.000],  loss: 0.617550, mae: 21.401571, mean_q: -30.939381, mean_eps: 0.241330\n"," 126592/150000: episode: 628, duration: 1.097s, episode steps:  88, steps per second:  80, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.558655, mae: 21.506029, mean_q: -31.087435, mean_eps: 0.240715\n"," 126721/150000: episode: 629, duration: 1.556s, episode steps: 129, steps per second:  83, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.132 [0.000, 2.000],  loss: 0.504293, mae: 21.128073, mean_q: -30.507381, mean_eps: 0.240064\n"," 126859/150000: episode: 630, duration: 1.135s, episode steps: 138, steps per second: 122, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.101 [0.000, 2.000],  loss: 0.549713, mae: 21.372814, mean_q: -30.883952, mean_eps: 0.239263\n"," 126948/150000: episode: 631, duration: 0.727s, episode steps:  89, steps per second: 122, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.978 [0.000, 2.000],  loss: 0.544995, mae: 21.539203, mean_q: -31.159430, mean_eps: 0.238582\n"," 127042/150000: episode: 632, duration: 0.770s, episode steps:  94, steps per second: 122, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.138 [0.000, 2.000],  loss: 0.561625, mae: 21.843951, mean_q: -31.609759, mean_eps: 0.238033\n"," 127188/150000: episode: 633, duration: 1.171s, episode steps: 146, steps per second: 125, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 0.555407, mae: 21.218239, mean_q: -30.614064, mean_eps: 0.237313\n"," 127293/150000: episode: 634, duration: 0.888s, episode steps: 105, steps per second: 118, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 0.530188, mae: 21.538905, mean_q: -31.125435, mean_eps: 0.236560\n"," 127397/150000: episode: 635, duration: 0.849s, episode steps: 104, steps per second: 123, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.067 [0.000, 2.000],  loss: 0.540963, mae: 21.497464, mean_q: -31.072677, mean_eps: 0.235933\n"," 127506/150000: episode: 636, duration: 0.907s, episode steps: 109, steps per second: 120, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.165 [0.000, 2.000],  loss: 0.523459, mae: 21.620169, mean_q: -31.245061, mean_eps: 0.235294\n"," 127609/150000: episode: 637, duration: 0.823s, episode steps: 103, steps per second: 125, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.146 [0.000, 2.000],  loss: 0.509003, mae: 21.435396, mean_q: -31.007236, mean_eps: 0.234658\n"," 127723/150000: episode: 638, duration: 0.900s, episode steps: 114, steps per second: 127, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.079 [0.000, 2.000],  loss: 0.486795, mae: 21.184903, mean_q: -30.632985, mean_eps: 0.234007\n"," 127836/150000: episode: 639, duration: 0.909s, episode steps: 113, steps per second: 124, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 0.519329, mae: 21.197489, mean_q: -30.640700, mean_eps: 0.233326\n"," 127933/150000: episode: 640, duration: 0.765s, episode steps:  97, steps per second: 127, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.113 [0.000, 2.000],  loss: 0.561421, mae: 21.172656, mean_q: -30.629297, mean_eps: 0.232696\n"," 128053/150000: episode: 641, duration: 0.946s, episode steps: 120, steps per second: 127, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.025 [0.000, 2.000],  loss: 0.585904, mae: 21.170784, mean_q: -30.552941, mean_eps: 0.232045\n"," 128180/150000: episode: 642, duration: 0.988s, episode steps: 127, steps per second: 128, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.984 [0.000, 2.000],  loss: 0.630859, mae: 21.228628, mean_q: -30.662109, mean_eps: 0.231304\n"," 128303/150000: episode: 643, duration: 0.966s, episode steps: 123, steps per second: 127, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.114 [0.000, 2.000],  loss: 0.622497, mae: 21.405303, mean_q: -30.892485, mean_eps: 0.230554\n"," 128443/150000: episode: 644, duration: 1.094s, episode steps: 140, steps per second: 128, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.121 [0.000, 2.000],  loss: 0.613515, mae: 20.940037, mean_q: -30.179670, mean_eps: 0.229765\n"," 128573/150000: episode: 645, duration: 1.043s, episode steps: 130, steps per second: 125, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.931 [0.000, 2.000],  loss: 0.561869, mae: 21.283238, mean_q: -30.765775, mean_eps: 0.228955\n"," 128690/150000: episode: 646, duration: 0.912s, episode steps: 117, steps per second: 128, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.923 [0.000, 2.000],  loss: 0.582020, mae: 21.404186, mean_q: -30.927911, mean_eps: 0.228214\n"," 128796/150000: episode: 647, duration: 0.821s, episode steps: 106, steps per second: 129, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.066 [0.000, 2.000],  loss: 0.517307, mae: 21.517462, mean_q: -31.099698, mean_eps: 0.227545\n"," 128900/150000: episode: 648, duration: 1.225s, episode steps: 104, steps per second:  85, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.077 [0.000, 2.000],  loss: 0.557846, mae: 21.193046, mean_q: -30.539373, mean_eps: 0.226915\n"," 129005/150000: episode: 649, duration: 0.843s, episode steps: 105, steps per second: 125, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 0.544152, mae: 21.385844, mean_q: -30.878783, mean_eps: 0.226288\n"," 129136/150000: episode: 650, duration: 1.009s, episode steps: 131, steps per second: 130, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.954 [0.000, 2.000],  loss: 0.702943, mae: 21.085452, mean_q: -30.432748, mean_eps: 0.225580\n"," 129369/150000: episode: 651, duration: 1.804s, episode steps: 233, steps per second: 129, episode reward: -232.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.863 [0.000, 2.000],  loss: 0.626153, mae: 21.286487, mean_q: -30.716157, mean_eps: 0.224488\n"," 129473/150000: episode: 652, duration: 0.794s, episode steps: 104, steps per second: 131, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.058 [0.000, 2.000],  loss: 0.581490, mae: 20.885222, mean_q: -30.142985, mean_eps: 0.223477\n"," 129572/150000: episode: 653, duration: 0.782s, episode steps:  99, steps per second: 127, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.020 [0.000, 2.000],  loss: 0.605424, mae: 21.113340, mean_q: -30.461384, mean_eps: 0.222868\n"," 129701/150000: episode: 654, duration: 0.981s, episode steps: 129, steps per second: 132, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.876 [0.000, 2.000],  loss: 0.565562, mae: 20.924890, mean_q: -30.164197, mean_eps: 0.222184\n"," 129790/150000: episode: 655, duration: 0.681s, episode steps:  89, steps per second: 131, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.933 [0.000, 2.000],  loss: 0.587536, mae: 21.370498, mean_q: -30.838966, mean_eps: 0.221530\n"," 129876/150000: episode: 656, duration: 0.696s, episode steps:  86, steps per second: 124, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.872 [0.000, 2.000],  loss: 0.517659, mae: 21.510395, mean_q: -31.070382, mean_eps: 0.221005\n"," 129990/150000: episode: 657, duration: 0.852s, episode steps: 114, steps per second: 134, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.167 [0.000, 2.000],  loss: 0.581629, mae: 21.064773, mean_q: -30.393130, mean_eps: 0.220405\n"," 130106/150000: episode: 658, duration: 0.891s, episode steps: 116, steps per second: 130, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.069 [0.000, 2.000],  loss: 0.595442, mae: 21.235067, mean_q: -30.641714, mean_eps: 0.219715\n"," 130203/150000: episode: 659, duration: 0.768s, episode steps:  97, steps per second: 126, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.938 [0.000, 2.000],  loss: 0.608349, mae: 21.133347, mean_q: -30.472743, mean_eps: 0.219076\n"," 130297/150000: episode: 660, duration: 0.724s, episode steps:  94, steps per second: 130, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.021 [0.000, 2.000],  loss: 0.639673, mae: 20.643980, mean_q: -29.745049, mean_eps: 0.218503\n"," 130395/150000: episode: 661, duration: 0.786s, episode steps:  98, steps per second: 125, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.980 [0.000, 2.000],  loss: 0.505049, mae: 21.248176, mean_q: -30.662634, mean_eps: 0.217927\n"," 130511/150000: episode: 662, duration: 0.864s, episode steps: 116, steps per second: 134, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.060 [0.000, 2.000],  loss: 0.557768, mae: 21.071260, mean_q: -30.413127, mean_eps: 0.217285\n"," 130596/150000: episode: 663, duration: 0.667s, episode steps:  85, steps per second: 127, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.953 [0.000, 2.000],  loss: 0.600333, mae: 21.024168, mean_q: -30.314092, mean_eps: 0.216682\n"," 130683/150000: episode: 664, duration: 0.699s, episode steps:  87, steps per second: 125, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.103 [0.000, 2.000],  loss: 0.585578, mae: 21.055435, mean_q: -30.427625, mean_eps: 0.216166\n"," 130846/150000: episode: 665, duration: 1.268s, episode steps: 163, steps per second: 129, episode reward: -162.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.939 [0.000, 2.000],  loss: 0.518747, mae: 20.666781, mean_q: -29.851638, mean_eps: 0.215416\n"," 130923/150000: episode: 666, duration: 0.610s, episode steps:  77, steps per second: 126, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.922 [0.000, 2.000],  loss: 0.539194, mae: 21.067075, mean_q: -30.443793, mean_eps: 0.214696\n"," 131041/150000: episode: 667, duration: 0.919s, episode steps: 118, steps per second: 128, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.932 [0.000, 2.000],  loss: 0.643118, mae: 20.967046, mean_q: -30.222793, mean_eps: 0.214111\n"," 131132/150000: episode: 668, duration: 0.755s, episode steps:  91, steps per second: 121, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.857 [0.000, 2.000],  loss: 0.659109, mae: 20.938692, mean_q: -30.188537, mean_eps: 0.213484\n"," 131216/150000: episode: 669, duration: 0.662s, episode steps:  84, steps per second: 127, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 0.579678, mae: 20.829118, mean_q: -30.111863, mean_eps: 0.212959\n"," 131318/150000: episode: 670, duration: 0.795s, episode steps: 102, steps per second: 128, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.882 [0.000, 2.000],  loss: 0.553805, mae: 21.063614, mean_q: -30.455298, mean_eps: 0.212401\n"," 131403/150000: episode: 671, duration: 0.660s, episode steps:  85, steps per second: 129, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 0.541181, mae: 20.897288, mean_q: -30.192941, mean_eps: 0.211840\n"," 131495/150000: episode: 672, duration: 0.715s, episode steps:  92, steps per second: 129, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.957 [0.000, 2.000],  loss: 0.537102, mae: 20.940740, mean_q: -30.220310, mean_eps: 0.211309\n"," 131590/150000: episode: 673, duration: 0.739s, episode steps:  95, steps per second: 129, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.640730, mae: 20.768421, mean_q: -29.929204, mean_eps: 0.210748\n"," 131685/150000: episode: 674, duration: 0.730s, episode steps:  95, steps per second: 130, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.916 [0.000, 2.000],  loss: 0.604677, mae: 20.799240, mean_q: -29.953273, mean_eps: 0.210178\n"," 131798/150000: episode: 675, duration: 0.869s, episode steps: 113, steps per second: 130, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.947 [0.000, 2.000],  loss: 0.576449, mae: 20.876712, mean_q: -30.191378, mean_eps: 0.209554\n"," 131891/150000: episode: 676, duration: 0.722s, episode steps:  93, steps per second: 129, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.968 [0.000, 2.000],  loss: 0.494820, mae: 20.896191, mean_q: -30.186719, mean_eps: 0.208936\n"," 131986/150000: episode: 677, duration: 0.738s, episode steps:  95, steps per second: 129, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.916 [0.000, 2.000],  loss: 0.611641, mae: 20.779490, mean_q: -29.969472, mean_eps: 0.208372\n"," 132062/150000: episode: 678, duration: 0.597s, episode steps:  76, steps per second: 127, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.882 [0.000, 2.000],  loss: 0.561896, mae: 21.131953, mean_q: -30.542680, mean_eps: 0.207859\n"," 132177/150000: episode: 679, duration: 0.889s, episode steps: 115, steps per second: 129, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.826 [0.000, 2.000],  loss: 0.638672, mae: 21.250068, mean_q: -30.668892, mean_eps: 0.207286\n"," 132273/150000: episode: 680, duration: 0.732s, episode steps:  96, steps per second: 131, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.677 [0.000, 2.000],  loss: 0.531705, mae: 20.825979, mean_q: -30.037874, mean_eps: 0.206653\n"," 132390/150000: episode: 681, duration: 0.907s, episode steps: 117, steps per second: 129, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.034 [0.000, 2.000],  loss: 0.588621, mae: 20.905834, mean_q: -30.167058, mean_eps: 0.206014\n"," 132489/150000: episode: 682, duration: 0.802s, episode steps:  99, steps per second: 123, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.828 [0.000, 2.000],  loss: 0.577276, mae: 20.867652, mean_q: -30.123643, mean_eps: 0.205366\n"," 132573/150000: episode: 683, duration: 0.660s, episode steps:  84, steps per second: 127, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.929 [0.000, 2.000],  loss: 0.545222, mae: 20.812581, mean_q: -30.016079, mean_eps: 0.204817\n"," 132687/150000: episode: 684, duration: 0.914s, episode steps: 114, steps per second: 125, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 0.541918, mae: 20.530123, mean_q: -29.613499, mean_eps: 0.204223\n"," 132774/150000: episode: 685, duration: 0.683s, episode steps:  87, steps per second: 127, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.138 [0.000, 2.000],  loss: 0.520233, mae: 21.107438, mean_q: -30.485218, mean_eps: 0.203620\n"," 132866/150000: episode: 686, duration: 0.741s, episode steps:  92, steps per second: 124, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.750 [0.000, 2.000],  loss: 0.533806, mae: 20.855335, mean_q: -30.121125, mean_eps: 0.203083\n"," 132976/150000: episode: 687, duration: 0.876s, episode steps: 110, steps per second: 126, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.745 [0.000, 2.000],  loss: 0.483751, mae: 20.960880, mean_q: -30.267696, mean_eps: 0.202477\n"," 133080/150000: episode: 688, duration: 0.819s, episode steps: 104, steps per second: 127, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.865 [0.000, 2.000],  loss: 0.635739, mae: 20.890881, mean_q: -30.085603, mean_eps: 0.201835\n"," 133181/150000: episode: 689, duration: 0.779s, episode steps: 101, steps per second: 130, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.960 [0.000, 2.000],  loss: 0.631083, mae: 20.981528, mean_q: -30.225978, mean_eps: 0.201220\n"," 133262/150000: episode: 690, duration: 0.665s, episode steps:  81, steps per second: 122, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 0.568800, mae: 21.031643, mean_q: -30.339658, mean_eps: 0.200674\n"," 133349/150000: episode: 691, duration: 0.692s, episode steps:  87, steps per second: 126, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.828 [0.000, 2.000],  loss: 0.569848, mae: 20.790529, mean_q: -29.939444, mean_eps: 0.200170\n"," 133422/150000: episode: 692, duration: 0.562s, episode steps:  73, steps per second: 130, episode reward: -72.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.808 [0.000, 2.000],  loss: 0.539185, mae: 20.633918, mean_q: -29.703670, mean_eps: 0.199690\n"," 133514/150000: episode: 693, duration: 0.743s, episode steps:  92, steps per second: 124, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 0.585583, mae: 21.080003, mean_q: -30.419015, mean_eps: 0.199195\n"," 133617/150000: episode: 694, duration: 0.827s, episode steps: 103, steps per second: 124, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.845 [0.000, 2.000],  loss: 0.565636, mae: 20.978906, mean_q: -30.251388, mean_eps: 0.198610\n"," 133716/150000: episode: 695, duration: 0.798s, episode steps:  99, steps per second: 124, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.939 [0.000, 2.000],  loss: 0.537301, mae: 20.900244, mean_q: -30.144364, mean_eps: 0.198004\n"," 133848/150000: episode: 696, duration: 1.031s, episode steps: 132, steps per second: 128, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.871 [0.000, 2.000],  loss: 0.547704, mae: 20.837787, mean_q: -30.098732, mean_eps: 0.197311\n"," 133978/150000: episode: 697, duration: 0.996s, episode steps: 130, steps per second: 131, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.877 [0.000, 2.000],  loss: 0.516153, mae: 20.827302, mean_q: -30.077126, mean_eps: 0.196525\n"," 134086/150000: episode: 698, duration: 0.846s, episode steps: 108, steps per second: 128, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.870 [0.000, 2.000],  loss: 0.684537, mae: 20.763804, mean_q: -29.919941, mean_eps: 0.195811\n"," 134190/150000: episode: 699, duration: 0.805s, episode steps: 104, steps per second: 129, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.962 [0.000, 2.000],  loss: 0.572022, mae: 20.716883, mean_q: -29.914332, mean_eps: 0.195175\n"," 134282/150000: episode: 700, duration: 0.736s, episode steps:  92, steps per second: 125, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.967 [0.000, 2.000],  loss: 0.550819, mae: 21.089552, mean_q: -30.519248, mean_eps: 0.194587\n"," 134373/150000: episode: 701, duration: 0.727s, episode steps:  91, steps per second: 125, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.912 [0.000, 2.000],  loss: 0.527375, mae: 20.913351, mean_q: -30.231147, mean_eps: 0.194038\n"," 134463/150000: episode: 702, duration: 0.722s, episode steps:  90, steps per second: 125, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 0.593993, mae: 20.725641, mean_q: -29.908093, mean_eps: 0.193495\n"," 134564/150000: episode: 703, duration: 0.822s, episode steps: 101, steps per second: 123, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.950 [0.000, 2.000],  loss: 0.549855, mae: 20.801458, mean_q: -30.066704, mean_eps: 0.192922\n"," 134641/150000: episode: 704, duration: 0.637s, episode steps:  77, steps per second: 121, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.753 [0.000, 2.000],  loss: 0.478665, mae: 21.030293, mean_q: -30.384409, mean_eps: 0.192388\n"," 134729/150000: episode: 705, duration: 0.698s, episode steps:  88, steps per second: 126, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.886 [0.000, 2.000],  loss: 0.571668, mae: 21.074779, mean_q: -30.439241, mean_eps: 0.191893\n"," 134823/150000: episode: 706, duration: 0.744s, episode steps:  94, steps per second: 126, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.894 [0.000, 2.000],  loss: 0.513795, mae: 21.023946, mean_q: -30.409207, mean_eps: 0.191347\n"," 134907/150000: episode: 707, duration: 0.672s, episode steps:  84, steps per second: 125, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.952 [0.000, 2.000],  loss: 0.513452, mae: 20.809036, mean_q: -30.032430, mean_eps: 0.190813\n"," 135016/150000: episode: 708, duration: 0.891s, episode steps: 109, steps per second: 122, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.826 [0.000, 2.000],  loss: 0.473465, mae: 21.037151, mean_q: -30.424653, mean_eps: 0.190234\n"," 135096/150000: episode: 709, duration: 0.636s, episode steps:  80, steps per second: 126, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 0.656278, mae: 21.171424, mean_q: -30.600828, mean_eps: 0.189667\n"," 135179/150000: episode: 710, duration: 0.633s, episode steps:  83, steps per second: 131, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 0.590564, mae: 20.764785, mean_q: -30.001452, mean_eps: 0.189178\n"," 135255/150000: episode: 711, duration: 0.608s, episode steps:  76, steps per second: 125, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.842 [0.000, 2.000],  loss: 0.674352, mae: 20.669323, mean_q: -29.788666, mean_eps: 0.188701\n"," 135332/150000: episode: 712, duration: 0.611s, episode steps:  77, steps per second: 126, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.948 [0.000, 2.000],  loss: 0.510116, mae: 21.040512, mean_q: -30.446326, mean_eps: 0.188242\n"," 135468/150000: episode: 713, duration: 1.062s, episode steps: 136, steps per second: 128, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.978 [0.000, 2.000],  loss: 0.566744, mae: 20.936025, mean_q: -30.276820, mean_eps: 0.187603\n"," 135552/150000: episode: 714, duration: 0.653s, episode steps:  84, steps per second: 129, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.518003, mae: 20.829483, mean_q: -30.115456, mean_eps: 0.186943\n"," 135671/150000: episode: 715, duration: 0.927s, episode steps: 119, steps per second: 128, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.908 [0.000, 2.000],  loss: 0.509299, mae: 21.050273, mean_q: -30.452501, mean_eps: 0.186334\n"," 135788/150000: episode: 716, duration: 0.955s, episode steps: 117, steps per second: 123, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.060 [0.000, 2.000],  loss: 0.551343, mae: 21.168623, mean_q: -30.645329, mean_eps: 0.185626\n"," 135880/150000: episode: 717, duration: 0.737s, episode steps:  92, steps per second: 125, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.913 [0.000, 2.000],  loss: 0.508376, mae: 20.747847, mean_q: -30.001871, mean_eps: 0.184999\n"," 135972/150000: episode: 718, duration: 0.760s, episode steps:  92, steps per second: 121, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.978 [0.000, 2.000],  loss: 0.618323, mae: 21.141856, mean_q: -30.519395, mean_eps: 0.184447\n"," 136065/150000: episode: 719, duration: 0.737s, episode steps:  93, steps per second: 126, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.957 [0.000, 2.000],  loss: 0.568681, mae: 20.536558, mean_q: -29.621566, mean_eps: 0.183892\n"," 136147/150000: episode: 720, duration: 0.648s, episode steps:  82, steps per second: 127, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 0.570425, mae: 20.555696, mean_q: -29.693028, mean_eps: 0.183367\n"," 136233/150000: episode: 721, duration: 0.671s, episode steps:  86, steps per second: 128, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.895 [0.000, 2.000],  loss: 0.586986, mae: 20.684122, mean_q: -29.857498, mean_eps: 0.182863\n"," 136324/150000: episode: 722, duration: 0.734s, episode steps:  91, steps per second: 124, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.978 [0.000, 2.000],  loss: 0.560036, mae: 20.368099, mean_q: -29.406926, mean_eps: 0.182332\n"," 136415/150000: episode: 723, duration: 0.742s, episode steps:  91, steps per second: 123, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.121 [0.000, 2.000],  loss: 0.450949, mae: 21.245497, mean_q: -30.746102, mean_eps: 0.181786\n"," 136519/150000: episode: 724, duration: 0.843s, episode steps: 104, steps per second: 123, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.038 [0.000, 2.000],  loss: 0.539184, mae: 21.031803, mean_q: -30.384870, mean_eps: 0.181201\n"," 136600/150000: episode: 725, duration: 0.660s, episode steps:  81, steps per second: 123, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.049 [0.000, 2.000],  loss: 0.541844, mae: 20.649085, mean_q: -29.801089, mean_eps: 0.180646\n"," 136697/150000: episode: 726, duration: 0.763s, episode steps:  97, steps per second: 127, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.969 [0.000, 2.000],  loss: 0.553226, mae: 20.661571, mean_q: -29.794562, mean_eps: 0.180112\n"," 136800/150000: episode: 727, duration: 0.830s, episode steps: 103, steps per second: 124, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.942 [0.000, 2.000],  loss: 0.527898, mae: 20.422736, mean_q: -29.481692, mean_eps: 0.179512\n"," 136884/150000: episode: 728, duration: 0.669s, episode steps:  84, steps per second: 126, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.095 [0.000, 2.000],  loss: 0.612131, mae: 20.916003, mean_q: -30.210626, mean_eps: 0.178951\n"," 136969/150000: episode: 729, duration: 0.693s, episode steps:  85, steps per second: 123, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.082 [0.000, 2.000],  loss: 0.534747, mae: 20.680127, mean_q: -29.918199, mean_eps: 0.178444\n"," 137046/150000: episode: 730, duration: 0.609s, episode steps:  77, steps per second: 126, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.683643, mae: 20.528354, mean_q: -29.608711, mean_eps: 0.177958\n"," 137158/150000: episode: 731, duration: 0.866s, episode steps: 112, steps per second: 129, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.911 [0.000, 2.000],  loss: 0.619270, mae: 20.691754, mean_q: -29.836524, mean_eps: 0.177391\n"," 137253/150000: episode: 732, duration: 0.760s, episode steps:  95, steps per second: 125, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.968 [0.000, 2.000],  loss: 0.548789, mae: 20.807258, mean_q: -30.056834, mean_eps: 0.176770\n"," 137329/150000: episode: 733, duration: 0.611s, episode steps:  76, steps per second: 124, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.895 [0.000, 2.000],  loss: 0.574968, mae: 20.579609, mean_q: -29.704388, mean_eps: 0.176257\n"," 137433/150000: episode: 734, duration: 0.815s, episode steps: 104, steps per second: 128, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.058 [0.000, 2.000],  loss: 0.566695, mae: 20.819315, mean_q: -30.089560, mean_eps: 0.175717\n"," 137506/150000: episode: 735, duration: 0.582s, episode steps:  73, steps per second: 125, episode reward: -72.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.041 [0.000, 2.000],  loss: 0.507122, mae: 20.590572, mean_q: -29.734322, mean_eps: 0.175186\n"," 137616/150000: episode: 736, duration: 0.880s, episode steps: 110, steps per second: 125, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.973 [0.000, 2.000],  loss: 0.468600, mae: 20.773339, mean_q: -30.012886, mean_eps: 0.174637\n"," 137714/150000: episode: 737, duration: 0.773s, episode steps:  98, steps per second: 127, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.041 [0.000, 2.000],  loss: 0.629251, mae: 20.592803, mean_q: -29.664252, mean_eps: 0.174013\n"," 137793/150000: episode: 738, duration: 0.625s, episode steps:  79, steps per second: 126, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.873 [0.000, 2.000],  loss: 0.537574, mae: 20.653172, mean_q: -29.782070, mean_eps: 0.173482\n"," 137866/150000: episode: 739, duration: 0.594s, episode steps:  73, steps per second: 123, episode reward: -72.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.137 [0.000, 2.000],  loss: 0.440804, mae: 20.744068, mean_q: -29.959406, mean_eps: 0.173026\n"," 137950/150000: episode: 740, duration: 0.673s, episode steps:  84, steps per second: 125, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.524340, mae: 20.869996, mean_q: -30.119639, mean_eps: 0.172555\n"," 138029/150000: episode: 741, duration: 0.625s, episode steps:  79, steps per second: 126, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.114 [0.000, 2.000],  loss: 0.627777, mae: 21.061383, mean_q: -30.333951, mean_eps: 0.172066\n"," 138123/150000: episode: 742, duration: 0.754s, episode steps:  94, steps per second: 125, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.032 [0.000, 2.000],  loss: 0.624005, mae: 21.117678, mean_q: -30.431931, mean_eps: 0.171547\n"," 138220/150000: episode: 743, duration: 0.768s, episode steps:  97, steps per second: 126, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.959 [0.000, 2.000],  loss: 0.605480, mae: 20.897588, mean_q: -30.104122, mean_eps: 0.170974\n"," 138317/150000: episode: 744, duration: 0.763s, episode steps:  97, steps per second: 127, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.144 [0.000, 2.000],  loss: 0.589919, mae: 21.151520, mean_q: -30.551198, mean_eps: 0.170392\n"," 138416/150000: episode: 745, duration: 0.791s, episode steps:  99, steps per second: 125, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 0.581651, mae: 21.013460, mean_q: -30.267923, mean_eps: 0.169804\n"," 138511/150000: episode: 746, duration: 0.745s, episode steps:  95, steps per second: 128, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.126 [0.000, 2.000],  loss: 0.551348, mae: 21.089714, mean_q: -30.400978, mean_eps: 0.169222\n"," 138607/150000: episode: 747, duration: 0.758s, episode steps:  96, steps per second: 127, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.885 [0.000, 2.000],  loss: 0.601264, mae: 20.593839, mean_q: -29.721116, mean_eps: 0.168649\n"," 138721/150000: episode: 748, duration: 0.907s, episode steps: 114, steps per second: 126, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.930 [0.000, 2.000],  loss: 0.505678, mae: 20.904662, mean_q: -30.183509, mean_eps: 0.168019\n"," 138798/150000: episode: 749, duration: 0.604s, episode steps:  77, steps per second: 127, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.078 [0.000, 2.000],  loss: 0.577014, mae: 21.008260, mean_q: -30.308734, mean_eps: 0.167446\n"," 138903/150000: episode: 750, duration: 0.813s, episode steps: 105, steps per second: 129, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.895 [0.000, 2.000],  loss: 0.657677, mae: 20.879300, mean_q: -30.086126, mean_eps: 0.166900\n"," 139011/150000: episode: 751, duration: 0.837s, episode steps: 108, steps per second: 129, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.167 [0.000, 2.000],  loss: 0.531460, mae: 20.789571, mean_q: -30.016354, mean_eps: 0.166261\n"," 139092/150000: episode: 752, duration: 0.628s, episode steps:  81, steps per second: 129, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.963 [0.000, 2.000],  loss: 0.581283, mae: 21.188916, mean_q: -30.547866, mean_eps: 0.165694\n"," 139199/150000: episode: 753, duration: 0.836s, episode steps: 107, steps per second: 128, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.140 [0.000, 2.000],  loss: 0.570893, mae: 21.095286, mean_q: -30.445768, mean_eps: 0.165130\n"," 139302/150000: episode: 754, duration: 0.811s, episode steps: 103, steps per second: 127, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.175 [0.000, 2.000],  loss: 0.499240, mae: 21.052062, mean_q: -30.379147, mean_eps: 0.164500\n"," 139397/150000: episode: 755, duration: 0.735s, episode steps:  95, steps per second: 129, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 0.553354, mae: 20.823275, mean_q: -30.026428, mean_eps: 0.163906\n"," 139525/150000: episode: 756, duration: 0.996s, episode steps: 128, steps per second: 128, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.117 [0.000, 2.000],  loss: 0.501350, mae: 20.916314, mean_q: -30.176223, mean_eps: 0.163237\n"," 139636/150000: episode: 757, duration: 0.867s, episode steps: 111, steps per second: 128, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.566847, mae: 21.130594, mean_q: -30.462140, mean_eps: 0.162520\n"," 139721/150000: episode: 758, duration: 0.672s, episode steps:  85, steps per second: 126, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.129 [0.000, 2.000],  loss: 0.557626, mae: 21.050356, mean_q: -30.351647, mean_eps: 0.161932\n"," 139808/150000: episode: 759, duration: 0.696s, episode steps:  87, steps per second: 125, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.195 [0.000, 2.000],  loss: 0.578455, mae: 20.824790, mean_q: -30.042906, mean_eps: 0.161416\n"," 139932/150000: episode: 760, duration: 0.986s, episode steps: 124, steps per second: 126, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.105 [0.000, 2.000],  loss: 0.623281, mae: 20.840990, mean_q: -30.041664, mean_eps: 0.160783\n"," 140017/150000: episode: 761, duration: 0.676s, episode steps:  85, steps per second: 126, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.082 [0.000, 2.000],  loss: 0.695829, mae: 20.642966, mean_q: -29.720001, mean_eps: 0.160156\n"," 140122/150000: episode: 762, duration: 0.835s, episode steps: 105, steps per second: 126, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.019 [0.000, 2.000],  loss: 0.669258, mae: 21.016357, mean_q: -30.282325, mean_eps: 0.159586\n"," 140201/150000: episode: 763, duration: 0.643s, episode steps:  79, steps per second: 123, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: 0.561726, mae: 20.884938, mean_q: -30.140304, mean_eps: 0.159034\n"," 140278/150000: episode: 764, duration: 0.610s, episode steps:  77, steps per second: 126, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.013 [0.000, 2.000],  loss: 0.593004, mae: 20.578130, mean_q: -29.658039, mean_eps: 0.158566\n"," 140381/150000: episode: 765, duration: 0.828s, episode steps: 103, steps per second: 124, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.854 [0.000, 2.000],  loss: 0.555764, mae: 20.641681, mean_q: -29.790112, mean_eps: 0.158026\n"," 140501/150000: episode: 766, duration: 0.932s, episode steps: 120, steps per second: 129, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.586588, mae: 20.677167, mean_q: -29.824904, mean_eps: 0.157357\n"," 140608/150000: episode: 767, duration: 0.821s, episode steps: 107, steps per second: 130, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.953 [0.000, 2.000],  loss: 0.514593, mae: 20.971184, mean_q: -30.291395, mean_eps: 0.156676\n"," 140705/150000: episode: 768, duration: 0.772s, episode steps:  97, steps per second: 126, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.155 [0.000, 2.000],  loss: 0.613812, mae: 20.709914, mean_q: -29.846670, mean_eps: 0.156064\n"," 140809/150000: episode: 769, duration: 0.828s, episode steps: 104, steps per second: 126, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.962 [0.000, 2.000],  loss: 0.630378, mae: 20.750441, mean_q: -29.924041, mean_eps: 0.155461\n"," 140912/150000: episode: 770, duration: 0.804s, episode steps: 103, steps per second: 128, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.252 [0.000, 2.000],  loss: 0.546421, mae: 20.810671, mean_q: -30.001991, mean_eps: 0.154840\n"," 141005/150000: episode: 771, duration: 0.749s, episode steps:  93, steps per second: 124, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.022 [0.000, 2.000],  loss: 0.599847, mae: 20.725413, mean_q: -29.845550, mean_eps: 0.154252\n"," 141073/150000: episode: 772, duration: 0.540s, episode steps:  68, steps per second: 126, episode reward: -67.000, mean reward: -0.985 [-1.000,  0.000], mean action: 0.897 [0.000, 2.000],  loss: 0.639844, mae: 20.983038, mean_q: -30.261724, mean_eps: 0.153769\n"," 141167/150000: episode: 773, duration: 0.744s, episode steps:  94, steps per second: 126, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.979 [0.000, 2.000],  loss: 0.550753, mae: 20.996681, mean_q: -30.326718, mean_eps: 0.153283\n"," 141273/150000: episode: 774, duration: 0.831s, episode steps: 106, steps per second: 128, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.132 [0.000, 2.000],  loss: 0.675910, mae: 20.753265, mean_q: -29.926084, mean_eps: 0.152683\n"," 141367/150000: episode: 775, duration: 0.742s, episode steps:  94, steps per second: 127, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.064 [0.000, 2.000],  loss: 0.667643, mae: 20.925299, mean_q: -30.155076, mean_eps: 0.152083\n"," 141474/150000: episode: 776, duration: 0.840s, episode steps: 107, steps per second: 127, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 0.567325, mae: 20.936905, mean_q: -30.164695, mean_eps: 0.151480\n"," 141556/150000: episode: 777, duration: 0.655s, episode steps:  82, steps per second: 125, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 0.640398, mae: 20.676997, mean_q: -29.811009, mean_eps: 0.150913\n"," 141682/150000: episode: 778, duration: 1.010s, episode steps: 126, steps per second: 125, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.048 [0.000, 2.000],  loss: 0.537539, mae: 20.978653, mean_q: -30.275218, mean_eps: 0.150289\n"," 141761/150000: episode: 779, duration: 0.631s, episode steps:  79, steps per second: 125, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 0.550065, mae: 21.205600, mean_q: -30.605235, mean_eps: 0.149674\n"," 141854/150000: episode: 780, duration: 0.728s, episode steps:  93, steps per second: 128, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.151 [0.000, 2.000],  loss: 0.536618, mae: 20.717289, mean_q: -29.845237, mean_eps: 0.149158\n"," 141945/150000: episode: 781, duration: 0.708s, episode steps:  91, steps per second: 129, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.143 [0.000, 2.000],  loss: 0.564633, mae: 20.959460, mean_q: -30.260522, mean_eps: 0.148606\n"," 142026/150000: episode: 782, duration: 0.660s, episode steps:  81, steps per second: 123, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.099 [0.000, 2.000],  loss: 0.493410, mae: 20.825801, mean_q: -30.025994, mean_eps: 0.148090\n"," 142105/150000: episode: 783, duration: 0.648s, episode steps:  79, steps per second: 122, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.038 [0.000, 2.000],  loss: 0.634439, mae: 20.740017, mean_q: -29.887257, mean_eps: 0.147610\n"," 142195/150000: episode: 784, duration: 0.718s, episode steps:  90, steps per second: 125, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.033 [0.000, 2.000],  loss: 0.646542, mae: 20.788300, mean_q: -29.924981, mean_eps: 0.147103\n"," 142284/150000: episode: 785, duration: 0.706s, episode steps:  89, steps per second: 126, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.843 [0.000, 2.000],  loss: 0.574672, mae: 20.762839, mean_q: -29.952304, mean_eps: 0.146566\n"," 142369/150000: episode: 786, duration: 0.655s, episode steps:  85, steps per second: 130, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.965 [0.000, 2.000],  loss: 0.647968, mae: 20.771417, mean_q: -29.911329, mean_eps: 0.146044\n"," 142478/150000: episode: 787, duration: 0.855s, episode steps: 109, steps per second: 127, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 0.592262, mae: 20.770172, mean_q: -29.960448, mean_eps: 0.145462\n"," 142590/150000: episode: 788, duration: 0.885s, episode steps: 112, steps per second: 126, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.902 [0.000, 2.000],  loss: 0.644590, mae: 20.713486, mean_q: -29.854112, mean_eps: 0.144799\n"," 142697/150000: episode: 789, duration: 0.894s, episode steps: 107, steps per second: 120, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 0.656776, mae: 20.611943, mean_q: -29.662631, mean_eps: 0.144142\n"," 142789/150000: episode: 790, duration: 0.734s, episode steps:  92, steps per second: 125, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.022 [0.000, 2.000],  loss: 0.557664, mae: 20.559989, mean_q: -29.605777, mean_eps: 0.143545\n"," 142860/150000: episode: 791, duration: 0.551s, episode steps:  71, steps per second: 129, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.944 [0.000, 2.000],  loss: 0.528429, mae: 20.831693, mean_q: -30.100817, mean_eps: 0.143056\n"," 142948/150000: episode: 792, duration: 0.699s, episode steps:  88, steps per second: 126, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.114 [0.000, 2.000],  loss: 0.583529, mae: 20.938595, mean_q: -30.206881, mean_eps: 0.142579\n"," 143024/150000: episode: 793, duration: 0.624s, episode steps:  76, steps per second: 122, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.092 [0.000, 2.000],  loss: 0.615055, mae: 21.005377, mean_q: -30.281235, mean_eps: 0.142087\n"," 143396/150000: episode: 794, duration: 2.910s, episode steps: 372, steps per second: 128, episode reward: -371.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.532 [0.000, 2.000],  loss: 0.658646, mae: 20.770504, mean_q: -29.879826, mean_eps: 0.140743\n"," 143475/150000: episode: 795, duration: 0.617s, episode steps:  79, steps per second: 128, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.797 [0.000, 2.000],  loss: 0.743915, mae: 20.696245, mean_q: -29.791379, mean_eps: 0.139390\n"," 143554/150000: episode: 796, duration: 0.648s, episode steps:  79, steps per second: 122, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.949 [0.000, 2.000],  loss: 0.657476, mae: 20.734245, mean_q: -29.856481, mean_eps: 0.138916\n"," 143645/150000: episode: 797, duration: 0.726s, episode steps:  91, steps per second: 125, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.978 [0.000, 2.000],  loss: 0.618339, mae: 20.759622, mean_q: -29.893210, mean_eps: 0.138406\n"," 143736/150000: episode: 798, duration: 0.709s, episode steps:  91, steps per second: 128, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.644966, mae: 20.700814, mean_q: -29.757296, mean_eps: 0.137860\n"," 143853/150000: episode: 799, duration: 0.925s, episode steps: 117, steps per second: 127, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 0.604320, mae: 21.018236, mean_q: -30.296242, mean_eps: 0.137236\n"," 143957/150000: episode: 800, duration: 0.878s, episode steps: 104, steps per second: 119, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.038 [0.000, 2.000],  loss: 0.639058, mae: 20.765983, mean_q: -29.863434, mean_eps: 0.136573\n"," 144068/150000: episode: 801, duration: 0.904s, episode steps: 111, steps per second: 123, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.883 [0.000, 2.000],  loss: 0.589344, mae: 20.923825, mean_q: -30.123677, mean_eps: 0.135928\n"," 144170/150000: episode: 802, duration: 0.832s, episode steps: 102, steps per second: 123, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.892 [0.000, 2.000],  loss: 0.701149, mae: 20.980537, mean_q: -30.139216, mean_eps: 0.135289\n"," 144256/150000: episode: 803, duration: 0.684s, episode steps:  86, steps per second: 126, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.105 [0.000, 2.000],  loss: 0.707899, mae: 20.565710, mean_q: -29.608741, mean_eps: 0.134725\n"," 144343/150000: episode: 804, duration: 0.681s, episode steps:  87, steps per second: 128, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.862 [0.000, 2.000],  loss: 0.557009, mae: 20.765254, mean_q: -29.903663, mean_eps: 0.134206\n"," 144446/150000: episode: 805, duration: 0.827s, episode steps: 103, steps per second: 125, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.816 [0.000, 2.000],  loss: 0.627033, mae: 21.062035, mean_q: -30.402682, mean_eps: 0.133636\n"," 144548/150000: episode: 806, duration: 0.825s, episode steps: 102, steps per second: 124, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.775 [0.000, 2.000],  loss: 0.585369, mae: 20.527934, mean_q: -29.547745, mean_eps: 0.133021\n"," 144657/150000: episode: 807, duration: 0.849s, episode steps: 109, steps per second: 128, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.817 [0.000, 2.000],  loss: 0.572910, mae: 20.789875, mean_q: -29.956322, mean_eps: 0.132388\n"," 144749/150000: episode: 808, duration: 0.736s, episode steps:  92, steps per second: 125, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.924 [0.000, 2.000],  loss: 0.486154, mae: 20.733831, mean_q: -29.823977, mean_eps: 0.131785\n"," 144825/150000: episode: 809, duration: 0.606s, episode steps:  76, steps per second: 125, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.711 [0.000, 2.000],  loss: 0.470667, mae: 20.701647, mean_q: -29.830924, mean_eps: 0.131281\n"," 144905/150000: episode: 810, duration: 0.648s, episode steps:  80, steps per second: 123, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500291, mae: 20.766485, mean_q: -29.911343, mean_eps: 0.130813\n"," 145025/150000: episode: 811, duration: 0.925s, episode steps: 120, steps per second: 130, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.917 [0.000, 2.000],  loss: 0.663259, mae: 20.780524, mean_q: -29.909300, mean_eps: 0.130213\n"," 145114/150000: episode: 812, duration: 0.687s, episode steps:  89, steps per second: 130, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.798 [0.000, 2.000],  loss: 0.790829, mae: 20.557897, mean_q: -29.539181, mean_eps: 0.129586\n"," 145249/150000: episode: 813, duration: 1.089s, episode steps: 135, steps per second: 124, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.763 [0.000, 2.000],  loss: 0.706640, mae: 21.122104, mean_q: -30.408449, mean_eps: 0.128914\n"," 145342/150000: episode: 814, duration: 0.728s, episode steps:  93, steps per second: 128, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.914 [0.000, 2.000],  loss: 0.700537, mae: 20.348856, mean_q: -29.227798, mean_eps: 0.128230\n"," 145452/150000: episode: 815, duration: 0.870s, episode steps: 110, steps per second: 126, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.936 [0.000, 2.000],  loss: 0.653622, mae: 20.590914, mean_q: -29.634201, mean_eps: 0.127621\n"," 145536/150000: episode: 816, duration: 0.688s, episode steps:  84, steps per second: 122, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.964 [0.000, 2.000],  loss: 0.607420, mae: 20.947067, mean_q: -30.198837, mean_eps: 0.127039\n"," 145619/150000: episode: 817, duration: 0.626s, episode steps:  83, steps per second: 133, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.867 [0.000, 2.000],  loss: 0.712747, mae: 20.823117, mean_q: -29.976265, mean_eps: 0.126538\n"," 145716/150000: episode: 818, duration: 0.762s, episode steps:  97, steps per second: 127, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.866 [0.000, 2.000],  loss: 0.686059, mae: 20.792162, mean_q: -29.978174, mean_eps: 0.125998\n"," 145795/150000: episode: 819, duration: 0.650s, episode steps:  79, steps per second: 122, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.911 [0.000, 2.000],  loss: 0.682093, mae: 20.732944, mean_q: -29.866046, mean_eps: 0.125470\n"," 145926/150000: episode: 820, duration: 1.045s, episode steps: 131, steps per second: 125, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.855 [0.000, 2.000],  loss: 0.650178, mae: 20.587352, mean_q: -29.677796, mean_eps: 0.124840\n"," 145997/150000: episode: 821, duration: 0.551s, episode steps:  71, steps per second: 129, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.859 [0.000, 2.000],  loss: 0.529634, mae: 20.698450, mean_q: -29.838774, mean_eps: 0.124234\n"," 146118/150000: episode: 822, duration: 0.944s, episode steps: 121, steps per second: 128, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.926 [0.000, 2.000],  loss: 0.779571, mae: 20.363392, mean_q: -29.258158, mean_eps: 0.123658\n"," 146197/150000: episode: 823, duration: 0.634s, episode steps:  79, steps per second: 125, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.962 [0.000, 2.000],  loss: 0.731568, mae: 20.791096, mean_q: -29.892235, mean_eps: 0.123058\n"," 146325/150000: episode: 824, duration: 1.014s, episode steps: 128, steps per second: 126, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.070 [0.000, 2.000],  loss: 0.663557, mae: 21.138798, mean_q: -30.466031, mean_eps: 0.122437\n"," 146396/150000: episode: 825, duration: 0.547s, episode steps:  71, steps per second: 130, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.958 [0.000, 2.000],  loss: 0.671701, mae: 20.891702, mean_q: -30.082385, mean_eps: 0.121840\n"," 146494/150000: episode: 826, duration: 0.793s, episode steps:  98, steps per second: 124, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.857 [0.000, 2.000],  loss: 0.671587, mae: 20.967801, mean_q: -30.170609, mean_eps: 0.121333\n"," 146583/150000: episode: 827, duration: 0.718s, episode steps:  89, steps per second: 124, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.146 [0.000, 2.000],  loss: 0.601186, mae: 20.902231, mean_q: -30.123835, mean_eps: 0.120772\n"," 146660/150000: episode: 828, duration: 0.596s, episode steps:  77, steps per second: 129, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.883 [0.000, 2.000],  loss: 0.578509, mae: 20.939852, mean_q: -30.163106, mean_eps: 0.120274\n"," 146731/150000: episode: 829, duration: 0.566s, episode steps:  71, steps per second: 126, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.901 [0.000, 2.000],  loss: 0.514999, mae: 21.152896, mean_q: -30.546312, mean_eps: 0.119830\n"," 146833/150000: episode: 830, duration: 0.814s, episode steps: 102, steps per second: 125, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.912 [0.000, 2.000],  loss: 0.577987, mae: 20.962042, mean_q: -30.232313, mean_eps: 0.119311\n"," 146911/150000: episode: 831, duration: 0.613s, episode steps:  78, steps per second: 127, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.038 [0.000, 2.000],  loss: 0.604150, mae: 20.878926, mean_q: -30.074731, mean_eps: 0.118771\n"," 147020/150000: episode: 832, duration: 0.849s, episode steps: 109, steps per second: 128, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.954 [0.000, 2.000],  loss: 0.613264, mae: 20.915276, mean_q: -30.187492, mean_eps: 0.118210\n"," 147106/150000: episode: 833, duration: 0.688s, episode steps:  86, steps per second: 125, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.977 [0.000, 2.000],  loss: 0.787818, mae: 20.710611, mean_q: -29.774409, mean_eps: 0.117625\n"," 147200/150000: episode: 834, duration: 0.740s, episode steps:  94, steps per second: 127, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.032 [0.000, 2.000],  loss: 0.762866, mae: 20.897646, mean_q: -30.107392, mean_eps: 0.117085\n"," 147264/150000: episode: 835, duration: 0.508s, episode steps:  64, steps per second: 126, episode reward: -63.000, mean reward: -0.984 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.602009, mae: 20.965147, mean_q: -30.243999, mean_eps: 0.116611\n"," 147329/150000: episode: 836, duration: 0.527s, episode steps:  65, steps per second: 123, episode reward: -64.000, mean reward: -0.985 [-1.000,  0.000], mean action: 0.862 [0.000, 2.000],  loss: 0.686084, mae: 21.030683, mean_q: -30.335886, mean_eps: 0.116224\n"," 147421/150000: episode: 837, duration: 0.715s, episode steps:  92, steps per second: 129, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.098 [0.000, 2.000],  loss: 0.628073, mae: 20.702005, mean_q: -29.851580, mean_eps: 0.115753\n"," 147501/150000: episode: 838, duration: 0.632s, episode steps:  80, steps per second: 127, episode reward: -79.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.963 [0.000, 2.000],  loss: 0.706746, mae: 20.894129, mean_q: -30.098862, mean_eps: 0.115237\n"," 147573/150000: episode: 839, duration: 0.570s, episode steps:  72, steps per second: 126, episode reward: -71.000, mean reward: -0.986 [-1.000,  0.000], mean action: 0.736 [0.000, 2.000],  loss: 0.709890, mae: 20.407163, mean_q: -29.320934, mean_eps: 0.114781\n"," 147661/150000: episode: 840, duration: 0.686s, episode steps:  88, steps per second: 128, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.898 [0.000, 2.000],  loss: 0.670381, mae: 21.017889, mean_q: -30.245877, mean_eps: 0.114301\n"," 147781/150000: episode: 841, duration: 0.973s, episode steps: 120, steps per second: 123, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.925 [0.000, 2.000],  loss: 0.642023, mae: 20.841907, mean_q: -30.039948, mean_eps: 0.113677\n"," 147884/150000: episode: 842, duration: 0.836s, episode steps: 103, steps per second: 123, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 0.586113, mae: 20.711562, mean_q: -29.833606, mean_eps: 0.113008\n"," 147990/150000: episode: 843, duration: 0.838s, episode steps: 106, steps per second: 126, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.868 [0.000, 2.000],  loss: 0.599490, mae: 20.858272, mean_q: -30.034823, mean_eps: 0.112381\n"," 148075/150000: episode: 844, duration: 0.675s, episode steps:  85, steps per second: 126, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.106 [0.000, 2.000],  loss: 0.768990, mae: 20.579681, mean_q: -29.597479, mean_eps: 0.111808\n"," 148172/150000: episode: 845, duration: 0.781s, episode steps:  97, steps per second: 124, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.948 [0.000, 2.000],  loss: 0.608135, mae: 20.841271, mean_q: -30.060466, mean_eps: 0.111262\n"," 148240/150000: episode: 846, duration: 0.547s, episode steps:  68, steps per second: 124, episode reward: -67.000, mean reward: -0.985 [-1.000,  0.000], mean action: 0.956 [0.000, 2.000],  loss: 0.673419, mae: 20.570376, mean_q: -29.630258, mean_eps: 0.110767\n"," 148336/150000: episode: 847, duration: 0.767s, episode steps:  96, steps per second: 125, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.062 [0.000, 2.000],  loss: 0.729250, mae: 20.318156, mean_q: -29.189681, mean_eps: 0.110275\n"," 148421/150000: episode: 848, duration: 0.686s, episode steps:  85, steps per second: 124, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.118 [0.000, 2.000],  loss: 0.618518, mae: 21.037163, mean_q: -30.341512, mean_eps: 0.109732\n"," 148527/150000: episode: 849, duration: 0.837s, episode steps: 106, steps per second: 127, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.104 [0.000, 2.000],  loss: 0.600320, mae: 20.776733, mean_q: -29.904293, mean_eps: 0.109159\n"," 148693/150000: episode: 850, duration: 1.301s, episode steps: 166, steps per second: 128, episode reward: -165.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.030 [0.000, 2.000],  loss: 0.624465, mae: 20.816075, mean_q: -29.962707, mean_eps: 0.108343\n"," 148770/150000: episode: 851, duration: 0.610s, episode steps:  77, steps per second: 126, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 0.569448, mae: 21.106220, mean_q: -30.458822, mean_eps: 0.107614\n"," 148849/150000: episode: 852, duration: 0.648s, episode steps:  79, steps per second: 122, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.038 [0.000, 2.000],  loss: 0.570098, mae: 20.789259, mean_q: -29.954789, mean_eps: 0.107146\n"," 148964/150000: episode: 853, duration: 0.907s, episode steps: 115, steps per second: 127, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.930 [0.000, 2.000],  loss: 0.584690, mae: 20.489943, mean_q: -29.514304, mean_eps: 0.106564\n"," 149086/150000: episode: 854, duration: 1.005s, episode steps: 122, steps per second: 121, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.033 [0.000, 2.000],  loss: 0.746571, mae: 20.547382, mean_q: -29.516352, mean_eps: 0.105853\n"," 149186/150000: episode: 855, duration: 0.933s, episode steps: 100, steps per second: 107, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.120 [0.000, 2.000],  loss: 0.739928, mae: 20.624642, mean_q: -29.624745, mean_eps: 0.105187\n"," 149263/150000: episode: 856, duration: 0.934s, episode steps:  77, steps per second:  82, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.117 [0.000, 2.000],  loss: 0.675351, mae: 20.628329, mean_q: -29.660300, mean_eps: 0.104656\n"," 149338/150000: episode: 857, duration: 0.893s, episode steps:  75, steps per second:  84, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.107 [0.000, 2.000],  loss: 0.588434, mae: 20.968151, mean_q: -30.195263, mean_eps: 0.104200\n"," 149419/150000: episode: 858, duration: 0.941s, episode steps:  81, steps per second:  86, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.074 [0.000, 2.000],  loss: 0.581710, mae: 20.758692, mean_q: -29.873409, mean_eps: 0.103732\n"," 149511/150000: episode: 859, duration: 1.103s, episode steps:  92, steps per second:  83, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.152 [0.000, 2.000],  loss: 0.555381, mae: 20.787871, mean_q: -29.931517, mean_eps: 0.103213\n"," 149596/150000: episode: 860, duration: 1.029s, episode steps:  85, steps per second:  83, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.047 [0.000, 2.000],  loss: 0.634566, mae: 20.610702, mean_q: -29.634182, mean_eps: 0.102682\n"," 149702/150000: episode: 861, duration: 1.286s, episode steps: 106, steps per second:  82, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.066 [0.000, 2.000],  loss: 0.708211, mae: 20.932758, mean_q: -30.091163, mean_eps: 0.102109\n"," 149778/150000: episode: 862, duration: 0.952s, episode steps:  76, steps per second:  80, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.079 [0.000, 2.000],  loss: 0.717760, mae: 20.615380, mean_q: -29.662380, mean_eps: 0.101563\n"," 149855/150000: episode: 863, duration: 0.979s, episode steps:  77, steps per second:  79, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.567034, mae: 20.757845, mean_q: -29.927643, mean_eps: 0.101104\n"," 149964/150000: episode: 864, duration: 1.288s, episode steps: 109, steps per second:  85, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.917 [0.000, 2.000],  loss: 0.723055, mae: 20.708101, mean_q: -29.724497, mean_eps: 0.100546\n","done, took 1187.855 seconds\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fb56ca37ee0>"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["dqn.save_weights(f\"my_weights_cartpole.h5f\",overwrite=True)"],"metadata":{"id":"x-EcUh8dTz9z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dqn.load_weights(f\"my_weights_cartpole.h5f\",overwrite=True)"],"metadata":{"id":"3zAmUk-oT0Aj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dqn.test(env,nb_episodes=5,visualize=True)\n","env.close()"],"metadata":{"id":"QXjg-mG0UFaR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"W-doLtWaTIlT"},"execution_count":null,"outputs":[]}]}